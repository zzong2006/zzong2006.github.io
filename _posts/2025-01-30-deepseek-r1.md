---
layout: post
title: DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델
date: 2025-01-30 12:18:00
giscus_comments: true
categories: paper-review
toc:
  beginning: true
  sidebar: true
tags: DeepSeek LLM RL
---

최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.

도대체 어떤 모델을 만들었는지 한번 살펴보자.

## Introduction

OpenAI o1 은 inference-time 이 확장될수록 수학, 코딩, 과학 추론 능력이 향상되는것을 보여준 모델이다. 이후 효율적인 test-time scaling 을 위해 프로세스 기반 보상 모델(PRM), 강화 학습(RL), 몬테카를로 트리 탐색(MCTS) 등의 기술을 시도해봤지만 o1 의 아성을 넘기에는 한계가 있었다.

DeepSeek 은 DeepSeek-V3-Base를 기본 모델로 사용하고 GRPO(Shao et al., 2024)를 RL 프레임워크로 적용해 추론 성능을 개선했다. 이는 SFT 를 제외한, 순수 RL 과정을 통해 **자기 진화(self-evolution)**하며 추론 능력을 발전시킬 수 있는 잠재력을 탐구한 시도로 생각할 수 있겠다.

수천번의 RL 단계를 거치면서 흥미로운 추론 행동이 자연스럽게 발현된 DeepSeek-Zero 는 초창기 o1-0912 급 모델의 추론 수준을 보였으나, 가독성과 언어가 혼합되어 나오는 문제를 발견했다.

이런 문제를 해결하기 위해 수천개의 cold-start 데이터만을 모아 v3-base 모델을 먼저 파인튜닝하고, 이후 zero 와 비슷한 RL 단계를 거치면서 훈련을 진행했다. RL 과정에서 수렴 단계로 도달한 경우, rejection sampling 을 통해 새로운 sft 데이터를 생성하고, 기존 DeepSeek-V3 의 지도 학습 데이터와 합쳐 v3-base 모델을 다시 학습했다. 이런 과정을 반복해서 만들어진 최종 deepseek-r1 checkpoint 는 o1-1217 와 동등한 수준의 추론 능력을 달성했다.

## 주요 모델

DeepSeek-R1-Zero, DeepSeek-R1, Qwen/Llama 기반의 1.5B~70B 수준 지식 증류 모델 공개.

### (1) DeepSeek-R1-Zero

SFT 없이 RL로 훈련된 초기 모델이다. 추론 능력은 뛰어나지만 가독성(poor readability), language mixing 이슈가 존재한다고 한다.

RL 학습을 위해서 GRPO (Group Relative Policy Optimization) 방식을 적용했다. 일반적으로 강화 학습 과정에서는 policy 와 critic 모델을 따로 가져가는데, critic 모델 대신 어떤 하나의 그룹의 점수들로 critic 의 평가를 대체하는 전략을 활용한다.

GRPO 학습 과정에서는 어떤 질문 $q$ 가 주어졌을 때, 이전 정책 $\pi_{\theta_{\text{old}}}$로부터 여러 개의 출력 $\{o_1, o_2, \cdots, o_G\}$을 생성하고, 보상(점수) $\{r_1, r_2, \dots, r_G\}$을 구한뒤, 이를 바탕으로 정책 모델 $\pi_{\theta}$를 최적화한다. 이때, 점수들은 그룹 내부의 평균 및 표준편차로 정규화하여 "어느 후보가 상대적으로 좋았는지의 개념인 advantage $A_i$ 를 계산한다. 즉, 우도(advantage)를 바탕으로 새 정책 $\pi_{\theta}$를 업데이트한다.

그럼 보상(점수)는 어떻게 계산헀을까? Rule based 기반의 채점 방식을 사용했는데, 수학이나 코딩 문제처럼 정답이 명확한 경우에만 점수를 주도록 설계했다고 한다. 다른 신경망 모델을 이용해서 점수를 채점하는 방식은 사용하지 않았다고 하는데, 그 이유는 reward hacking 문제나 reward model 자체를 재학습 하는 과정이 복잡해지고 난이도가 상승하기 때문이라고 한다.

응답을 생성할때는 특정 content 에 대한 bias 를 제한하기 위해 아래처럼 고정된 프롬프트를 사용했다고 한다.

![20250130130652](https://i.imgur.com/aHE9I7H.png){: width="80%"}

이렇게 학습을 할수록 모델의 응답 길이가 늘어나면서 동시에 aha moment 가 발생한다고 한다. aha moment 는 모델이 문제 해결 과정에서 초기 접근법을 다시 평가하며 사고 시간을 더 많이 할당하는 법이 자연스럽게 발현되는 현상을 의미한다. 즉, 문제를 해결하는 구체적 방법을 명시적으로 가르치지 않고도, 그저 올바른 보상 체계를 부여하기만 하면 모델이 스스로 고도의 문제 해결 전략을 창출해낸다는 점이라고 한다.

![20250130131632](https://i.imgur.com/scvFkSj.png){: width="80%"}

위 내용처럼 `Wait, wait. Wait.` 과 같은 문장과 함께 모델이 문제의 접근 방식을 다시한번 스스로 생각하는 모습을 볼 수 있다.

이렇게 학습된 Zero 모델은 위에서 언급했듯이 추론 능력을 크게 끌어올릴 수 있었지만, 추론 과정에서 가독성이 떨어지고 여러 언어가 섞여서 응답하는 문제가 있다고 한다.

### (2) DeepSeek-R1

DeepSeek-R1 모델은 초기 모델인 DeepSeek-R1-Zero 에서 발견된 문제를 완화한 모델이다. RL 적용전에 cold-start 데이터와 multi-stage 훈련을 진행했고, 그 결과 OpenAI-o1-1217과 유사한 성능을 달성했다고 한다.

Zero 에서는 초기 RL 학습시 불안정한 모습을 보이는 경우가 많았다고 한다. 이런 문제를 해결하기 위해 초기 모델에 cold-start 데이터를 이용해서 학습을 진행했는데, cold-start 데이터는 다양한 시도를 진행했다고 한다.

1. 긴 CoT를 사례로 삼아 few-shot 프롬프트를 적용
2. 모델에 직접 reflection과 verification 과정을 포함해 상세한 답변을 생성하도록 지시하는 방법
3. DeepSeek-R1-Zero의 출력을 읽기 쉬운 형태로 정리한 뒤 Human annotators가 후처리를 거쳐 결과를 개선하는 방법

주로 데이터 형태는 `|special_token|<reasoning_process>|special_token|<summary>` 로 표현되었는데, reasoning_process는 쿼리에 대한 CoT(Chain of Thought)를, summary는 추론 결과를 요약하는 용도로 사용했다.

인간의 사전 지식(prior)을 고려해 콜드스타트 데이터를 신중히 설계하면, DeepSeek-R1-Zero와 비교했을 때 더 나은 성능을 발휘한다고 한다 (역시 정성적으로 살펴본 고퀄리티 데이터도 중요한거 같다).



## Lesson Learned

지식 증류 vs. RL:

1. 소형 모델은 RL만으로는 성능 한계 존재 → 지식 증류가 효율적.
2. 단, 지능 한계 돌파에는 대형 모델과 대규모 RL 필요.

실패 사례:

- PRM(Process Reward Model): 보상 해킹 및 복잡성 문제.
- MCTS(Monte Carlo Tree Search): 토큰 생성 공간의 복잡성으로 확장 어려움.

## Limitations

강력한 모델이지만 o1 과 비슷한 느낌의 단점들이 존재하는 것으로 보인다.

**(1) General Capability**

함수 호출, 멀티턴 대화, 복잡한 롤플레잉, JSON 출력과 같은 작업에서 DeepSeek-V3에 비해 부족한 면

**(2) Language Mixing**

DeepSeek-R1은 현재 중국어와 영어에 특화되어 있으므로, 다른 언어 쿼리를 처리할 때 언어 혼합 문제가 발생할 수 있다. 예컨대 영어 외 언어로 쿼리를 입력해도, 영어로 추론 과정을 작성하고 응답을 출력할 수 있다.

**(3) Prompting Engineering**

모델이 프롬프트(prompt)에 민감하게 반응한다.Few-shot 프롬프트 설정을 할 경우, 성능이 지속적으로 저하되는 경향을 보인다. 따라서 가장 효율적인 결과를 얻기 위해서는 제로샷(zero-shot) 방식으로 문제를 직접 설명하고 출력 형식을 지정하는 방법을 권장한다.

**(4) Software Engineering**

추론 시간이 길어지면 RL 프로세스의 효율에 영향을 미치기 때문에, 대규모 RL이 소프트웨어 엔지니어링 과제에 광범위하게 적용된 적은 아직 많지 않다. 이로 인해 DeepSeek-R1은 소프트웨어 엔지니어링 벤치마크에서 DeepSeek-V3 대비 현저한 성능 향상을 보이진 못했다고 한다.

## Reference

- [DeepSeek-R1 (github)](https://github.com/deepseek-ai/DeepSeek-R1)
