---
layout: post
title: DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델
date: 2025-01-30 12:18:00
giscus_comments: true
categories: paper-review
toc:
  beginning: true
  sidebar: true
tags: DeepSeek LLM RL
---

최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.

도대체 어떤 모델을 만들었는지 한번 살펴보자.

지도 학습 없이 **순수 강화 학습(RL)**을 통해 LLM의 추론 능력을 발전시키는 방법

## Main Stream

OpenAI o1 은 inference-time 이 확장될수록 수학, 코딩, 과학 추론 능력이 향상되는것을 보여준 모델이다. 이후 효율적인 test-time scaling 을 위해 프로세스 기반 보상 모델(PRM), 강화 학습(RL), 몬테카를로 트리 탐색(MCTS) 등의 기술을 시도해봤지만 o1 의 아성을 넘기에는 한계가 있었다.

DeepSeek 은 DeepSeek-V3-Base를 기본 모델로 사용하고 GRPO(Shao et al., 2024)를 RL 프레임워크로 적용해 추론 성능을 개선했다. 이는 SFT 를 제외한, 순수 RL 과정을 통해 **자기 진화(self-evolution)**하며 추론 능력을 발전시킬 수 있는 잠재력을 탐구한 시도로 생각할 수 있겠다.

수천번의 RL 단계를 거치면서 흥미로운 추론 행동이 자연스럽게 발현된 DeepSeek-Zero 는 초창기 o1-0912 급 모델의 추론 수준을 보였으나, 가독성과 언어가 혼합되어 나오는 문제를 발견했다.

이런 문제를 해결하기 위해 수천개의 cold-start 데이터만을 모아 v3-base 모델을 먼저 파인튜닝하고, 이후 zero 와 비슷한 RL 단계를 거치면서 훈련을 진행했다. RL 과정에서 수렴 단계로 도달한 경우, rejection sampling 을 통해 새로운 sft 데이터를 생성하고, 기존 DeepSeek-V3 의 지도 학습 데이터와 합쳐 v3-base 모델을 다시 학습했다. 이런 과정을 반복해서 만들어진 최종 deepseek-r1 checkpoint 는 o1-1217 와 동등한 수준의 추론 능력을 달성했다.

## 주요 모델

DeepSeek-R1-Zero, DeepSeek-R1, Qwen/Llama 기반의 1.5B~70B 수준 지식 증류 모델 공개.

### (1) DeepSeek-R1-Zero

SFT 없이 RL로 훈련된 초기 모델이다. 추론 능력은 뛰어나지만 가독성(poor readability), language mixing 이슈가 존재한다고 한다.

RL 학습을 위해서 GRPO (Group Relative Policy Optimization) 방식을 적용했다. 일반적으로 강화 학습 과정에서는 policy 와 critic 모델을 따로 가져가는데, critic 모델 대신 어떤 하나의 그룹의 점수들로 critic 의 평가를 대체하는 전략을 활용한다.

GRPO 학습 과정에서는 어떤 질문 $q$ 가 주어졌을 때, 이전 정책 $\pi_{\theta_{\text{old}}}$로부터 여러 개의 출력 $\{o_1, o_2, \cdots, o_G\}$을 생성하고, 보상(점수) $\{r_1, r_2, \dots, r_G\}$을 구한뒤, 이를 바탕으로 정책 모델 $\pi_{\theta}$를 최적화한다. 이때, 점수들은 그룹 내부의 평균 및 표준편차로 정규화하여 "어느 후보가 상대적으로 좋았는지의 개념인 advantage $A_i$ 를 계산한다. 즉, 우도(advantage)를 바탕으로 새 정책 $\pi_{\theta}$를 업데이트한다.

그럼 보상(점수)는 어떻게 계산헀을까? Rule based 기반의 채점 방식을 사용했는데, 수학이나 코딩 문제처럼 정답이 명확한 경우에만 점수를 주도록 설계했다고 한다. 다른 신경망 모델을 이용해서 점수를 채점하는 방식은 사용하지 않았다고 하는데, 그 이유는 reward hacking 문제나 reward model 자체를 재학습 하는 과정이 복잡해지고 난이도가 상승하기 때문이라고 한다.

응답을 생성할때는 특정 content 에 대한 bias 를 제한하기 위해 아래처럼 고정된 프롬프트를 사용했다고 한다.

![20250130130652](https://i.imgur.com/aHE9I7H.png){: width="80%"}

이렇게 학습을 할수록 모델의 응답 길이가 늘어나면서 동시에 aha moment 가 발생한다고 한다. aha moment 는 모델이 문제 해결 과정에서 초기 접근법을 다시 평가하며 사고 시간을 더 많이 할당하는 법이 자연스럽게 발현되는 현상을 의미한다. 즉, 문제를 해결하는 구체적 방법을 명시적으로 가르치지 않고도, 그저 올바른 보상 체계를 부여하기만 하면 모델이 스스로 고도의 문제 해결 전략을 창출해낸다는 점이라고 한다.

![20250130131208](https://i.imgur.com/ystpLR5.png){: width="80%"}


### (2) DeepSeek-R1

- RL 적용전에 cold-start 데이터와 multi-stage 훈련으로 zero 의 문제를 해결. 
- OpenAI-o1-1217과 유사한 성능 달성.

## Lesson Learned

지식 증류 vs. RL:

1. 소형 모델은 RL만으로는 성능 한계 존재 → 지식 증류가 효율적.
2. 단, 지능 한계 돌파에는 대형 모델과 대규모 RL 필요.

실패 사례:

- PRM(Process Reward Model): 보상 해킹 및 복잡성 문제.
- MCTS(Monte Carlo Tree Search): 토큰 생성 공간의 복잡성으로 확장 어려움.

Limitations:

- 다국어 혼합 문제.
- 소프트웨어 엔지니어링 태스크 성능 미흡.

## Reference

- [DeepSeek-R1 (github)](https://github.com/deepseek-ai/DeepSeek-R1)