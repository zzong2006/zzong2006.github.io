---
layout: post
title: DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델
date: 2025-01-30 12:18:00
giscus_comments: true
categories: paper-review
toc:
  beginning: true
  sidebar: true
tags: DeepSeek LLM RL
---

최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.

도대체 어떤 모델을 만들었는지 한번 살펴보자.

지도 학습 없이 **순수 강화 학습(RL)**을 통해 LLM의 추론 능력을 발전시키는 방법

## 주요 모델

DeepSeek-R1-Zero, DeepSeek-R1, Qwen/Llama 기반의 1.5B~70B 수준 지식 증류 모델 공개.

1. DeepSeek-R1-Zero: SFT 없이 RL로 훈련된 초기 모델. 추론 능력은 뛰어나지만 가독성, 언어 혼합 문제 존재.
2. DeepSeek-R1: 콜드 스타트 데이터와 다단계 훈련으로 문제 해결. OpenAI-o1-1217과 유사한 성능 달성.

## Lesson Learned

지식 증류 vs. RL:

1. 소형 모델은 RL만으로는 성능 한계 존재 → 지식 증류가 효율적.
2. 단, 지능 한계 돌파에는 대형 모델과 대규모 RL 필요.

실패 사례:

- PRM(Process Reward Model): 보상 해킹 및 복잡성 문제.
- MCTS(Monte Carlo Tree Search): 토큰 생성 공간의 복잡성으로 확장 어려움.

Limitations:

- 다국어 혼합 문제.
- 소프트웨어 엔지니어링 태스크 성능 미흡.

## Reference

- [DeepSeek-R1 (github)](https://github.com/deepseek-ai/DeepSeek-R1)