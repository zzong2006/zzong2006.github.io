---
layout: post
title: Meta 에서 만든 Agent 벤치마크, GAIA
date: 2025-01-09 20:00:00
giscus_comments: true
categories: benchmark
toc:
  beginning: true
  sidebar: left
tags: LLM agent WIP
---

## What is the GAIA?

GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc).

Even equipped with tools, GPT4 does not exceed a 30% success rate for the easiest of our tasks, and 0% for the hardest. In the meantime, the average success rate for human respondents is 92%.

![20250109203102](https://i.imgur.com/64s7BqX.png){: width="100%"}

GAIA는 인간에게는 개념적으로 간단하지만, AI에게는 복잡한 작업을 요구하는 방식으로 설계되었다. 

작업 자체는 간단하지만, **복잡한 순차적 행동(sequence of actions)**과 **조합적 공간(combinatorial space)**을 필요로 합니다.
작업의 결과물은 작업이 완전히 성공적으로 수행되었을 때만 얻어지며, 결과를 검증하는 것은 상대적으로 간단합니다.

## GAIA 특징

1. GAIA는 실세계에서 사용될 가능성이 높은 질문을 기반으로 설계됨: Browse the open and changing web, handle multi-modality, or reason over multiple steps to answer our questions.
2. Non-gameability: 다양한 단계를 성공적으로 완료해야 정답을 완료할 수 있으므로, 추측이나 우연으로 정답을 맞추기가 어렵게 설계됨
3. Easy Interpretability: GAIA의 질문은 간단하며, **명확한 이유 추적(reasoning trace)**이 가능하여 비전문가도 쉽게 확인할 수 있는 검증 과정 제공
   1. 반면 MMLU 같은 벤치마크는 잘못된 추론을 해도 정답에 도달할 가능성이 높음


## Related Works

APIBench 또는 AgentBench 같은 다양한 **폐쇄형 환경(closed box environments)**에서 어시스턴트 LLM을 테스트할 수 있는 기존 벤치마크들이 있었다.

하지만 (1) 폐쇄된 환경에서 (2) 특정 API나 시스템 사용을 평가하는데 주요 관심이 있다. 즉, 모델이 "특정 API를 얼마나 잘 사용하는가"를 측정하는 데 초점이 맞춰져 있다.

이러한 접근은 현실 세계에서의 상호작용 결과를 일반화하기 어렵게 만들고, 일반적인(real-world grounded) 평가가 아닌, 특정 도구 사용 능력을 평가하는 데 그칠 위험이 있다.

## GAIA 데이터

### 난이도에 따른 데이터 샘플 예시

총 3개의 난이도가 있음: Level 1, Level 2, Level 3

**Level 1**

- Question: 2018년 1월부터 5월까지 NIH 웹사이트에 기재된 H. pylori와 여드름 환자에 대한 임상 시험의 실제 등록 수는 얼마였습니까?
- 정답: 90

**Level 2**

(파인트 아이스크림 성분 구성표가 사진으로 제공됨)

- 질문: 이 파인트 전체가 아이스크림으로 구성되어 있다면, 2020년 위키피디아에 보고된 미국 연방 기준의 버터 지방 함량에 비해 몇 퍼센트 높거나 낮습니까? + 또는 -로 시작하는 숫자로 소수점 첫째 자리까지 반올림하여 답변하세요.
- 정답: +4.6

**Level 3**

- 질문: 2006년 1월 21일, NASA의 천문학 데이에서 찍은 사진에서 두 명의 우주비행사가 보이며, 한 명이 다른 한 명보다 훨씬 작게 보입니다. 2023년 8월 기준으로, 작은 우주비행사가 속했던 NASA 우주비행사 그룹의 우주비행사 중 우주에서 가장 적은 시간을 보낸 사람은 누구이며, 그는 우주에서 몇 분(minutes)을 보냈습니까? 우주비행사의 성을 쓰고, 숫자와 세미콜론으로 구분하세요. 숫자에는 천 단위 구분 기호로 쉼표를 사용하세요.

- 정답: White; 5,876

### 난이도 구성

![20250109202854](https://i.imgur.com/TnbqeaX.png){: width="100%"}

## Limitation

(1) 정답에 이르는 추론 과정(trace)을 평가하지 않음

GAIA는 정답(ground truth)이 고유하다는 전제를 두지만, 다양한 경로가 정답에 이를 수 있음. 이러한 다양한 경로를 단순하고 명확하게 채점할 수 있는 방법이 아직 부재

(2) 도구 사용 모델에 국한된 평가

(3) 일부 GAIA 질문은 많은 세부 사항을 포함하여 자연스럽지 않게 보일 수 있음

하지만 이런 세부 사항은 평가를 더 철저히 하기 위해서 어쩔수 없는 것으로 보임.

(4) 언어 다양성 부족: GAIA의 질문은 **"표준 영어(standard English)"**로만 작성되어 있음

(5) GAIA 벤치마크는 시간이 지남에 따라 다음과 같은 이유로 유효성이 떨어질 수 있음 (decay)

- 데이터 오염(catastrophic contamination): 사전 학습 데이터에 질문 내용이 포함되는 경우.
- 웹 콘텐츠의 소멸: 질문에 필요한 정보가 인터넷에서 사라지는 경우.

## Appendix

### GAIA 벤치마크에서의 System prompt

> You are a general AI assistant. I will ask you a question.
> Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].
> YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.
> If you are asked for a number, don’t use comma to write your number neither use units such as $ or percent sign unless specified otherwise.
> If you are asked for a string, don’t use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.
> If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.

### Leaderboard (2025-01-09 기준)

![20250109205637](https://i.imgur.com/ZBSW9w5.png){: width="100%"}

## References


- [GAIA Leaderboard](https://huggingface.co/spaces/gaia-benchmark/leaderboard)
- [GAIA (huggingface dataset)](https://huggingface.co/datasets/gaia-benchmark/GAIA)
- [GAIA (github)](https://github.com/aymeric-roucher/GAIA)
