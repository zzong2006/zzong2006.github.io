---
layout: post
title: Don't do RAG
date: 2025-01-05
giscus_comments: true
categories: paper-review
toc:
  beginning: true
  sidebar: left
tags: RAG LLM
---

## Introduction

RAG는 검색 지연, 문서 선택 오류, 시스템 복잡성 증가 같은 문제를 일으킨다. 최근 LLM의 컨텍스트 길이가 크게 늘어나면서, 실시간 검색 전략인 캐시 증강 생성(CAG)이 제안된다. 

CAG는 모델의 컨텍스트 윈도우에 모든 관련 리소스를 미리 로드해서 캐시 형태로 활용하는 방식이다. 그래서 인퍼런스 과정에서 모델이 이미 캐시된 파라미터를 사용해서 추가적인 검색 과정 없이 쿼리에 답변할 수 있다. 

CAG는 검색 지연을 줄이고 검색 오류를 최소화하면서도 컨텍스트 관련성을 유지할 수 있다고 한다. 여러 벤치마크 데이터셋에서 CAG가 기존 RAG보다 더 나은 성능을 보인다. 제한된 지식 기반을 가진 경우, CAG가 RAG보다 더 괜찮은 선택지로 고려해볼 수 있다고 한다.



## Reference

- paper: https://arxiv.org/abs/2412.15605
- code: https://github.com/hhhuang/cag