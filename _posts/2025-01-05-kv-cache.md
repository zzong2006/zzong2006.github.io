---
layout: post
title: KV-Cache 에 대해 알아보자
date: 2025-01-05 10:00:00
giscus_comments: true
categories: paper-review
toc:
  beginning: true
  sidebar: left
tags: Transformers LLM
---

The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).

Therefore, when iteratively calling forward() instead of the generate() method, it’s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape (batch_size, past_kv_length + new_tokens_length). This is usually handled internally when you call generate() method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.

## References

- [huggingface transformers - kv_cache](https://huggingface.co/docs/transformers/kv_cache)
