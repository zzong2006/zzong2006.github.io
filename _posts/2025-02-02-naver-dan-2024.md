---
layout: post
title: 네이버 컨퍼런스 DAN 24 정리
date: 2025-02-02 12:18:00
giscus_comments: true
categories: paper-review
toc:
  beginning: true
  sidebar: true
tags: NAVER Conference DAN 2024
---

DAN 발표를 보면서 네이버가 현재 어떤 방식으로 개발을 진행하고 있는지 정리해본다.

## Models

추천/광고에 해당되는 파운데이션 모델을 따로 개발하고 있음

- 언어 모델은 결국 next-token prediction 이므로, 입력으로 활용하는 pretext 를 얼마나 잘 구성하는지가 중요함
- 파운데이션 모델을 활용한 downstream task 는 pretext task 로 표현할 수 있다
- pretrain loss 줄이는 가장 쉬운 방법은 scale-up 을 통한 computation 을 늘리는것

### CLIP For E-Commerce

- 쇼핑에서는 중복되는 상품이 많기 때문에, 이를 해결하지 않고서는 학습이 잘 안된다
  - 카탈로그 정보를 활용하여 동일한 카탈로그면 동일한 상품이라고 판단하여 학습하지 않도록 유도
- 기존의 constrative Learning 은 모든 negative-positive pair 를 동일한 선상으로 생각하지만, 사람은 이러한 차이를 상대적으로 생각한다 (Pairtial False Negative Problem).
- 상품 텍스트만 가지고는 Masked Language Modeling 과 같이 빈칸 추론을 위한 충분한 맥락을 제공하고 있지 않다.
- eFoundation 이 제대로 학습되었다면 임베딩을 통한 상품의 이해를 표현할 수 있을 뿐만 아니라, 상품 관련 정보를 "생성" 할 수 있어야 한다.

## Metrics

Search: nDCG, MRR, Recall, Precision

## Embedding

Language: ColBERT, ColBERTv2
Multi-modal: eCLIP (CLIP For E-Commerce)


## Ranking

- XTR Ranking



## Reference

[팀네이버 컨퍼런스 DAN 24]

- [벡터 검색의 정점에 오르다: 최적의 뉴럴 검색 엔진으로 업그레이드 하기](https://tv.naver.com/v/67452264)
- [네이버 검색이 이렇게 좋아졌어? LLM의 Re-Ranking Ability 검색에 이식하기](https://tv.naver.com/v/67444172)
- [LLM 기반 추천/광고 파운데이션 모델](https://tv.naver.com/v/67445059)
- [eFoundation: 상품을 딱 잘 표현하는 임베딩을 만들었지 뭐야 ꒰⍢꒱ 완전 럭키비키잔앙 ☘︎](https://tv.naver.com/v/67444878)
