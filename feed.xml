<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-06T12:19:23+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">Believe I.Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Algorithm lesson learned - array</title><link href="https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array/" rel="alternate" type="text/html" title="Algorithm lesson learned - array"/><published>2025-01-06T10:00:00+00:00</published><updated>2025-01-06T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array/"><![CDATA[<p>ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ í’€ë©´ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì€ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</p> <h2 id="êµ¬ê°„-ì²˜ë¦¬">êµ¬ê°„ ì²˜ë¦¬</h2> <p>ì–´ë–¤ êµ¬ê°„ì„ ì²˜ë¦¬í•˜ëŠ” ë¬¸ì œëŠ” í•´ë‹¹ êµ¬ê°„ì„ ëª¨ë‘ ì²˜ë¦¬í• ë ¤ í•˜ì§€ë§ê³ , êµ¬ê°„ì˜ ì•ê³¼ ë ë¶€ë¶„ë§Œ ë‹¤ë£° ìˆ˜ ìˆëŠ”ì§€ ìƒê°í•´ë³´ì.</p> <p>auxiliary array (difference array) ì„ ì‚¬ìš©í•˜ì—¬ ë²”ìœ„ ì—…ë°ì´íŠ¸ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤. ë²”ìœ„ ë‚´ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹ , ë²”ìœ„ì˜ ì‹œì‘ì ê³¼ ëì ì„ í‘œì‹œí•˜ê³  ë‚˜ì¤‘ì— prefix sumì„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•œë‹¤.</p> <p>ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì–´ì§„ ë°°ì—´ì— ëŒ€í•´ [0, 4] êµ¬ê°„ì— +1ì„ ì ìš©í•˜ê³ , [2, 5] êµ¬ê°„ì— -1ì„ ì ìš©í•œë‹¤ê³  ê°€ì •í•´ë³´ì.</p> <ul> <li>auxiliary ë°°ì—´ì˜ [0] ì¸ë±ìŠ¤ì— +1ì„, [5] ì¸ë±ìŠ¤ì— -1ì„ ì ìš©í•˜ê³ , [2] ì¸ë±ìŠ¤ì— +1ì„, [6] ì¸ë±ìŠ¤ì— -1ì„ ì ìš©</li> <li>ê° ìš”ì†Œê°€ ì›ë˜ ë°°ì—´ì—ì„œ ì–¼ë§ˆë‚˜ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí–ˆëŠ”ì§€ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ prefix sumì„ ì‚¬ìš©í•˜ì—¬ ëˆ„ì  í•©ì„ ê³„ì‚°</li> </ul> <h3 id="ê´€ë ¨-ë¬¸ì œ">ê´€ë ¨ ë¬¸ì œ</h3> <ul> <li><a href="https://leetcode.com/problems/shifting-letters-ii/description/">leetcode: shifting-letters-ii</a></li> </ul>]]></content><author><name></name></author><category term="algorithm"/><category term="competitive-programming"/><category term="array"/><category term="string"/><category term="lesson-learned"/><summary type="html"><![CDATA[ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ í’€ë©´ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì€ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.]]></summary></entry><entry><title type="html">llama ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì</title><link href="https://zzong2006.github.io/blog/2025/llama/" rel="alternate" type="text/html" title="llama ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì"/><published>2025-01-06T10:00:00+00:00</published><updated>2025-01-06T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/llama</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/llama/"><![CDATA[<p>LLaMA 1, 2 ëª¨ë¸ì„ ëœ¯ì–´ë³´ë©´ì„œ ì•Œê²Œëœ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</p> <blockquote> <p>We attribute their success, as all else, to divine benevolence.</p> </blockquote> <h2 id="rmsnorm-root-mean-square-layer-normalization">RMSNorm (Root Mean Square Layer Normalization)</h2> <p>Why need a layer normalization? Internal covariate shift</p> <ul> <li>Make output of each layer have mean 0 (re-centering), variance 1 (re-scaling)</li> <li>layer Normalize ì€ row ë‹¨ìœ„ë¡œ ì ìš©ëœë‹¤</li> </ul> <p>RMS ëŠ” LayerNorm ì˜ íš¨ê³¼ê°€ mean ë³´ë‹¤ëŠ” variance ìª½ì— ê¸°ì—¬ì¹˜ê°€ ë” ë†’ì„ ê²ƒì´ë¼ ê°€ì •í•œë‹¤. ê·¸ë˜ì„œ RMSNorm ì€ í‰ê·  ê³„ì‚°ì„ í¬ê¸°í•˜ê³  variance ë§Œ ê³„ì‚°í•˜ì—¬ ì •ê·œí™”í•´ì„œ computation-efficiency ì— ì´ë“ì„ ì·¨í•œë‹¤.</p> <h2 id="rotary-positional-embedding">Rotary positional embedding</h2> <p>Rotary positional embedding ì€ relative positional embedding ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, distance ì •ë³´ë¥¼ ìƒìˆ˜ê°’ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ embedding vector ì— ì ìš©í•˜ë©´ì„œ ê³„ì‚° íš¨ìœ¨ì„ ë†’ì¸ë‹¤.</p> <p>ê·¸ ìƒìˆ˜ëŠ” complex number ë¡œ, Eulerâ€™s formula ë¥¼ ì´ìš©í•´ì„œ attention ê°’ì„ ê³„ì‚°í•œë‹¤.</p> <p>Other positional embedding methods</p> <ul> <li>Absolute: Vanilla transformers ì—ì„œ ì ìš©ëœ ë°©ë²•ìœ¼ë¡œ, Attention ê³„ì‚° ì‹œ ì´ë¯¸ ê³ ì •ëœ constant position ì •ë³´ê°€ ì ìš©ë˜ì–´ ìˆëŠ” ê° embedding vector ë¥¼ ê³„ì‚°ì— í™œìš©í•œë‹¤.</li> <li>Relative: Attention ê³„ì‚° ì‹œ, ê° embedding vector pair ë§ˆë‹¤ ìƒëŒ€ì ì¸ distance ì •ë³´ë¥¼ ë³€ìˆ˜ë¡œ í™œìš©í•˜ì—¬ ê³„ì‚°í•œë‹¤.</li> </ul> <p>Absolute, relative ì™€ ë‹¤ë¥´ê²Œ rotary positional embedding ì€ <strong>q, k weight ê°€ ë¨¼ì € ì ìš©ëœ ì´í›„</strong> ì— ì ìš©ëœë‹¤ëŠ” ì ì´ë‹¤.</p> <h2 id="grouped-multi-query-attention">Grouped Multi-Query Attention</h2> <p><strong>Multi-Query Attention</strong></p> <p>ì¼ë°˜ì ì¸ multi-head attention ì€ ê° head ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ key, value ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ë¥¼ í•˜ë‚˜ë¡œ í†µì¼í•˜ì—¬ ëª¨ë“  query ì— ë™ì¼í•œ key, value ë¥¼ ì‚¬ìš©í•œë‹¤.</p> <p>Why?</p> <ul> <li>Problem: GPU ì˜ memory bandwidth ëŠ” GPU ì˜ ê³„ì‚° ì†ë„ (FLOPS) ë³´ë‹¤ í›¨ì”¬ ëŠë¦¬ë‹¤.</li> <li>KV cache ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ, í•˜ë‚˜ì˜ í† í° query ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ë¯€ë¡œ, i/o bottleneck ì´ ë°œìƒí•œë‹¤.</li> </ul> <p><strong>Grouped Multi-Query Attention</strong></p> <p>Grouped Multi-Query ëŠ” ì¼ì • ê°œìˆ˜ì˜ ê·¸ë£¹ë§ˆë‹¤ ë™ì¼í•œ key, value ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, group ì‚¬ì´ì¦ˆê°€ 1 ì¼ ë•ŒëŠ” ì¼ë°˜ì ì¸ multi-head attention ê³¼ ë™ì¼í•˜ë‹¤.</p> <h2 id="swiglu-activation-function">SwiGLU activation function</h2> <p>SwiGLUëŠ” Swish + GLU, ë‘ê°œì˜ Activation Functionsë¥¼ ì„ì–´ ë§Œë“  í•¨ìˆ˜</p> <p><strong>Swish Function</strong></p> <ul> <li>$\sigma(x) = x \cdot \sigma(x)$ ë¡œ í‘œí˜„ëœë‹¤.</li> <li>Original Transformer ì—ì„œì˜ ReLU ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, ìŒìˆ˜ìª½ì—ì„œ 0 ì— ê°€ê¹Œì›Œì§ˆë•Œ ê¸°ìš¸ê¸°ê°€ 0 ì´ ë˜ëŠ” ë¬¸ì œ(Dying ReLU)ë¥¼ í•´ê²°í•œë‹¤.</li> </ul> <p><strong>GLU (Gated Linear Unit)</strong></p> \[GLU(a, b) = a \otimes \sigma(b)\] <p>The GLU also has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.</p> <p><strong>SwiGLU</strong></p> <p>GLU ì—ì„œ sigmoid ëŒ€ì‹  Swish Function ì„ ì‚¬ìš©í•œë‹¤.</p> \[SwiGLU(a, b) = a \otimes \text{Swish}_\beta(b)\] <p>êµ¬ì²´ì ìœ¼ë¡œëŠ” ì´ 3ê°œì˜ weight matrix ë¥¼ ì‚¬ìš©í•˜ì—¬ LLaMA ì˜ FFN ì„ êµ¬ì„±í•œë‹¤.</p> \[\text{FFN}_\text{SwiGLU}(x, W, V, W_2) = (\text{Swish}_1(xW) \otimes xV)W_2\] <p><strong>vs. ReLU</strong></p> <p>ReLU ë³´ë‹¤ëŠ” SwiGLU ê°€ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ëŠ” ëŠë‚Œì´ì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  ì—„ì²­ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ì—ì„œëŠ” ReLU ëŠ” 83.80 ì ì´ê³ , SwiGLU ëŠ” 84.36 ì  ì •ë„ë¡œ, ê±°ì˜ ì°¨ì´ê°€ ì—†ëŠ” ëŠë‚Œ.</p> <p>í•˜ì§€ë§Œ ì „ë°˜ì ì¸ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ì—ì„œ SwiGLU ìª½ì´ ìš°ìœ„ì¸ ìƒí™©.</p> <h2 id="references">References</h2> <ul> <li><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&amp;ab_channel=UmarJamil">LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</a></li> <li><a href="https://jihan819.tistory.com/entry/AI-SwiGLU%EB%8A%94-%EC%96%B4%EB%96%A4-%ED%95%A8%EC%88%98%EC%9D%BC%EA%B9%8C">SwiGLUëŠ” ì–´ë–¤ í•¨ìˆ˜ì¼ê¹Œ?</a></li> </ul>]]></content><author><name></name></author><category term="algorithm"/><category term="LLaMA"/><category term="Meta"/><summary type="html"><![CDATA[LLaMA 1, 2 ëª¨ë¸ì„ ëœ¯ì–´ë³´ë©´ì„œ ì•Œê²Œëœ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.]]></summary></entry><entry><title type="html">Semantic Retrieval at Walmart</title><link href="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/" rel="alternate" type="text/html" title="Semantic Retrieval at Walmart"/><published>2025-01-05T17:00:00+00:00</published><updated>2025-01-05T17:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/"><![CDATA[<h2 id="1-summary-of-the-proposed-method">(1) Summary of the proposed method</h2> <ol> <li>A hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries.</li> <li>A novel method of selecting negative examples for training a large neural retrieval model and an approximate metric to evaluate the performance</li> </ol> <h2 id="2-related-works">(2) Related Works</h2> <p><strong>Production search vs. Web search</strong></p> <p>Production search is way more challenging than web search.</p> <ul> <li>Product titles (the main search-able text) are generally much shorter than web documents.</li> <li>While many web documents may contain the same information, a specific product from a seller rarely has a duplicate.</li> </ul> <p><strong>Traditional Solutions</strong></p> <ul> <li>knowledge graph: need a huge amount of domain expertise, and the cost of maintaining these components is high, since the catalog and product vocabulary frequently change in e-commerce</li> <li>BM25, an inverted index: suffers from vocabulary mismatch between the query and the product title</li> <li>neural systems: limited by the fact that the embedding size cannot be too large due to latency concerns</li> </ul> <h2 id="3-proposed-methods">(3) Proposed Methods</h2> <p><strong>Reducing the size of embedding</strong></p> <p>Reducing the size of embedding is beneficial as it allows the item embedding and the ANN index to be refreshed more frequently.</p> <p>Tried 2 approaches:</p> <ol> <li>Add a linear projection layer to reduce the embedding size to 368, 256, 128, and 64</li> <li>Use a transformer architecture that has a smaller embedding size: MiniLM (12 layers and an embedding size of 368), XtremeDistil (6 layers and an embedding size of 368)</li> </ol> <p>The <strong>linear projection</strong> is very effective in reducing the size of the embedding with very little performance cost.</p> <p><strong>A hybrid architecture</strong></p> <p>â€¦</p> <p><strong>ANN</strong></p> <ul> <li>Normalized vectors of dimension 256, the ANN services can yield 99% for recall@20 evaluated against the full nearest neighborhood search, with an average latency around 13 ms;</li> <li>For normalized vectors of dimension 768, the services can achieve a similar recall@20 but with three times the storage space;</li> </ul> <h2 id="6-lesson-learned">(6) Lesson Learned</h2> <p><strong>Cosine similarity vs. Inner product</strong></p> <ul> <li>The inner product is more stable during training and does not require the temperature factor ğœ.</li> <li>But, <strong>inner product was much harder to optimize</strong> when creating the ANN index, compared to cosine similarity.</li> </ul> <p><strong>Text fields</strong></p> <p>Many text fields are generally available for each product, and the quality of the text fields varies. But, we could not extract any boost in performance by using these text fields. This is probably because descriptions can contain a lot of irrelevant text that simply adds noise.</p> <p><strong>Model Complexity</strong></p> <p>A very deep model or very large embedding size is not necessary to achieve top performance. This is probably because queries and product titles are not very complex from a semantic perspective.</p> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2412.04637">Semantic Retrieval at Walmart</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Walmart"/><category term="ANN"/><category term="RAG"/><category term="WIP"/><summary type="html"><![CDATA[(1) Summary of the proposed method]]></summary></entry><entry><title type="html">KV-Cache ì— ëŒ€í•´ ì•Œì•„ë³´ì</title><link href="https://zzong2006.github.io/blog/2025/kv-cache/" rel="alternate" type="text/html" title="KV-Cache ì— ëŒ€í•´ ì•Œì•„ë³´ì"/><published>2025-01-05T10:00:00+00:00</published><updated>2025-01-05T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/kv-cache</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/kv-cache/"><![CDATA[<p>The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).</p> <p>Therefore, when iteratively calling forward() instead of the generate() method, itâ€™s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape (batch_size, past_kv_length + new_tokens_length). This is usually handled internally when you call generate() method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.</p> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/docs/transformers/kv_cache">huggingface transformers - kv_cache</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Transformers"/><category term="LLM"/><summary type="html"><![CDATA[The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).]]></summary></entry><entry><title type="html">The collection of RAG competitions</title><link href="https://zzong2006.github.io/blog/2025/rag-competition/" rel="alternate" type="text/html" title="The collection of RAG competitions"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/rag-competition</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/rag-competition/"><![CDATA[<p>RAG ê´€ë ¨ ëŒ€íšŒ ëª©ë¡ì„ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤.</p> <hr/> <h3 id="1-financerag-challenge-at-icaif-24-ì¢…ë£Œ">1. FinanceRAG Challenge at ICAIF â€˜24 (ì¢…ë£Œ)</h3> <p>5th ACM International Conference on AI in Finance (ICAIFâ€™24)ì—ì„œ RAG ê´€ë ¨ ê²½ì§„ëŒ€íšŒë¥¼ ê°œìµœí–ˆìŠµë‹ˆë‹¤.</p> <p>ìµœê·¼ ê¸ˆìœµê¶Œì—ì„œë„ LLM ì‚¬ìš©ì— ëŒ€í•´ ë§¤ìš° ë§ì€ ê´€ì‹¬ë“¤ì„ ê°–ê³  ê³„ì‹œê³ , ë³´ë‹¤ ì •êµí•œ ì‚¬ìš©ì„ ìœ„í•´ RAGì´ ë§ì€ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤.</p> <p>ì´ë²ˆ ê²½ì§„ëŒ€íšŒëŠ” ì°¸ê°€ìë“¤ì´ ì§ì ‘ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³¼ ìˆ˜ ìˆë„ë¡ ì¤€ë¹„ í•˜ì˜€ìŠµë‹ˆë‹¤. Linqì—ì„œ ì§ì ‘ ë¯¸êµ­ì˜ ì¦ê¶Œì‹œì¥ ê³µì‹œ, ë¦¬í¬íŠ¸ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ query-corpus datasetì„ ì„¸ì‹¬í•˜ê²Œ ì¤€ë¹„í•´ì£¼ì…¨ìŠµë‹ˆë‹¤. íŠ¹íˆ ê¸ˆìœµë¶„ì•¼ì—ëŠ” ì´ëŸ¬í•œ ë°ì´í„°ì…‹ì´ ì—†ëŠ”ë°ìš”, ì°¸ê°€ìë“¤ì´ ë§¤ìš° ì¢‹ì€ ë°ì´í„°ì…‹ì„ ë°”íƒ•ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë³´ê³  evaluationë„ í•´ë³¼ ìˆ˜ ìˆëŠ” ë§¤ìš° ì¢‹ì€ ê¸°íšŒë¼ê³  ìƒê°í•©ë‹ˆë‹¤.</p> <p>https://www.kaggle.com/competitions/icaif-24-finance-rag-challenge/overview</p> <h3 id="2-dacon-ì¬ì •ì •ë³´-ai-ê²€ìƒ‰-ì•Œê³ ë¦¬ì¦˜-ê²½ì§„ëŒ€íšŒ-ì¢…ë£Œ">2. Dacon: <a href="https://dacon.io/competitions/official/236295/overview/description">ì¬ì •ì •ë³´ AI ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ê²½ì§„ëŒ€íšŒ</a> (ì¢…ë£Œ)</h3> <p>ëŒ€íšŒ ê¸°ê°„: 2024ë…„ 7ì›” 29ì¼ ~ 2024ë…„ 8ì›” 23ì¼</p> <p>íŠ¹ì´í•œì ì€ RAG ë¡œ í™œìš©í•  ë°ì´í„°ë¥¼ PDF í˜•íƒœë¡œ ì œê³µí•œë‹¤ëŠ” ì ì´ë‹¤.</p> <p>ë¬¼ë¡  LLM í•™ìŠµì— í•„ìš”í•œ Question-Answer pair ì—­ì‹œ ì œê³µëœë‹¤.</p> <p>ëŒ€ë¶€ë¶„ e5 ëª¨ë¸ë¡œ vector search ë¥¼ ìœ„í•œ ì„ë² ë”©ì„ ì§„í–‰í•˜ì˜€ë‹¤.</p> <p>í•œ ìš°ìŠ¹íŒ€ì—ì„œëŠ” YOLOv10 ê¸°ë°˜ Document Layout Analysisë¥¼ í†µí•´ PDF parserë¥¼ êµ¬í˜„í•˜ì˜€ë‹¤ê³  í•œë‹¤.</p> <h3 id="3-aicrowd-crag-comprehensive-rag-benchmark-ì¢…ë£Œ">3. AiCrowd: <a href="https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024">CRAG: Comprehensive RAG Benchmark</a> (ì¢…ë£Œ)</h3> <p>ëŒ€íšŒ ê¸°ê°„: 2024ë…„ 5ì›” 20ì¼ ~ 2025ë…„ 1ì›” 25ì¼</p>]]></content><author><name></name></author><category term="RAG"/><category term="competition"/><summary type="html"><![CDATA[RAG ê´€ë ¨ ëŒ€íšŒ ëª©ë¡ì„ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤.]]></summary></entry><entry><title type="html">Donâ€™t do RAG</title><link href="https://zzong2006.github.io/blog/2025/no-rag/" rel="alternate" type="text/html" title="Donâ€™t do RAG"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/no-rag</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/no-rag/"><![CDATA[<h2 id="quick-summary">Quick Summary</h2> <p>RAGëŠ” ê²€ìƒ‰ ì§€ì—°, ë¬¸ì„œ ì„ íƒ ì˜¤ë¥˜, ì‹œìŠ¤í…œ ë³µì¡ì„± ì¦ê°€ ê°™ì€ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¨ë‹¤. ìµœê·¼ LLMì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ í¬ê²Œ ëŠ˜ì–´ë‚˜ë©´ì„œ, ì‹¤ì‹œê°„ ê²€ìƒ‰ ì „ëµì¸ ìºì‹œ ì¦ê°• ìƒì„±(CAG)ì´ ì œì•ˆëœë‹¤.</p> <p>CAGëŠ” ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì— ëª¨ë“  ê´€ë ¨ ë¦¬ì†ŒìŠ¤ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•´ì„œ KV-cache í˜•íƒœë¡œ ì ì¬í•˜ê³ , ì¸í¼ëŸ°ìŠ¤ ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ì´ ìºì‹œë¥¼ ì‚¬ìš©í•´ì„œ ì¶”ê°€ì ì¸ ê²€ìƒ‰ ê³¼ì • ì—†ì´ ì¿¼ë¦¬ì— ë‹µë³€í•  ìˆ˜ ìˆë‹¤.</p> <p>CAGëŠ” ê²€ìƒ‰ ì§€ì—°ì„ ì¤„ì´ê³  ê²€ìƒ‰ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œë„ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ CAGê°€ ê¸°ì¡´ RAGë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  ì£¼ì¥í•œë‹¤.</p> <p>ì œí•œëœ ì§€ì‹ ê¸°ë°˜ì„ ê°€ì§„ ê²½ìš°, CAGê°€ RAGë³´ë‹¤ ë” ê´œì°®ì€ ì„ íƒì§€ë¡œ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì™œëƒí•˜ë©´, CAG ëŠ” í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ëª¨ë¸ì˜ context ì•ˆì— ìºì‹œ í˜•íƒœë¡œ ë°€ì–´ ë„£ê¸° ë•Œë¬¸ì— ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ìƒí™©ì´ë¼ë©´ ì ì ˆí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ LLM ì´ long context prompt ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ ë‹¨ì ì„ ê³ ë ¤í•´ì„œ CAG ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.</p> <h2 id="implementation">Implementation</h2> <p>ì €ìì˜ ì½”ë“œë¥¼ í™•ì¸í•´ë³´ë‹ˆ ì•„ë˜ì™€ ê°™ì´ kv cache ë¥¼ ìƒì„±í•œë‹¤.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers.cache_utils</span> <span class="kn">import</span> <span class="n">DynamicCache</span>

<span class="c1"># Initialize dynamic cache
</span><span class="n">past_key_values</span> <span class="o">=</span> <span class="nc">DynamicCache</span><span class="p">()</span>

<span class="c1"># Generate KV cache with proper device placement
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

<span class="c1"># The model's device mapping will automatically place each layer's
# KV cache on the correct device
</span><span class="k">return</span> <span class="n">outputs</span><span class="p">.</span><span class="n">past_key_values</span>
</code></pre></div></div> <p>ìƒì„±í•œ kv cache ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•œë°, vllm ê°™ì€ framework ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  naive í•œ iteration ì„ í†µí•´ì„œ ì§„í–‰í•œë‹¤.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Main generation loop
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="c1"># Forward pass with proper device placement
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">next_token</span><span class="p">,</span>  <span class="c1"># Only process last token
</span>        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>

    <span class="c1"># Get next token prediction (logits will be on the last device)
</span>    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token_logits</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="bp">...</span>

    <span class="c1"># Update KV cache
</span>    <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">past_key_values</span>

    <span class="c1"># Append prediction
</span>    <span class="n">output_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">output_ids</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="experiment">Experiment</h2> <h3 id="baselines">Baselines</h3> <ul> <li>Sparse Retrieval System (BM25): ì¼ë°˜ì ì¸ BM25. It ranks documents based on term frequency inverse document frequency (TF-IDF) and document length normalization.</li> <li>Dense Retrieval System (OpenAI Indexes): LlamaIndex framework ì— openai Embedding ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•í•œ vector search system.</li> </ul> <h3 id="other-settings">Other Settings</h3> <ul> <li>Metrics: BERT-Score</li> <li>Datasets: SQuAD, HotpotQA</li> </ul> <h3 id="results">Results</h3> <ul> <li>ì„±ëŠ¥: CAG ì™€ ê¸°ì¡´ Sparse, Dense RAG ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ë°, CAG ê°€ ì•„ì£¼ ì•½ì†Œí•˜ê²Œ ì¢‹ìŒ</li> <li>ì†ë„: CAG ê°€ ì••ë„ì ìœ¼ë¡œ ë¹ ë¥¸í¸ì´ë‹¤. ë°ì´í„° ì‚¬ì´ì¦ˆë¥¼ small, medium, large ë¡œ ë‚˜ëˆ ì„œ ëª¨ë¸ generation ì†ë„ë¥¼ ë¹„êµí•˜ëŠ”ë°, ì‚¬ì´ì¦ˆê°€ í´ìˆ˜ë¡ 10ë°°ì´ìƒ ë¹¨ë¼ì§„ë‹¤ (e.g. 2ì´ˆ vs. 100ì´ˆ).</li> </ul> <h2 id="ëŠë‚€ì ">ëŠë‚€ì </h2> <p>í‰ê°€ ì§„í–‰ì— ìˆì–´ì„œ ì¢€ ì•„ì‰¬ìš´ ì ë“¤ì´ ìˆë‹¤.</p> <ol> <li>BERT-Score ì™¸ì— Rouge, GPT-eval ê°™ì€ ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¡œ ì§„í–‰í•˜ë©´ ì¢‹ì•˜ì„ ê²ƒ ê°™ìŒ.</li> <li>CAG ëŠ” KV-cache ë¥¼ ìƒì„±í•˜ëŠ” ì†ë„ê¹Œì§€ ê³ ë ¤í•´ì•¼ ë” ê³µí‰í•œ ê²°ê³¼ì¼ ê²ƒì´ë‹¤.</li> </ol> <p>ì´ ì ‘ê·¼ì— ì°©ì•ˆí•´ì„œ ìƒê°í•´ë´¤ëŠ”ë° KV-cache ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ê³ , í•´ë‹¹ cache ë¥¼ ê²€ìƒ‰í•˜ëŠ” RAG ë¥¼ ë§Œë“œëŠ”ê²ƒë„ í•˜ë‚˜ì˜ ì ‘ê·¼ë²•ì¼ìˆ˜ë„ ìˆì„ê²ƒ ê°™ë‹¤.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/abs/2412.15605">paper (arxiv)</a></li> <li><a href="https://github.com/hhhuang/cag">code (github)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="LLM"/><summary type="html"><![CDATA[Quick Summary]]></summary></entry><entry><title type="html">í–‰ë ¬ ë¯¸ë¶„ ê¸°ì´ˆ (with Trace)</title><link href="https://zzong2006.github.io/blog/2025/matrix-derivative/" rel="alternate" type="text/html" title="í–‰ë ¬ ë¯¸ë¶„ ê¸°ì´ˆ (with Trace)"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/matrix-derivative</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/matrix-derivative/"><![CDATA[<p>í–‰ë ¬ì˜ ëŒ€ê° ì„±ë¶„ì˜ í•©ì¸ Trace ($ \text{Tr} $) ê°€ í¬í•¨ëœ í–‰ë ¬ ë¯¸ë¶„ ê·œì¹™ì— ëŒ€í•´ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤.</p> <hr/> <h2 id="ê·œì¹™-1--fracpartialpartial-w_d-texttra-w_d--atop-">ê·œì¹™ 1: $ \frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top $</h2> <p>ì—¬ê¸°ì„œ $ A $ëŠ” $ m \times n $ í–‰ë ¬ì´ê³  $ W_d $ëŠ” $ n \times m $ í–‰ë ¬ì´ë¼ê³  ê°€ì •í•œë‹¤.</p> <p>í–‰ë ¬ ë¯¸ë¶„ì˜ ê²°ê³¼ëŠ” ë¯¸ë¶„í•œ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ë”°ë¼ì•¼ í•˜ë¯€ë¡œ, ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ì„œ transpose ë¥¼ ì·¨í•œë‹¤ê³  ìƒê°í•˜ë©´ í¸í•˜ë‹¤.</p> <p>(ì°¸ê³ ) Trace ëŠ” ì •ì‚¬ê°í–‰ë ¬ì— ëŒ€í•´ì„œë§Œ ì •ì˜ëœë‹¤.</p> <h3 id="ì˜ˆì‹œ">ì˜ˆì‹œ</h3> <p>ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì.</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix}, \quad W_d = \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix}\] <p>$ \text{Tr}(A W_d) $ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤.</p> \[\text{Tr}(A W_d) = \text{Tr}\left( \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix} \right) \\[10pt] = \text{Tr}\left( \begin{bmatrix} 1x + 2z &amp; 1y + 2w \\ 3x + 4z &amp; 3y + 4w \\ \end{bmatrix} \right) \\[10pt] = (1x + 2z) + (3y + 4w)\] <p>ì´ì œ $ W_d $ ì˜ ê° ì›ì†Œì— ëŒ€í•´ í¸ë¯¸ë¶„ì„ ê³„ì‚°í•´ë³´ì.</p> <ul> <li>$ \frac{\partial}{\partial x} \text{Tr}(A W_d) = 1 $</li> <li>$ \frac{\partial}{\partial y} \text{Tr}(A W_d) = 3 $</li> <li>$ \frac{\partial}{\partial z} \text{Tr}(A W_d) = 2 $</li> <li>$ \frac{\partial}{\partial w} \text{Tr}(A W_d) = 4 $</li> </ul> <p>Trace ì—°ì‚°ì€ ìŠ¤ì¹¼ë¼ ê°’ìœ¼ë¡œ ê³„ì‚°ë˜ì§€ë§Œ, ì´ë¥¼ í–‰ë ¬ $ W_d $ ì˜ ì›ì†Œ ê°ê°ì— ëŒ€í•´ í¸ë¯¸ë¶„í•˜ë©´, ì›ì†Œë³„ ë³€í™”ìœ¨ì„ í¬í•¨í•œ ê²°ê³¼ê°€ í–‰ë ¬ í˜•íƒœë¡œ ë‚˜íƒ€ë‚œë‹¤. ê·¸ ê²°ê³¼, ì•„ë˜ì™€ ê°™ì´ transpose ëœ Aê°€ ë‚˜ì˜¨ë‹¤.</p> \[\frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top = \begin{bmatrix} 1 &amp; 3 \\ 2 &amp; 4 \\ \end{bmatrix}\]]]></content><author><name></name></author><category term="math"/><category term="matrix"/><category term="calculus"/><summary type="html"><![CDATA[í–‰ë ¬ì˜ ëŒ€ê° ì„±ë¶„ì˜ í•©ì¸ Trace ($ \text{Tr} $) ê°€ í¬í•¨ëœ í–‰ë ¬ ë¯¸ë¶„ ê·œì¹™ì— ëŒ€í•´ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤.]]></summary></entry><entry><title type="html">Text Embedding ëª¨ë¸: E5</title><link href="https://zzong2006.github.io/blog/2025/e5/" rel="alternate" type="text/html" title="Text Embedding ëª¨ë¸: E5"/><published>2025-01-02T00:00:00+00:00</published><updated>2025-01-02T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/e5</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/e5/"><![CDATA[<h1 id="original-e5">Original E5</h1> <h2 id="introduction">Introduction</h2> <p>ê¸°ì¡´ì˜ BERT ì™€ GPT ê°™ì€ ëª¨ë¸ì„ ì´ìš©í•´ (ì¶”ê°€ í•™ìŠµ ì—†ì´) text representation ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìœ¼ë‚˜, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ contrastive learning ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì§€ ì•Šë‹¤ë³´ë‹ˆ embedding í’ˆì§ˆì´ ìƒëŒ€ì ìœ¼ë¡œ ì•„ì‰¬ìš¸ ìˆ˜ ìˆë‹¤.</p> <p>a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings</p> <p>two prefix identifiers â€œquery:â€ and â€œpassage:â€ to $q$ and $d$</p> <h2 id="datasets-ccpairs">Datasets: CCPairs</h2> <p>1.3B unfiltered</p> <p>pair (query, passage) ì˜ˆì‹œ</p> <ul> <li>reddit: (post, comment)</li> <li>stackoverflow: (question, upvoted answer)</li> <li>wikipedia: (title, abstract)</li> </ul> <h3 id="a-consistency-based-data-filtering-technique">A consistency-based data filtering technique</h3> <p>ì‹ ê²½ë§ ëª¨ë¸ì´ clean label ì„ ë¨¼ì € í•™ìŠµí•˜ê³  ê·¸ ì´í›„ì— noisy label ì„ í•™ìŠµí•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë¯€ë¡œ, ì´ëŸ¬í•œ ê²½í–¥ì„ í™œìš©í•˜ì—¬ ì¼ê´€ì„± ê¸°ë°˜ ë°ì´í„°ì…‹ í•„í„°ë§ ê¸°ë²•ì„ ì ìš©</p> <p>ì•„ë˜ì˜ ë°©ë²•ì„ í†µí•´ 1.3B text pairs ë¥¼ 270M ê°œë¡œ ì¤„ì„</p> <ol> <li>ìš°ì„  1.3B ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì„ í•™ìŠµ</li> <li>í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•´ query ì™€ (ì •ë‹µì´ í¬í•¨ëœ) 1M random passages ì— ëŒ€í•œ ì—°ê´€ì„±ì„ ë­í‚¹</li> <li>Top-k(=2) ê°œì˜ passage ì•ˆì— ì‹¤ì œ ì •ë‹µì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸</li> <li>ë§Œì•½ ì •ë‹µì´ í¬í•¨ë˜ë©´ pair ë¥¼ keep, ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ discard</li> </ol> <h2 id="training">Training</h2> <h3 id="contrastive-loss-for-pre-training">Contrastive loss (for pre-training)</h3> <p>ì•„ë˜ì™€ ê°™ì€ InfoNCE contrastive loss ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ</p> \[\min L_{\mathrm{cont}}=-\frac{1}{n} \sum_i \log \frac{\mathrm{e}^{s_\theta\left(q_i, p_i\right)}}{\mathrm{e}^{s_\theta\left(q_i, p_i\right)}+\sum_j \mathrm{e}^{s_\theta\left(q_i, p_{i j}^{-}\right)}}\] <ul> <li>$s_\theta$: ëª¨ë¸ íŒŒë¼ë¯¸í„° $\theta$ ì— ê¸°ë°˜í•œ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜</li> <li>$q_i$: query</li> <li>$p_i$: a positive passage</li> <li>$p_{i j}^{-}$: negative passages</li> </ul> <p>ì‹¤ì œ êµ¬í˜„ì€ sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í–ˆëŠ”ë°, ì°¾ì•„ë³¸ ê²°ê³¼ <a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py#L13-L125">MultipleNegativesRankingLoss</a> ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.</p> <h3 id="knowledge-distillation-for-finetuning">Knowledge distillation (for finetuning)</h3> <p>knowledge distillation from a cross-encoder (CE) teacher model</p> <p>KL divergence $D_{\mathrm{KL}}$ for distilling soft labels from the teacher model</p> \[\min D_{\mathrm{KL}}\left(p_{\mathrm{ce}}, p_{\mathrm{stu}}\right)+\alpha L_{\mathrm{cont}}\] <ul> <li>$p_{\mathrm{ce}}$, $p_{\mathrm{stu}}$: probability from the cross-encoder (CE) teacher and student model respectively</li> <li>$\alpha$: a hyperparameter to balance the two loss functions</li> <li>$L_{\mathrm{cont}}$: contrastive loss</li> </ul> <p>ì—¬ê¸°ì„œëŠ” SimLM ëª¨ë¸ì„ cross-encoder (teacher model)ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•œë‹¤.</p> <h1 id="multilingual-e5">Multilingual E5</h1> <p>e5-multilingual ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‘ ë‹¨ê³„ì˜ í•™ìŠµ ì „ëµì„ í†µí•´ í•™ìŠµë˜ì—ˆë‹¤.</p> <h2 id="training-two-stage-methodology">Training: Two-stage methodology</h2> <ol> <li>Weakly-supervised contrastive pre-training on billions of text pairs</li> <li>Supervised finetuning on small quantity of high-quality labeled data</li> </ol> <h3 id="1-weakly-supervised-contrastive-pre-training">(1) Weakly-supervised contrastive pre-training</h3> <p>ì•½ 1B ì •ë„ì˜ ë‹¤êµ­ì–´ ë°ì´í„°ì…‹(text pairs)ì„ ì´ìš©í•˜ì—¬ í•™ìŠµí–ˆë‹¤.</p> <p><img src="https://i.imgur.com/hNlpaUx.png" alt="Image" width="45%"/></p> <p>í•™ìŠµ loss ì˜ ê²½ìš°, in-batch negatives ê°€ ì ìš©ëœ InfoNCE contrastive loss ë¥¼ ì‚¬ìš©í–ˆë‹¤. ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì˜ì–´ ì „ìš© e5 ëª¨ë¸ì„ í•™ìŠµí• ë•Œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •í•˜ì˜€ë‹¤ê³  í•œë‹¤.</p> <p>ì°¸ê³ ë¡œ, In-batch negative ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œ ë°°ì¹˜(batch) ë‚´ì˜ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ì„ ë¶€ì • ìƒ˜í”Œ(negative sample)ë¡œ í™œìš©í•˜ëŠ” ê¸°ë²•ì´ë‹¤. í•´ë‹¹ ë°©ì‹ì— ëŒ€í•´ ëª‡ê°€ì§€ ì˜ê²¬ì´ ìˆë‹¤ë©´â€¦</p> <ul> <li>ì´ëŠ” ë³„ë„ì˜ negative sampling ì‘ì—…ì—†ì´ í˜„ì¬ ë°°ì¹˜ì— í¬í•¨ëœ ë°ì´í„°ë§Œìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì–´ íš¨ìœ¨ì ì´ë‹¤.</li> <li>ë˜í•œ, ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì¶©ë¶„íˆ í¬ë‹¤ë©´ ì´ëŸ° ë‹¨ìˆœí•œ ì „ëµì´ ë‹¤ë¥¸ ë°©ì‹ë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ì´ê³  ì•ˆì •ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤ê³  ë§í•œë‹¤.</li> <li>ë¬¼ë¡ , batch size ë¥¼ ì¤„ì´ê³  hard negatives ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒë„ ë‚˜ì˜ì§„ ì•Šì§€ë§Œ, 100M ì´ìƒì˜ í° ë°ì´í„°ì…‹ì—ì„œ hard negatives ë¥¼ ì°¾ì•„ë‚´ëŠ”ê±´ ì‰½ì§€ ì•Šë‹¤ (non-trivial).</li> </ul> <h3 id="2-supervised-finetuning">(2) Supervised finetuning</h3> <p>ì´ ë‹¨ê³„ì—ì„œëŠ” in-batch negatives ì „ëµ ì™¸ì—ë„, mined hard negatives ì™€ êµì°¨ ì¸ì½”ë” ëª¨ë¸(cross-encoder model)ë¡œë¶€í„°ì˜ ì§€ì‹ ì¦ë¥˜(knowledge distillation)ë¥¼ ì „ëµì„ ì¶”ê°€ë¡œ í™œìš©í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆì„ ë”ìš± í–¥ìƒì‹œì¼°ë‹¤.</p> <p><strong>Instruction model</strong> ì˜ ê²½ìš°, Reference [2] ì—ì„œ ì‚¬ìš©í–ˆë˜ GPT-3/4 ê¸°ë°˜ í•©ì„± ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ embedding model ì— ëŒ€í•´ instruction íŠœë‹ì„ ì§„í–‰í•˜ì˜€ë‹¤.</p> <h2 id="training-hyper-parameters">Training: hyper-parameters</h2> <p>mE5 (multilingual E5) ëª¨ë¸ì€ ê°ê° ë‹¤ìŒê³¼ ê°™ì€ ëª¨ë¸ë¡œë¶€í„° ì´ˆê¸°í™”ë˜ì—ˆë‹¤.</p> <ol> <li>mE5small: multilingual MiniLM</li> <li>mE5base: xlm-roberta-base</li> <li>mE5large: xlm-roberta-large</li> </ol> <p>Learning Rate ì˜ ê²½ìš° ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆìˆ˜ë¡ ë” ë‚®ê²Œ ì„¤ì •í–ˆë‹¤.</p> <ul> <li>Pretraining: small, base, large ê°ê° {3, 2, 1}Ã—10â»â´</li> <li>Finetuning: small, base, large ê°ê° {3, 2, 1}Ã—10â»âµ</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>MTEB benchmark</li> <li>MIRACL multilingual retrieval benchmark (MAP, nDCG)</li> <li>Bitext mining</li> </ul> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2402.05672">Multilingual E5 Text Embeddings: A Technical Report</a></li> <li>[2] <a href="https://arxiv.org/pdf/2402.05672">Improving Text Embeddings with Large Language Models</a></li> <li>[3] <a href="https://arxiv.org/abs/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></li> </ul> <p>Code</p> <ul> <li>e5-evaluation: https://github.com/microsoft/unilm/tree/master/e5</li> <li>sentence-transformers: https://github.com/UKPLab/sentence-transformers</li> </ul> <p>Models</p> <ul> <li>í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸: https://github.com/nlpai-lab/KURE</li> </ul> <p>Blog</p> <ul> <li><a href="https://yjoonjang.medium.com/koe5-%EC%B5%9C%EC%B4%88%EC%9D%98-%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%AA%A8%EB%8D%B8-multilingual-e5-finetune-22fa7e56d220">KURE: ìµœì´ˆì˜ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Microsoft"/><category term="encoder"/><summary type="html"><![CDATA[Original E5]]></summary></entry><entry><title type="html">ìƒˆë¡œìš´ Bert ëª¨ë¸: ModernBERT</title><link href="https://zzong2006.github.io/blog/2025/modern-bert/" rel="alternate" type="text/html" title="ìƒˆë¡œìš´ Bert ëª¨ë¸: ModernBERT"/><published>2025-01-01T00:00:00+00:00</published><updated>2025-01-01T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/modern-bert</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/modern-bert/"><![CDATA[<p>ê¸°ì¡´ BERT family ë“¤ì˜ ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ ëª¨ë‘ ì´ê¸¸ ìˆ˜ ìˆëŠ” ModernBERT ê°€ ë‚˜ì™”ë‹¤ê³  í•œë‹¤.</p> <p>ì—¬ê¸°ì„œ ì„±ëŠ¥ì´ë¼í•˜ë©´ GLUE score ë¥¼ ì˜ë¯¸í•˜ê³ , ì†ë„ë¼ í•˜ë©´ ì´ˆë‹¹ í† í° ì²˜ë¦¬ë¥¼ ì˜ë¯¸í•œë‹¤. ì•„ë˜ ê·¸ë˜í”„ë¥¼ ì°¸ê³ í•˜ì.</p> <p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/modernbert/modernbert_pareto_curve.png" style="width: 85%;"/></p> <h2 id="íŠ¹ì§•">íŠ¹ì§•</h2> <p>ModernBERT ëŠ” ê¸°ì¡´ì˜ BERT ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ê°€ì§€ê³  ìˆë‹¤.</p> <ol> <li>8k í† í° ì‚¬ì´ì¦ˆ ì§€ì›: ëŒ€ë¶€ë¶„ì˜ BERT ì‹œë¦¬ì¦ˆëŠ” ìµœëŒ€ 512 í† í° ì‚¬ì´ì¦ˆë¥¼ ì§€ì›í–ˆë‹¤.</li> <li>ë©”ëª¨ë¦¬ íš¨ìœ¨ì : Kaggle ì—ì„œ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” DeBERTaV3 ëª¨ë¸ë³´ë‹¤ 1/5 ì •ë„ ë©”ëª¨ë¦¬ë§Œ í•„ìš”í•˜ë‹¤.</li> <li>ì¶”ë¡  ì†ë„ê°€ ë¹ ë¦„: DeBERTa ëª¨ë¸ë³´ë‹¤ ê¸°ë³¸ì ìœ¼ë¡œ 2ë°° ë¹ ë¥´ë©°, mixed length ì¸ ì¼€ì´ìŠ¤ì—ì„œëŠ” ìµœëŒ€ 4ë°° ë” ë¹ ë¥´ë‹¤.</li> </ol> <p>ê·¸ ì™¸ì—ë„ í•™ìŠµ ë°ì´í„°ì…‹ ì‚¬ì´ì¦ˆê°€ 2T token ì´ë¼ëŠ” íŠ¹ì§•ì´ ìˆë‹¤.</p> <p>ì•„ë˜ì˜ ì„¸ê°€ì§€ ë°©ë²•ë¡ ì„ ì ìš©í•´ì„œ ModernBERT ë¥¼ ë§Œë“¤ì—ˆë‹¤ê³  í•œë‹¤.</p> <ol> <li>í˜„ëŒ€í™”ëœ transformer êµ¬ì¡°</li> <li>Particular attention to efficiency</li> <li>í•™ìŠµ ë°ì´í„°ì…‹ì„ í˜„ëŒ€ì ìœ¼ë¡œ ì¬êµ¬ì„±</li> </ol> <h2 id="recipe-1-transformer-êµ¬ì¡°-ì—…ë°ì´íŠ¸">(Recipe 1) Transformer êµ¬ì¡° ì—…ë°ì´íŠ¸</h2> <p>Llama2 íŒ¨ë°€ë¦¬ ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ Transformer++ êµ¬ì¡°ì— ì˜ê°ì„ ì–»ì–´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í–ˆë‹¤ê³  í•œë‹¤.</p> <ul> <li>Old positional encoding ì„ RoPE(rotary positional embeddings) ë¡œ ëŒ€ì²´</li> <li>ê¸°ì¡´ BERT ì˜ GeLU activation function ì´ í¬í•¨ëœ MLP layers ë¥¼ GeGLU layers ë¡œ êµì²´</li> <li>ë¶ˆí•„ìš”í•œ bias term ì„ ì œê±°í•´ì„œ parameter ìˆ˜ë¥¼ ì¤„ì´ëŠ”ë° ê¸°ì—¬</li> <li>ì„ë² ë”© ë ˆì´ì–´ ì´í›„ì— normalization layer ë¥¼ ì¶”ê°€</li> </ul> <h2 id="recipe-2-attention-ë°©ì‹-ë³€ê²½ìœ¼ë¡œ-íš¨ìœ¨ì„±-ë†’ì´ê¸°">(Recipe 2) Attention ë°©ì‹ ë³€ê²½ìœ¼ë¡œ íš¨ìœ¨ì„± ë†’ì´ê¸°</h2> <p>Flash Attention 2 ë¥¼ í™œìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í–ˆë‹¤.</p> <ol> <li>Attention ë³€ê²½: local and global</li> <li>Sequence Packing ì ìš©</li> <li>í•˜ë“œì›¨ì–´ ë°ì¶¤í˜• ëª¨ë¸ ë””ìì¸</li> </ol> <h3 id="2-1-local-and-global-attention">2-1. Local and Global Attention</h3> <p>ê¸°ì¡´ BERT ì—ì„œëŠ” ëª¨ë“  ë ˆì´ì–´ì—ì„œ ëª¨ë“  input token ë“¤ì— ëŒ€í•´ attention ë¡œì§ì„ ì ìš©í–ˆë‹¤. ì´ë¥¼ full attention ì´ë¼ê³  í‘œí˜„í•´ë³´ì.</p> <p>ModernBert ì—ì„œëŠ” full attention ë°©ì‹ì„ global ê³¼ local attention ìœ¼ë¡œ ë‚˜ëˆ´ë‹¤.</p> <ul> <li>global attention: ë§¤ 3ë²ˆì§¸ layer ì—ì„œ ëª¨ë“  input token ë“¤ì— ëŒ€í•´ attention ë¡œì§ì„ ì ìš©</li> <li>local attention: ë‚˜ë¨¸ì§€ layer ì—ì„œ sliding window ë°©ì‹ìœ¼ë¡œ <strong>ì¼ë¶€ input token ë“¤</strong>ì— ëŒ€í•´ attention ë¡œì§ì„ ì ìš© (window size: 128)</li> </ul> <p>ì´ëŠ” ë§ˆì¹˜ ì±…ì„ ì½ì„ë•Œì™€ ë¹„ìŠ·í•œë°, ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ì„œ ì§‘ì¤‘í•˜ëŠ” ê²ƒ(full attention) ê³¼ ì£¼ìš” ì¤„ê±°ë¦¬ì™€ ì¼ë¶€ ë¬¸ì¥ì— ëŒ€í•´ì„œë§Œ ì§‘ì¤‘í•˜ëŠ” ê²ƒ(global and local attention) ì˜ ê°œë…ì  ì°¨ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.</p> <p>Attention ì˜ ê³„ì‚° ë³µì¡ë„ëŠ” í† í° ì‚¬ì´ì¦ˆì— ë”°ë¼ì„œ ê¸‰ì¦í•˜ë¯€ë¡œ, modernBert ëŠ” ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ì²˜í•  ìˆ˜ ìˆë‹¤.</p> <h3 id="2-2-sequence-packing">2-2. Sequence Packing</h3> <p>ì¼ë°˜ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ sequence length ë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” padding ì„ ì ìš©í•´ì•¼ í•œë‹¤.</p> <p>í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ ë‚­ë¹„ë˜ëŠ” íŒ¨ë”© í† í°ì— ì˜í•´ì„œ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°œìƒì‹œí‚¤ê³ , íŒ¨ë”© í† í° ì—­ì‹œ ì–´ë– í•œ ì˜ë¯¸ë¡ ì  ê¸°ì—¬ë¥¼ í•˜ì§€ë„ ì•ŠëŠ”ë‹¤.</p> <p>ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, ModernBert ëŠ” padding ì„ ë¶™ì´ì§€ ì•ŠëŠ” Sequence packing ë°©ì‹ì„ ì ìš©í–ˆë‹¤.</p> <p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/modernbert/modernbert_unpadding.png" style="width: 100%;"/></p> <p>Sequence packing ì˜ íš¨ê³¼ë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´, ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œ model max-length ì™€ ìµœëŒ€í•œ ê°€ê¹Œìš´ sequence ê¸¸ì´ì˜ ìˆœì„œë¡œ packing ì„ ì ìš©í•˜ì—¬ ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì˜€ë‹¤ê³  í•œë‹¤.</p> <h3 id="2-3-í•˜ë“œì›¨ì–´-ë°ì¶¤í˜•-ëª¨ë¸-ë””ìì¸">2-3. í•˜ë“œì›¨ì–´ ë°ì¶¤í˜• ëª¨ë¸ ë””ìì¸</h3> <p>ì—°êµ¬ì— ë”°ë¥´ë©´ deep &amp; narrow layer ì¡°í•©ì´ wide &amp; shallow layer ì¡°í•©ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ë‹¤. í•˜ì§€ë§Œ trade-off ê°€ ë°œìƒí•˜ëŠ”ë°, ëª¨ë¸ layer ê°€ ê¹Šì„ìˆ˜ë¡ ë³‘ë ¬í™”ê°€ ì¤„ì–´ë“¤ê³  ë”°ë¼ì„œ ë™ì¼í•œ íŒŒë¼ë§¤í„° ê°œìˆ˜ë¼ë„ ì†ë„ê°€ ëŠë ¤ì§€ê²Œ ëœë‹¤.</p> <p>ì¼ë°˜ì ì¸ GPU ì‚¬ì–‘ì´ RTX 3090/4090 ì •ë„ ê¸‰ì´ë¼ëŠ”ê±¸ ê°€ì •í•˜ê³ , ì œí•œëœ grid search ë¥¼ í†µí•´ì„œ ìµœì ì˜ ëª¨ë¸ ë””ìì¸ì„ ì°¾ì•˜ë‹¤ê³  í•œë‹¤.</p> <p>ê·¸ ê²°ê³¼, base ëŠ” 150M ì •ë„, large ëŠ” 400M ì •ë„ í¬ê¸°ì˜ ëª¨ë¸ì´ ë˜ì—ˆìœ¼ë©°, ì„ë² ë”© í¬ê¸°ëŠ” 768 (base) ì™€ 1024 (large) ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p> <h2 id="recipe-3-í•™ìŠµ-ë°ì´í„°ì…‹-ì¬êµ¬ì„±">(Recipe 3) í•™ìŠµ ë°ì´í„°ì…‹ ì¬êµ¬ì„±</h2> <p>ê¸°ì¡´ì˜ DeBERTaV3 ê°™ì€ ëª¨ë¸ë“¤ì€ Wikipedia and Wikibooks ì™€ ê°™ì´ ê³ í’ˆì§ˆ ë°ì´í„°ë§Œ í•™ìŠµì…‹ìœ¼ë¡œ êµ¬ì„±í•œ ë°ì´í„°ë¡œë§Œ í•™ìŠµí–ˆë‹¤ (single text modality).</p> <p>í•˜ì§€ë§Œ modernBert ëŠ” web documents, code, and scientific articles ê°™ì€ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë°ì´í„°ë„ í•™ìŠµ ë°ì´í„°ì…‹ì„ í™œìš©í•œë‹¤.</p> <p>ì´ëŸ° í•™ìŠµ ë°ì´í„°ì…‹ì˜ ë³€í™”ëŠ” ModernBERT ê°€ í”„ë¡œê·¸ë˜ë° ê´€ë ¨ task ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆê² ë‹¤.</p> <h2 id="í•™ìŠµ-í”„ë¡œì„¸ìŠ¤">í•™ìŠµ í”„ë¡œì„¸ìŠ¤</h2> <p>ê¸°ì¡´ì˜ BERT í•™ìŠµ ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, ë³„ íš¨ê³¼ê°€ ì—†ëŠ” next-sentence prediction ëª©ì í•¨ìˆ˜ë¥¼ ì œê±°í•˜ê³ , masking rate ë¥¼ 15% ì—ì„œ 30% ë¡œ ì¦ê°€ì‹œì¼°ë‹¤.</p> <p>ê·¸ë¦¬ê³  2T ë°ì´í„°ì…‹ì„ í•œë²ˆì— í•™ìŠµí•˜ì§€ ì•Šê³ , ì´ 3 ë‹¨ê³„ì˜ í•™ìŠµì„ ê±°ì³¤ë‹¤.</p> <ol> <li>1.7T ì˜ ë°ì´í„°ëŠ” 1024 í† í° ì‚¬ì´ì¦ˆë¡œ í•™ìŠµ</li> <li>ë‚˜ë¨¸ì§€ 250B í† í°ì€ 8192 í† í° ì‚¬ì´ì¦ˆë¡œ í•™ìŠµ</li> <li>ë§ˆì§€ë§‰ìœ¼ë¡œ ë‚˜ë¨¸ì§€ 50B í† í°ì€ ê¸¸ì´ë¥¼ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµ</li> </ol> <p>ì´ëŠ” ProLong ì´ë¼ëŠ” ê²ƒì—ì„œ ì˜ê°ì„ ë°›ì•˜ë‹¤ê³  í•˜ëŠ”ë°, êµ¬ì²´ì ì¸ ë‚´ìš©ì€ í™•ì¸í•´ë³´ì§€ ì•Šì•„ì„œ ì˜ ëª¨ë¥´ê² ë‹¤.</p> <p>ì´ ì™¸ì—ë„ ì•„ë˜ ë‘ê°€ì§€ íŠ¸ë¦­ìœ¼ë¡œ ì¢€ ë” íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í–ˆë‹¤ê³  í•œë‹¤.</p> <ol> <li>Batch Size warming up: ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ í•™ìŠµ ì´ˆê¸°ì—ëŠ” ì‘ê²Œ ê°€ì ¸ê°”ë‹¤ê°€ ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì¡°ê¸ˆì”© ëŠ˜ë ¤ê°€ëŠ” ë°©ë²•</li> <li>Tiling Model weight: Large ëª¨ë¸ weight ë¥¼ ì´ˆê¸°í™”í• ë•Œ í•™ìŠµëœ Base ëª¨ë¸ì„ í™œìš©í•˜ì—¬ loss ìˆ˜ë ´ ì†ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•</li> </ol> <h2 id="ëŠë‚€ì ">ëŠë‚€ì </h2> <p>ìµœê·¼ 8k ìˆ˜ì¤€ì˜ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì¸ì½”ë” ì „ëµë“¤ì´ ë§ì´ ë‚˜ì˜¤ëŠ”ê²ƒ ê°™ë‹¤.</p> <p>FlashAttention ê¸°ë°˜ì˜ Sequence Packing ì„ ì ìš©í–ˆë‹¤ëŠ” ì , attention ë°©ì‹ì„ Layer ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì ìš©í•œ ì ë„ í¥ë¯¸ë¡œì› ë‹¤. ìƒë‹¹íˆ ë°°ì›Œë³´ê³  ì‹¶ì€ ê¸°ìˆ ë“¤ì´ë‹¤.</p> <p>ìµœì ì˜ ë ˆì´ì–´ ë””ìì¸ì„ ì°¾ëŠ”ê²ƒì€ grid search ë¥¼ ì´ìš©í–ˆë‹¤ê³  í•˜ë‹ˆ, ì—­ì‹œ ì´ ë¶€ë¶„ì€ ë…¸ë‹¤ê°€ ì•„ë‹ˆë©´ ì•ˆë˜ëŠ” ì¼ì¸ë“¯ ì‹¶ë‹¤.</p> <p>ê·¸ë¦¬ê³  ë‹¨ìˆœíˆ ê³ í’ˆì§ˆ ë°ì´í„°ê°€ ë¬¸ì œê°€ ì•„ë‹ˆë¼ ë” ë‹¤ì–‘í•œ ë°ì´í„° ì—­ì‹œ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒë„ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì¸ë“¯ ì‹¶ë‹¤.</p> <h2 id="references">References</h2> <p>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</p> <ul> <li>blog: https://huggingface.co/blog/modernbert</li> <li>paper: https://huggingface.co/papers/2412.13663</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><category term="encoder"/><category term="BERT"/><category term="Flash_Attention"/><summary type="html"><![CDATA[ê¸°ì¡´ BERT family ë“¤ì˜ ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ ëª¨ë‘ ì´ê¸¸ ìˆ˜ ìˆëŠ” ModernBERT ê°€ ë‚˜ì™”ë‹¤ê³  í•œë‹¤.]]></summary></entry><entry><title type="html">SFT ë°ì´í„°ì…‹ì˜ ë…¸ì´ì¦ˆë¥¼ ì¤„ì—¬ë³´ì</title><link href="https://zzong2006.github.io/blog/2024/robust-ft/" rel="alternate" type="text/html" title="SFT ë°ì´í„°ì…‹ì˜ ë…¸ì´ì¦ˆë¥¼ ì¤„ì—¬ë³´ì"/><published>2024-12-30T00:00:00+00:00</published><updated>2024-12-30T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2024/robust-ft</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/robust-ft/"><![CDATA[<p>Downstream task ë¥¼ ìœ„í•œ LLM ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ, íŒŒì¸íŠœë‹(=SFT) ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ìˆëŠ” ê²½ìš°, ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤.</p> <p>ì´ ë…¼ë¬¸ì—ì„œëŠ” RobustFT ë¼ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬, ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íƒì§€í•˜ê³ , ë…¸ì´ì¦ˆê°€ ìˆëŠ” ê²½ìš° ì¬ë ˆì´ë¸”ë§ì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.</p> <h2 id="ë…¸ì´ì¦ˆ-íƒì§€-noise-identification">ë…¸ì´ì¦ˆ íƒì§€ (noise identification)</h2> <p>ì—¬ê¸°ì„œ ë§í•˜ëŠ” ë°ì´í„°ì— í¬í•¨ëœ â€œë…¸ì´ì¦ˆâ€ë€, ì—¬ëŸ¬ LLM ì˜ ì‘ë‹µì´ ì¼ê´€ì„±ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ë§í•œë‹¤. ì¦‰, prompt ì™€ response ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì—¬ëŸ¬ LLM ì˜ ì‘ë‹µì´ ì¼ê´€ì„±ì´ ì—†ëŠ” ê²½ìš° ë…¸ì´ì¦ˆê°€ ìˆë‹¤ê³  íŒë‹¨í•œë‹¤.</p> <h3 id="methods">Methods</h3> <p>ì—¬ëŸ¬ ì „ë¬¸ê°€ LLMs ê³¼ Checker ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë¥¼ ì‹ë³„í•œë‹¤.</p> <ol> <li>ì–´ë–¤ base LLM ì„ ì‚¬ìš©í•˜ì—¬ ì •ë‹µ $y$ ê°€ ìˆëŠ” query $q_i$ ì— ëŒ€í•´ ì‘ë‹µ $\hat{y}_i$ ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ìƒê°í•´ë³´ì.</li> <li>LLM ìœ¼ë¡œ (1) step-by-step reasoning (2) reflection ì„ ê³„ì† ë°˜ë³µí•˜ë©´ì„œ ì‘ë‹µ $\hat{y}^{reas}_i$ ë¥¼ ìƒì„±í•˜ë„ë¡ í•œë‹¤.</li> <li>Checker ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ì‘ë‹µë“¤ ($y,\hat{y}_i, \hat{y}^{reas}_i$) ì„ ì´ìš©í•´ì„œ ë…¸ì´ì¦ˆê°€ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•œë‹¤.</li> <li>Checker ëŠ” ì‘ë‹µì˜ ì¼ê´€ì„± ì •ë„ë¥¼ 0 ë˜ëŠ” 1 ì˜ ê°’ìœ¼ë¡œ íŒë‹¨í•˜ëŠ”ë°, 0 ì´ë©´ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ê²ƒì´ê³  1 ì´ë©´ ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ê²ƒì´ë‹¤.</li> </ol> <p>ì—¬ê¸°ì„œ Checker ë©”ì»¤ë‹ˆì¦˜ì€ ìƒë‹¹íˆ naive í•œë°, ë°ì´í„°ì…‹ë§ˆë‹¤ ë©”ì»¤ë‹ˆì¦˜ì´ ë‹¤ë¥´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, MMLU ê°™ì€ ê²½ìš°ëŠ” ì‘ë‹µë“¤ì˜ ì •ë‹µì„ ì¶”ì¶œí•´ì„œ ì„œë¡œ ê°™ì€ì§€ ë¹„êµí•˜ëŠ” ê²ƒìœ¼ë¡œ ë…¸ì´ì¦ˆ ì—¬ë¶€ë¥¼ íŒë‹¨í•œë‹¤.</p> <h3 id="ì°¸ê³ ">ì°¸ê³ </h3> <p>ì‹¤í—˜ì—ì„œ LLM ìœ¼ë¡œ Gemma2-9B, Llama3.1-8B ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.</p> <h2 id="ë…¸ì´ì¦ˆ-ì œê±°-de-noising">ë…¸ì´ì¦ˆ ì œê±° (de-noising)</h2> <ol> <li>Review Agent: context-enhanced reasoning with clean samples to store label noisy instances</li> <li>a perplexity-based data selection mechanism to exclude samples with low confidence scores</li> </ol> <h2 id="ëŠë‚€ì ">ëŠë‚€ì </h2> <p>ì •ë‹µ(ë˜ëŠ” golden reference) ì´ ì—†ëŠ” ê²½ìš° Checker ê°€ ì˜ ì‘ë™í•˜ì§€ ì•Šì„ê²ƒ ê°™ë‹¤.</p> <p>ë°ì´í„° ì •ì œí•˜ëŠ” ê±´ ì¢‹ì€ë°, ë‹¤ìˆ˜ì˜ LLM ì´ìš©í•´ì„œ ì •ì œí•˜ëŠ” ë¹„ìš©ì€ ë˜ ë‹¤ë¥¸ ì–˜ê¸°ë‹¤.</p> <h2 id="references">References</h2> <p>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</p> <ul> <li>paper: https://huggingface.co/papers/2412.14922</li> <li>code: https://github.com/luo-junyu/RobustFT</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><category term="SFT"/><category term="Preprocessing"/><summary type="html"><![CDATA[ë°ì´í„° ë””ë…¸ì´ì§• í”„ë ˆì„ì›Œí¬ RobustFT]]></summary></entry></feed>