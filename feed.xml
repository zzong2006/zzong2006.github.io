<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-09T23:09:47+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">Believe I.Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Meta ì—ì„œ ë§Œë“  Agent ë²¤ì¹˜ë§ˆí¬, GAIA</title><link href="https://zzong2006.github.io/blog/2025/gaia/" rel="alternate" type="text/html" title="Meta ì—ì„œ ë§Œë“  Agent ë²¤ì¹˜ë§ˆí¬, GAIA"/><published>2025-01-09T20:00:00+00:00</published><updated>2025-01-09T20:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/gaia</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/gaia/"><![CDATA[<h2 id="what-is-the-gaia">What is the GAIA?</h2> <p>GAIA is a benchmark which aims at evaluating next-generation LLMs (LLMs with augmented capabilities due to added tooling, efficient prompting, access to search, etc).</p> <p>Even equipped with tools, GPT4 does not exceed a 30% success rate for the easiest of our tasks, and 0% for the hardest. In the meantime, the average success rate for human respondents is 92%.</p> <p><img src="https://i.imgur.com/64s7BqX.png" alt="20250109203102" width="100%"/></p> <p>GAIAëŠ” ì¸ê°„ì—ê²ŒëŠ” ê°œë…ì ìœ¼ë¡œ ê°„ë‹¨í•˜ì§€ë§Œ, AIì—ê²ŒëŠ” ë³µì¡í•œ ì‘ì—…ì„ ìš”êµ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆë‹¤.</p> <p>ì‘ì—… ìì²´ëŠ” ê°„ë‹¨í•˜ì§€ë§Œ, <strong>ë³µì¡í•œ ìˆœì°¨ì  í–‰ë™(sequence of actions)</strong>ê³¼ <strong>ì¡°í•©ì  ê³µê°„(combinatorial space)</strong>ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ì‘ì—…ì˜ ê²°ê³¼ë¬¼ì€ ì‘ì—…ì´ ì™„ì „íˆ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆì„ ë•Œë§Œ ì–»ì–´ì§€ë©°, ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ê²ƒì€ ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨í•©ë‹ˆë‹¤.</p> <h2 id="gaia-íŠ¹ì§•">GAIA íŠ¹ì§•</h2> <ol> <li>GAIAëŠ” ì‹¤ì„¸ê³„ì—ì„œ ì‚¬ìš©ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë¨: Browse the open and changing web, handle multi-modality, or reason over multiple steps to answer our questions.</li> <li>Non-gameability: ë‹¤ì–‘í•œ ë‹¨ê³„ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•´ì•¼ ì •ë‹µì„ ì™„ë£Œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¶”ì¸¡ì´ë‚˜ ìš°ì—°ìœ¼ë¡œ ì •ë‹µì„ ë§ì¶”ê¸°ê°€ ì–´ë µê²Œ ì„¤ê³„ë¨</li> <li>Easy Interpretability: GAIAì˜ ì§ˆë¬¸ì€ ê°„ë‹¨í•˜ë©°, <strong>ëª…í™•í•œ ì´ìœ  ì¶”ì (reasoning trace)</strong>ì´ ê°€ëŠ¥í•˜ì—¬ ë¹„ì „ë¬¸ê°€ë„ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ê³¼ì • ì œê³µ <ol> <li>ë°˜ë©´ MMLU ê°™ì€ ë²¤ì¹˜ë§ˆí¬ëŠ” ì˜ëª»ëœ ì¶”ë¡ ì„ í•´ë„ ì •ë‹µì— ë„ë‹¬í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ</li> </ol> </li> </ol> <h2 id="related-works">Related Works</h2> <p>APIBench ë˜ëŠ” AgentBench ê°™ì€ ë‹¤ì–‘í•œ <strong>íì‡„í˜• í™˜ê²½(closed box environments)</strong>ì—ì„œ ì–´ì‹œìŠ¤í„´íŠ¸ LLMì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ë“¤ì´ ìˆì—ˆë‹¤.</p> <p>í•˜ì§€ë§Œ (1) íì‡„ëœ í™˜ê²½ì—ì„œ (2) íŠ¹ì • APIë‚˜ ì‹œìŠ¤í…œ ì‚¬ìš©ì„ í‰ê°€í•˜ëŠ”ë° ì£¼ìš” ê´€ì‹¬ì´ ìˆë‹¤. ì¦‰, ëª¨ë¸ì´ â€œíŠ¹ì • APIë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì‚¬ìš©í•˜ëŠ”ê°€â€ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ì´ˆì ì´ ë§ì¶°ì ¸ ìˆë‹¤.</p> <p>ì´ëŸ¬í•œ ì ‘ê·¼ì€ í˜„ì‹¤ ì„¸ê³„ì—ì„œì˜ ìƒí˜¸ì‘ìš© ê²°ê³¼ë¥¼ ì¼ë°˜í™”í•˜ê¸° ì–´ë µê²Œ ë§Œë“¤ê³ , ì¼ë°˜ì ì¸(real-world grounded) í‰ê°€ê°€ ì•„ë‹Œ, íŠ¹ì • ë„êµ¬ ì‚¬ìš© ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë° ê·¸ì¹  ìœ„í—˜ì´ ìˆë‹¤.</p> <h2 id="gaia-ë°ì´í„°">GAIA ë°ì´í„°</h2> <h3 id="ë‚œì´ë„ì—-ë”°ë¥¸-ë°ì´í„°-ìƒ˜í”Œ-ì˜ˆì‹œ">ë‚œì´ë„ì— ë”°ë¥¸ ë°ì´í„° ìƒ˜í”Œ ì˜ˆì‹œ</h3> <p>ì´ 3ê°œì˜ ë‚œì´ë„ê°€ ìˆìŒ: Level 1, Level 2, Level 3</p> <p><strong>Level 1</strong></p> <ul> <li>Question: 2018ë…„ 1ì›”ë¶€í„° 5ì›”ê¹Œì§€ NIH ì›¹ì‚¬ì´íŠ¸ì— ê¸°ì¬ëœ H. pyloriì™€ ì—¬ë“œë¦„ í™˜ìì— ëŒ€í•œ ì„ìƒ ì‹œí—˜ì˜ ì‹¤ì œ ë“±ë¡ ìˆ˜ëŠ” ì–¼ë§ˆì˜€ìŠµë‹ˆê¹Œ?</li> <li>ì •ë‹µ: 90</li> </ul> <p><strong>Level 2</strong></p> <p>(íŒŒì¸íŠ¸ ì•„ì´ìŠ¤í¬ë¦¼ ì„±ë¶„ êµ¬ì„±í‘œê°€ ì‚¬ì§„ìœ¼ë¡œ ì œê³µë¨)</p> <ul> <li>ì§ˆë¬¸: ì´ íŒŒì¸íŠ¸ ì „ì²´ê°€ ì•„ì´ìŠ¤í¬ë¦¼ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ë©´, 2020ë…„ ìœ„í‚¤í”¼ë””ì•„ì— ë³´ê³ ëœ ë¯¸êµ­ ì—°ë°© ê¸°ì¤€ì˜ ë²„í„° ì§€ë°© í•¨ëŸ‰ì— ë¹„í•´ ëª‡ í¼ì„¼íŠ¸ ë†’ê±°ë‚˜ ë‚®ìŠµë‹ˆê¹Œ? + ë˜ëŠ” -ë¡œ ì‹œì‘í•˜ëŠ” ìˆ«ìë¡œ ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.</li> <li>ì •ë‹µ: +4.6</li> </ul> <p><strong>Level 3</strong></p> <ul> <li> <p>ì§ˆë¬¸: 2006ë…„ 1ì›” 21ì¼, NASAì˜ ì²œë¬¸í•™ ë°ì´ì—ì„œ ì°ì€ ì‚¬ì§„ì—ì„œ ë‘ ëª…ì˜ ìš°ì£¼ë¹„í–‰ì‚¬ê°€ ë³´ì´ë©°, í•œ ëª…ì´ ë‹¤ë¥¸ í•œ ëª…ë³´ë‹¤ í›¨ì”¬ ì‘ê²Œ ë³´ì…ë‹ˆë‹¤. 2023ë…„ 8ì›” ê¸°ì¤€ìœ¼ë¡œ, ì‘ì€ ìš°ì£¼ë¹„í–‰ì‚¬ê°€ ì†í–ˆë˜ NASA ìš°ì£¼ë¹„í–‰ì‚¬ ê·¸ë£¹ì˜ ìš°ì£¼ë¹„í–‰ì‚¬ ì¤‘ ìš°ì£¼ì—ì„œ ê°€ì¥ ì ì€ ì‹œê°„ì„ ë³´ë‚¸ ì‚¬ëŒì€ ëˆ„êµ¬ì´ë©°, ê·¸ëŠ” ìš°ì£¼ì—ì„œ ëª‡ ë¶„(minutes)ì„ ë³´ëƒˆìŠµë‹ˆê¹Œ? ìš°ì£¼ë¹„í–‰ì‚¬ì˜ ì„±ì„ ì“°ê³ , ìˆ«ìì™€ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”. ìˆ«ìì—ëŠ” ì²œ ë‹¨ìœ„ êµ¬ë¶„ ê¸°í˜¸ë¡œ ì‰¼í‘œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.</p> </li> <li> <p>ì •ë‹µ: White; 5,876</p> </li> </ul> <h3 id="ë‚œì´ë„-êµ¬ì„±">ë‚œì´ë„ êµ¬ì„±</h3> <p><img src="https://i.imgur.com/TnbqeaX.png" alt="20250109202854" width="100%"/></p> <h2 id="limitation">Limitation</h2> <p>(1) ì •ë‹µì— ì´ë¥´ëŠ” ì¶”ë¡  ê³¼ì •(trace)ì„ í‰ê°€í•˜ì§€ ì•ŠìŒ</p> <p>GAIAëŠ” ì •ë‹µ(ground truth)ì´ ê³ ìœ í•˜ë‹¤ëŠ” ì „ì œë¥¼ ë‘ì§€ë§Œ, ë‹¤ì–‘í•œ ê²½ë¡œê°€ ì •ë‹µì— ì´ë¥¼ ìˆ˜ ìˆìŒ. ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ ë‹¨ìˆœí•˜ê³  ëª…í™•í•˜ê²Œ ì±„ì í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì•„ì§ ë¶€ì¬</p> <p>(2) ë„êµ¬ ì‚¬ìš© ëª¨ë¸ì— êµ­í•œëœ í‰ê°€</p> <p>(3) ì¼ë¶€ GAIA ì§ˆë¬¸ì€ ë§ì€ ì„¸ë¶€ ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ìì—°ìŠ¤ëŸ½ì§€ ì•Šê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ</p> <p>í•˜ì§€ë§Œ ì´ëŸ° ì„¸ë¶€ ì‚¬í•­ì€ í‰ê°€ë¥¼ ë” ì² ì €íˆ í•˜ê¸° ìœ„í•´ì„œ ì–´ì©”ìˆ˜ ì—†ëŠ” ê²ƒìœ¼ë¡œ ë³´ì„.</p> <p>(4) ì–¸ì–´ ë‹¤ì–‘ì„± ë¶€ì¡±: GAIAì˜ ì§ˆë¬¸ì€ <strong>â€œí‘œì¤€ ì˜ì–´(standard English)â€</strong>ë¡œë§Œ ì‘ì„±ë˜ì–´ ìˆìŒ</p> <p>(5) GAIA ë²¤ì¹˜ë§ˆí¬ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ìœ íš¨ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ (decay)</p> <ul> <li>ë°ì´í„° ì˜¤ì—¼(catastrophic contamination): ì‚¬ì „ í•™ìŠµ ë°ì´í„°ì— ì§ˆë¬¸ ë‚´ìš©ì´ í¬í•¨ë˜ëŠ” ê²½ìš°.</li> <li>ì›¹ ì½˜í…ì¸ ì˜ ì†Œë©¸: ì§ˆë¬¸ì— í•„ìš”í•œ ì •ë³´ê°€ ì¸í„°ë„·ì—ì„œ ì‚¬ë¼ì§€ëŠ” ê²½ìš°.</li> </ul> <h2 id="appendix">Appendix</h2> <h3 id="gaia-ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜-system-prompt">GAIA ë²¤ì¹˜ë§ˆí¬ì—ì„œì˜ System prompt</h3> <blockquote> <p>You are a general AI assistant. I will ask you a question. Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER]. YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings. If you are asked for a number, donâ€™t use comma to write your number neither use units such as $ or percent sign unless specified otherwise. If you are asked for a string, donâ€™t use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise. If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.</p> </blockquote> <h3 id="leaderboard-2025-01-09-ê¸°ì¤€">Leaderboard (2025-01-09 ê¸°ì¤€)</h3> <p><img src="https://i.imgur.com/ZBSW9w5.png" alt="20250109205637" width="100%"/></p> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/spaces/gaia-benchmark/leaderboard">GAIA Leaderboard</a></li> <li><a href="https://huggingface.co/datasets/gaia-benchmark/GAIA">GAIA (huggingface dataset)</a></li> <li><a href="https://github.com/aymeric-roucher/GAIA">GAIA (github)</a></li> </ul>]]></content><author><name></name></author><category term="benchmark"/><category term="LLM"/><category term="agent"/><category term="WIP"/><summary type="html"><![CDATA[What is the GAIA?]]></summary></entry><entry><title type="html">Google ì˜ agent ë¼ì´ë¸ŒëŸ¬ë¦¬, langfun</title><link href="https://zzong2006.github.io/blog/2025/langfun/" rel="alternate" type="text/html" title="Google ì˜ agent ë¼ì´ë¸ŒëŸ¬ë¦¬, langfun"/><published>2025-01-09T20:00:00+00:00</published><updated>2025-01-09T20:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/langfun</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/langfun/"><![CDATA[<p>Google ì—ì„œ Agent êµ¬ì¶•ì„ íŒŒì´ì¬ í´ë˜ìŠ¤ ì •ì˜ë¡œ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ìˆë„ë¡ í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì‚¬ìš©ë²•ì´ ìƒë‹¹íˆ ì§ê´€ì ì´ë‹¤.</p> <p>We hypothize that LLMs trained on code installed a strong tendency for LLMs to follow schema such as class definitions. Therefore, LLMs could be guided by the fields defined in a structure. The code below illustrates how Chain-of-Thoughts could be implemented:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">question</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">'</span><span class="s">Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. </span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">She sells the remainder at the farmers</span><span class="se">\'</span><span class="s"> market daily for $2 per fresh duck egg. </span><span class="sh">'</span>
    <span class="sh">'</span><span class="s">How much in dollars does she make every day at the farmers</span><span class="se">\'</span><span class="s"> market?</span><span class="sh">'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Step</span><span class="p">(</span><span class="n">pg</span><span class="p">.</span><span class="n">Object</span><span class="p">):</span>
  <span class="n">description</span><span class="p">:</span> <span class="nb">str</span>
  <span class="n">step_output</span><span class="p">:</span> <span class="nb">float</span>

<span class="k">class</span> <span class="nc">Solution</span><span class="p">(</span><span class="n">pg</span><span class="p">.</span><span class="n">Object</span><span class="p">):</span>
  <span class="n">steps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Step</span><span class="p">]</span>
  <span class="n">final_answer</span><span class="p">:</span> <span class="nb">int</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">lf</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">question</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">Solution</span><span class="p">,</span> <span class="n">lm</span><span class="o">=</span><span class="n">lf</span><span class="p">.</span><span class="n">llms</span><span class="p">.</span><span class="nc">Gpt4o</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
</code></pre></div></div> <p>Output:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Solution</span><span class="p">(</span>
  <span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">0</span> <span class="p">:</span> <span class="nc">Step</span><span class="p">(</span>
      <span class="n">description</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Calculate total eggs laid by ducks per day</span><span class="sh">'</span><span class="p">,</span>
      <span class="n">step_output</span> <span class="o">=</span> <span class="mf">16.0</span>
    <span class="p">),</span>
    <span class="mi">1</span> <span class="p">:</span> <span class="nc">Step</span><span class="p">(</span>
      <span class="n">description</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Eggs eaten for breakfast</span><span class="sh">'</span><span class="p">,</span>
      <span class="n">step_output</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="p">),</span>
    <span class="mi">2</span> <span class="p">:</span> <span class="nc">Step</span><span class="p">(</span>
      <span class="n">description</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Eggs used for baking muffins</span><span class="sh">'</span><span class="p">,</span>
      <span class="n">step_output</span> <span class="o">=</span> <span class="mf">4.0</span>
    <span class="p">),</span>
    <span class="mi">3</span> <span class="p">:</span> <span class="nc">Step</span><span class="p">(</span>
      <span class="n">description</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Eggs remaining to be sold at the farmers</span><span class="sh">'</span><span class="s"> market</span><span class="sh">"</span><span class="p">,</span>
      <span class="n">step_output</span> <span class="o">=</span> <span class="mf">9.0</span>
    <span class="p">),</span>
    <span class="mi">4</span> <span class="p">:</span> <span class="nc">Step</span><span class="p">(</span>
      <span class="n">description</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Calculate the earnings from selling the remaining eggs</span><span class="sh">'</span><span class="p">,</span>
      <span class="n">step_output</span> <span class="o">=</span> <span class="mf">18.0</span>
    <span class="p">)</span>
  <span class="p">],</span>
  <span class="n">final_answer</span> <span class="o">=</span> <span class="mi">18</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://colab.research.google.com/github/google/langfun/blob/main/docs/notebooks/langfun101.ipynb">langfun 101</a></li> </ul>]]></content><author><name></name></author><category term="benchmark"/><category term="LLM"/><category term="agent"/><category term="WIP"/><summary type="html"><![CDATA[Google ì—ì„œ Agent êµ¬ì¶•ì„ íŒŒì´ì¬ í´ë˜ìŠ¤ ì •ì˜ë¡œ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ìˆë„ë¡ í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì‚¬ìš©ë²•ì´ ìƒë‹¹íˆ ì§ê´€ì ì´ë‹¤.]]></summary></entry><entry><title type="html">LLM ì„ ì´ìš©í•œ Dense Retrieval</title><link href="https://zzong2006.github.io/blog/2025/use-llm-for-dense-retrieval/" rel="alternate" type="text/html" title="LLM ì„ ì´ìš©í•œ Dense Retrieval"/><published>2025-01-08T16:00:00+00:00</published><updated>2025-01-08T16:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/use-llm-for-dense-retrieval</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/use-llm-for-dense-retrieval/"><![CDATA[<p>paper</p> <ul> <li>Making Large Language Models A Better Foundation For Dense Retrieval</li> </ul> <p>Model</p> <ul> <li><a href="https://huggingface.co/BAAI/bge-reranker-v2-gemma">BAAI/bge-reranker-v2-gemma</a></li> </ul>]]></content><author><name></name></author><category term="survey"/><category term="LLM"/><category term="dense_retrieval"/><summary type="html"><![CDATA[paper Making Large Language Models A Better Foundation For Dense Retrieval]]></summary></entry><entry><title type="html">FC ë³´ë‹¤ëŠ” Code ì‹¤í–‰ì´ ë” ì¢‹ì€ Agent ë¥¼ ë§Œë“ ë‹¤</title><link href="https://zzong2006.github.io/blog/2025/code-act/" rel="alternate" type="text/html" title="FC ë³´ë‹¤ëŠ” Code ì‹¤í–‰ì´ ë” ì¢‹ì€ Agent ë¥¼ ë§Œë“ ë‹¤"/><published>2025-01-07T17:00:00+00:00</published><updated>2025-01-07T17:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/code-act</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/code-act/"><![CDATA[<h2 id="references">References</h2> <ul> <li><a href="https://machinelearning.apple.com/research/codeact">CodeAct: Your LLM Agent Acts Better when Generating Code</a></li> <li><a href="https://github.com/xingyaoww/code-act">Executable Code Actions Elicit Better LLM Agents (github)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><category term="function_calling"/><category term="agent"/><summary type="html"><![CDATA[References]]></summary></entry><entry><title type="html">Multi-Agent ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ëª¨ìŒ</title><link href="https://zzong2006.github.io/blog/2025/multi-agents-resources/" rel="alternate" type="text/html" title="Multi-Agent ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ëª¨ìŒ"/><published>2025-01-07T16:00:00+00:00</published><updated>2025-01-07T16:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/multi-agents-resources</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/multi-agents-resources/"><![CDATA[<h2 id="multi-agent-ê´€ë ¨-ë¦¬ì†ŒìŠ¤">Multi-Agent ê´€ë ¨ ë¦¬ì†ŒìŠ¤</h2> <h3 id="agent-ë°©í–¥ì—-ëŒ€í•œ-ì˜ê²¬ë“¤-from-reddit-comment">Agent ë°©í–¥ì— ëŒ€í•œ ì˜ê²¬ë“¤ (<strong>from <a href="https://www.reddit.com/r/LocalLLaMA/comments/1hqt79i/top_agent_only_27_away_from_degreeholding_humans/">reddit comment</a></strong>)</h3> <ul> <li> <p>No easy way to really control termination. Letting the LLM decide to terminate with a string is a poor design. In my case, my termination is just that the <strong>LLM generates no more executable code blocks</strong>.</p> </li> <li> <p>No easy way to control executable vs. non-executable code blocks. In my case, I just extended the class to have an <code class="language-plaintext highlighter-rouge">executable</code> attribute with another <code class="language-plaintext highlighter-rouge"># execution: true</code></p> </li> <li> <p>Control hallucinations: Crucial element (even for top LLMs like sonnet-3.5-new) is to find ways to catch hallucinations before they get bad. A key to success was to not let the <strong>LLM do more than 1 executable code per turn</strong>, else it tends to make up stuff.</p> </li> <li> <p>Good prompt engineering: Being able to be clear about what the system prompt contains, and how each tool has good suggestions about what to do next with the output it generated.</p> </li> <li> <p>Multi-agent is not better than just single agent with tools. Itâ€™s much worse usually, I donâ€™t know why people are so excited about multi-agent. I think the tools paradigm is much better, where the tool might happen to be really an agent. But nominally better to build a tool that does more to offload what agent has to think about. Dynamic creation of tools (program synthesis) is future.</p> </li> <li> <p>Itâ€™s better to allow the LLM freedom to code but give access to reliable tools. Itâ€™s ok if one uses function calling to access a finite set of tools (say ~30 or so, depending upon the model) as long as it has access to code and can call those same functions via code. But a pure function calling is very limiting to a general agent.</p> </li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://github.com/huggingface/smolagents">huggingface/smolagents</a>: huggingface ì—ì„œ ì œê³µí•˜ëŠ” agent ë¼ì´ë¸ŒëŸ¬ë¦¬</li> <li><a href="https://github.com/aymeric-roucher/GAIA">GAIA (github)</a>: Multi-Agent ë²¤ì¹˜ë§ˆí¬ (2023, Meta)</li> <li><a href="https://cookbook.openai.com/examples/orchestrating_agents">Orchestrating Agents</a>: OpenAI ì—ì„œ ì œê³µí•˜ëŠ” agent ì˜ˆì œ</li> <li><a href="https://arxiv.org/abs/2402.01030">Executable Code Actions Elicit Better LLM Agents (arxiv, 2024)</a></li> <li><a href="https://github.com/microsoft/autogen">AutoGen (github)</a>: Agent library from Microsoft</li> <li><a href="https://www.anthropic.com/research/building-effective-agents">Building Effective Agents (anthropic blog)</a></li> </ul>]]></content><author><name></name></author><category term="survey"/><category term="LLM"/><category term="agent"/><summary type="html"><![CDATA[Multi-Agent ê´€ë ¨ ë¦¬ì†ŒìŠ¤]]></summary></entry><entry><title type="html">LLM ì´ json ì‘ë‹µì„ ì˜ í•˜ë„ë¡ í•˜ëŠ”ë²•</title><link href="https://zzong2006.github.io/blog/2025/constrained-decoding/" rel="alternate" type="text/html" title="LLM ì´ json ì‘ë‹µì„ ì˜ í•˜ë„ë¡ í•˜ëŠ”ë²•"/><published>2025-01-07T10:00:00+00:00</published><updated>2025-01-07T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/constrained-decoding</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/constrained-decoding/"><![CDATA[<h2 id="constrained-decoding">Constrained decoding</h2> <p><img src="https://blog.mlc.ai/img/xgrammar/constrained-decoding.png" alt="constrained decoding" width="80%"/></p> <p>Constrained decoding is a common technique to enforce the output format of an LLM. As shown in the figure above, an LLM engine maintains an internal state of the desired structure and the history of generated tokens. When generating a new token, the engine identifies tokens that may violate the required structure and masks them off in the logits. The masking causes the sampling process to avoid invalid tokens and only generate valid ones. In this example, only tokens <code class="language-plaintext highlighter-rouge">true</code> and <code class="language-plaintext highlighter-rouge">false</code> are allowed in the first decoding step, and only <code class="language-plaintext highlighter-rouge">,</code> and <code class="language-plaintext highlighter-rouge">,\n</code> are allowed in the second decoding step.</p> <h2 id="context-free-grammars-cfgs">Context-free grammars (CFGs)</h2> <p>Although JSON schema is a popular method for structure specification, it cannot define code syntax or recursive structures (such as nested brackets of any depth). Context-free grammars (CFGs) provide a more powerful and general representation that can describe many complex structures.</p> <h2 id="references">References</h2> <ul> <li><a href="https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar">Achieving Efficient, Flexible, and Portable Structured Generation with XGrammar</a></li> </ul>]]></content><author><name></name></author><category term="inference"/><category term="LLM"/><summary type="html"><![CDATA[Constrained decoding]]></summary></entry><entry><title type="html">Algorithm lesson learned - array</title><link href="https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array/" rel="alternate" type="text/html" title="Algorithm lesson learned - array"/><published>2025-01-06T10:00:00+00:00</published><updated>2025-01-06T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array/"><![CDATA[<p>ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ í’€ë©´ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì€ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</p> <h2 id="êµ¬ê°„-ì²˜ë¦¬">êµ¬ê°„ ì²˜ë¦¬</h2> <p>ì–´ë–¤ êµ¬ê°„ì„ ì²˜ë¦¬í•˜ëŠ” ë¬¸ì œëŠ” í•´ë‹¹ êµ¬ê°„ì„ ëª¨ë‘ ì²˜ë¦¬í• ë ¤ í•˜ì§€ë§ê³ , êµ¬ê°„ì˜ ì•ê³¼ ë ë¶€ë¶„ë§Œ ë‹¤ë£° ìˆ˜ ìˆëŠ”ì§€ ìƒê°í•´ë³´ì.</p> <p>auxiliary array (difference array) ì„ ì‚¬ìš©í•˜ì—¬ ë²”ìœ„ ì—…ë°ì´íŠ¸ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤. ë²”ìœ„ ë‚´ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹ , ë²”ìœ„ì˜ ì‹œì‘ì ê³¼ ëì ì„ í‘œì‹œí•˜ê³  ë‚˜ì¤‘ì— prefix sumì„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•œë‹¤.</p> <p>ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì–´ì§„ ë°°ì—´ì— ëŒ€í•´ [0, 4] êµ¬ê°„ì— +1ì„ ì ìš©í•˜ê³ , [2, 5] êµ¬ê°„ì— -1ì„ ì ìš©í•œë‹¤ê³  ê°€ì •í•´ë³´ì.</p> <ul> <li>auxiliary ë°°ì—´ì˜ [0] ì¸ë±ìŠ¤ì— +1ì„, [5] ì¸ë±ìŠ¤ì— -1ì„ ì ìš©í•˜ê³ , [2] ì¸ë±ìŠ¤ì— +1ì„, [6] ì¸ë±ìŠ¤ì— -1ì„ ì ìš©</li> <li>ê° ìš”ì†Œê°€ ì›ë˜ ë°°ì—´ì—ì„œ ì–¼ë§ˆë‚˜ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí–ˆëŠ”ì§€ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ prefix sumì„ ì‚¬ìš©í•˜ì—¬ ëˆ„ì  í•©ì„ ê³„ì‚°</li> </ul> <h3 id="ê´€ë ¨-ë¬¸ì œ">ê´€ë ¨ ë¬¸ì œ</h3> <ul> <li><a href="https://leetcode.com/problems/shifting-letters-ii/description/">leetcode: shifting-letters-ii</a></li> </ul>]]></content><author><name></name></author><category term="algorithm"/><category term="competitive-programming"/><category term="array"/><category term="string"/><category term="lesson-learned"/><summary type="html"><![CDATA[ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ í’€ë©´ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì€ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.]]></summary></entry><entry><title type="html">llama ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì</title><link href="https://zzong2006.github.io/blog/2025/llama/" rel="alternate" type="text/html" title="llama ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì"/><published>2025-01-06T10:00:00+00:00</published><updated>2025-01-06T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/llama</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/llama/"><![CDATA[<p>LLaMA 1, 2 ëª¨ë¸ì„ ëœ¯ì–´ë³´ë©´ì„œ ì•Œê²Œëœ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.</p> <blockquote> <p>We attribute their success, as all else, to divine benevolence.</p> </blockquote> <h2 id="rmsnorm-root-mean-square-layer-normalization">RMSNorm (Root Mean Square Layer Normalization)</h2> <p>Why need a layer normalization? Internal covariate shift</p> <ul> <li>ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í‰ê·  0 (re-centering), ë¶„ì‚° 1 (re-scaling)ë¡œ ë§ì¶˜ë‹¤.</li> <li>Layer Normalizationì€ í–‰(row) ë‹¨ìœ„ë¡œ ì ìš©ëœë‹¤.</li> </ul> <p>RMS ëŠ” LayerNorm ì˜ íš¨ê³¼ê°€ mean ë³´ë‹¤ëŠ” variance ìª½ì— ê¸°ì—¬ì¹˜ê°€ ë” ë†’ì„ ê²ƒì´ë¼ ê°€ì •í•œë‹¤. ê·¸ë˜ì„œ RMSNorm ì€ í‰ê·  ê³„ì‚°ì„ í¬ê¸°í•˜ê³  variance ë§Œ ê³„ì‚°í•˜ì—¬ ì •ê·œí™”í•´ì„œ computation-efficiency ì— ì´ë“ì„ ì·¨í•œë‹¤.</p> <h2 id="rotary-positional-embedding">Rotary positional embedding</h2> <p>Rotary positional embedding ì€ relative positional embedding ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, distance ì •ë³´ë¥¼ ìƒìˆ˜ê°’ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ embedding vector ì— ì ìš©í•˜ë©´ì„œ ê³„ì‚° íš¨ìœ¨ì„ ë†’ì¸ë‹¤.</p> <p>ê·¸ ìƒìˆ˜ëŠ” complex number ë¡œ, Eulerâ€™s formula ë¥¼ ì´ìš©í•´ì„œ attention ê°’ì„ ê³„ì‚°í•œë‹¤.</p> <p>Other positional embedding methods</p> <ul> <li>Absolute: Vanilla transformers ì—ì„œ ì ìš©ëœ ë°©ë²•ìœ¼ë¡œ, Attention ê³„ì‚° ì‹œ ì´ë¯¸ ê³ ì •ëœ constant position ì •ë³´ê°€ ì ìš©ë˜ì–´ ìˆëŠ” ê° embedding vector ë¥¼ ê³„ì‚°ì— í™œìš©í•œë‹¤.</li> <li>Relative: Attention ê³„ì‚° ì‹œ, ê° embedding vector pair ë§ˆë‹¤ ìƒëŒ€ì ì¸ distance ì •ë³´ë¥¼ ë³€ìˆ˜ë¡œ í™œìš©í•˜ì—¬ ê³„ì‚°í•œë‹¤.</li> </ul> <p>Absolute, relative ì™€ ë‹¤ë¥´ê²Œ rotary positional embedding ì€ <strong>q, k weight ê°€ ë¨¼ì € ì ìš©ëœ ì´í›„</strong> ì— ì ìš©ëœë‹¤ëŠ” ì ì´ë‹¤.</p> <h2 id="grouped-multi-query-attention">Grouped Multi-Query Attention</h2> <p><strong>Multi-Query Attention</strong></p> <p>ì¼ë°˜ì ì¸ multi-head attention ì€ ê° head ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ key, value ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ë¥¼ í•˜ë‚˜ë¡œ í†µì¼í•˜ì—¬ ëª¨ë“  query ì— ë™ì¼í•œ key, value ë¥¼ ì‚¬ìš©í•œë‹¤.</p> <p>Why?</p> <ul> <li>Problem: GPU ì˜ memory bandwidth ëŠ” GPU ì˜ ê³„ì‚° ì†ë„ (FLOPS) ë³´ë‹¤ í›¨ì”¬ ëŠë¦¬ë‹¤.</li> <li>KV cache ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ, í•˜ë‚˜ì˜ í† í° query ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ë¯€ë¡œ, i/o bottleneck ì´ ë°œìƒí•œë‹¤.</li> </ul> <p><strong>Grouped Multi-Query Attention</strong></p> <p>Grouped Multi-Query ëŠ” ì¼ì • ê°œìˆ˜ì˜ ê·¸ë£¹ë§ˆë‹¤ ë™ì¼í•œ key, value ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, group ì‚¬ì´ì¦ˆê°€ 1 ì¼ ë•ŒëŠ” ì¼ë°˜ì ì¸ multi-head attention ê³¼ ë™ì¼í•˜ë‹¤.</p> <h2 id="swiglu-activation-function">SwiGLU activation function</h2> <p>SwiGLUëŠ” Swish + GLU, ë‘ê°œì˜ Activation Functionsë¥¼ ì„ì–´ ë§Œë“  í•¨ìˆ˜</p> <p><strong>Swish Function</strong></p> <ul> <li>$\sigma(x) = x \cdot \sigma(x)$ ë¡œ í‘œí˜„ëœë‹¤.</li> <li>Original Transformer ì—ì„œì˜ ReLU ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, ìŒìˆ˜ìª½ì—ì„œ 0 ì— ê°€ê¹Œì›Œì§ˆë•Œ ê¸°ìš¸ê¸°ê°€ 0 ì´ ë˜ëŠ” ë¬¸ì œ(Dying ReLU)ë¥¼ í•´ê²°í•œë‹¤.</li> </ul> <p><strong>GLU (Gated Linear Unit)</strong></p> \[GLU(a, b) = a \otimes \sigma(b)\] <p>The GLU also has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.</p> <p><strong>SwiGLU</strong></p> <p>GLU ì—ì„œ sigmoid ëŒ€ì‹  Swish Function ì„ ì‚¬ìš©í•œë‹¤.</p> \[SwiGLU(a, b) = a \otimes \text{Swish}_\beta(b)\] <p>êµ¬ì²´ì ìœ¼ë¡œëŠ” ì´ 3ê°œì˜ weight matrix ë¥¼ ì‚¬ìš©í•˜ì—¬ LLaMA ì˜ FFN ì„ êµ¬ì„±í•œë‹¤.</p> \[\text{FFN}_\text{SwiGLU}(x, W, V, W_2) = (\text{Swish}_1(xW) \otimes xV)W_2\] <p><strong>vs. ReLU</strong></p> <p>ReLU ë³´ë‹¤ëŠ” SwiGLU ê°€ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ëŠ” ëŠë‚Œì´ì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  ì—„ì²­ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ì—ì„œëŠ” ReLU ëŠ” 83.80 ì ì´ê³ , SwiGLU ëŠ” 84.36 ì  ì •ë„ë¡œ, ê±°ì˜ ì°¨ì´ê°€ ì—†ëŠ” ëŠë‚Œ.</p> <p>í•˜ì§€ë§Œ ì „ë°˜ì ì¸ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ì—ì„œ SwiGLU ìª½ì´ ìš°ìœ„ì¸ ìƒí™©.</p> <h2 id="references">References</h2> <ul> <li><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&amp;ab_channel=UmarJamil">LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</a></li> <li><a href="https://jihan819.tistory.com/entry/AI-SwiGLU%EB%8A%94-%EC%96%B4%EB%96%A4-%ED%95%A8%EC%88%98%EC%9D%BC%EA%B9%8C">SwiGLUëŠ” ì–´ë–¤ í•¨ìˆ˜ì¼ê¹Œ?</a></li> </ul>]]></content><author><name></name></author><category term="algorithm"/><category term="LLaMA"/><category term="Meta"/><summary type="html"><![CDATA[LLaMA 1, 2 ëª¨ë¸ì„ ëœ¯ì–´ë³´ë©´ì„œ ì•Œê²Œëœ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.]]></summary></entry><entry><title type="html">Semantic Retrieval at Walmart</title><link href="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/" rel="alternate" type="text/html" title="Semantic Retrieval at Walmart"/><published>2025-01-05T17:00:00+00:00</published><updated>2025-01-05T17:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/"><![CDATA[<p>Walmart ì—ì„œ ì ìš©í•œ Embedding-based neural retrieval (EBR) ì— ëŒ€í•œ ë…¼ë¬¸ ë¦¬ë·°.</p> <p>2022ë…„, 2024ë…„ì— ê°ê° í•˜ë‚˜ì”© ì‹œë¦¬ì¦ˆë¬¼ ëŠë‚Œìœ¼ë¡œ ë°œí‘œë˜ì—ˆë‹¤.</p> <h2 id="1-summary-of-the-proposed-method">(1) Summary of the proposed method</h2> <ol> <li>A hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries.</li> <li>A novel method of selecting negative examples for training a large neural retrieval model and an approximate metric to evaluate the performance</li> </ol> <h2 id="2-related-works">(2) Related Works</h2> <p><strong>Production search vs. Web search</strong></p> <p>Production search is way more challenging than web search.</p> <ul> <li>Product titles (the main search-able text) are generally much shorter than web documents.</li> <li>While many web documents may contain the same information, a specific product from a seller rarely has a duplicate.</li> </ul> <p><strong>Traditional Solutions</strong></p> <ul> <li>knowledge graph: need a huge amount of domain expertise, and the cost of maintaining these components is high, since the catalog and product vocabulary frequently change in e-commerce</li> <li>BM25, an inverted index: suffers from vocabulary mismatch between the query and the product title</li> <li>neural systems: limited by the fact that the embedding size cannot be too large due to latency concerns</li> </ul> <h2 id="3-proposed-methods">(3) Proposed Methods</h2> <p><strong>Reducing the size of embedding</strong></p> <p>Reducing the size of embedding is beneficial as it allows the item embedding and the ANN index to be refreshed more frequently.</p> <p>Tried 2 approaches:</p> <ol> <li>Add a linear projection layer to reduce the embedding size to 368, 256, 128, and 64</li> <li>Use a transformer architecture that has a smaller embedding size: MiniLM (12 layers and an embedding size of 368), XtremeDistil (6 layers and an embedding size of 368)</li> </ol> <p>The <strong>linear projection</strong> is very effective in reducing the size of the embedding with very little performance cost.</p> <p><strong>A hybrid architecture</strong></p> <p>â€¦</p> <p><strong>ANN</strong></p> <ul> <li>Normalized vectors of dimension 256, the ANN services can yield 99% for recall@20 evaluated against the full nearest neighborhood search, with an average latency around 13 ms;</li> <li>For normalized vectors of dimension 768, the services can achieve a similar recall@20 but with three times the storage space;</li> </ul> <h2 id="6-lesson-learned">(6) Lesson Learned</h2> <p><strong>Cosine similarity vs. Inner product</strong></p> <ul> <li>The inner product is more stable during training and does not require the temperature factor ğœ.</li> <li>But, <strong>inner product was much harder to optimize</strong> when creating the ANN index, compared to cosine similarity.</li> </ul> <p><strong>Text fields</strong></p> <p>Many text fields are generally available for each product, and the quality of the text fields varies. But, we could not extract any boost in performance by using these text fields. This is probably because descriptions can contain a lot of irrelevant text that simply adds noise.</p> <p><strong>Model Complexity</strong></p> <p>A very deep model or very large embedding size is not necessary to achieve top performance. This is probably because queries and product titles are not very complex from a semantic perspective.</p> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2412.04637">Semantic Retrieval at Walmart</a></li> <li>[2] <a href="https://arxiv.org/abs/2408.04884">Enhancing Relevance of Embedding-based Retrieval at Walmart</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Walmart"/><category term="ANN"/><category term="RAG"/><category term="WIP"/><summary type="html"><![CDATA[Walmart ì—ì„œ ì ìš©í•œ Embedding-based neural retrieval (EBR) ì— ëŒ€í•œ ë…¼ë¬¸ ë¦¬ë·°.]]></summary></entry><entry><title type="html">KV-Cache ì— ëŒ€í•´ ì•Œì•„ë³´ì</title><link href="https://zzong2006.github.io/blog/2025/kv-cache/" rel="alternate" type="text/html" title="KV-Cache ì— ëŒ€í•´ ì•Œì•„ë³´ì"/><published>2025-01-05T10:00:00+00:00</published><updated>2025-01-05T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/kv-cache</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/kv-cache/"><![CDATA[<p>The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).</p> <p>Therefore, when iteratively calling forward() instead of the generate() method, itâ€™s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape (batch_size, past_kv_length + new_tokens_length). This is usually handled internally when you call generate() method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.</p> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/docs/transformers/kv_cache">huggingface transformers - kv_cache</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Transformers"/><category term="LLM"/><category term="WIP"/><summary type="html"><![CDATA[The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).]]></summary></entry></feed>