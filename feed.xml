<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-31T20:10:13+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">Believe I.Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델</title><link href="https://zzong2006.github.io/blog/2025/deepseek-r1/" rel="alternate" type="text/html" title="DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델"/><published>2025-01-30T12:18:00+00:00</published><updated>2025-01-30T12:18:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/deepseek-r1</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/deepseek-r1/"><![CDATA[<p>최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.</p> <p>도대체 어떤 모델을 만들었는지 한번 살펴보자.</p> <h2 id="introduction">Introduction</h2> <p>OpenAI o1 은 inference-time 이 확장될수록 수학, 코딩, 과학 추론 능력이 향상되는것을 보여준 모델이다. 이후 효율적인 test-time scaling 을 위해 프로세스 기반 보상 모델(PRM), 강화 학습(RL), 몬테카를로 트리 탐색(MCTS) 등의 기술을 시도해봤지만 o1 의 아성을 넘기에는 한계가 있었다.</p> <p>DeepSeek 은 DeepSeek-V3-Base를 기본 모델로 사용하고 GRPO(Shao et al., 2024)를 RL 프레임워크로 적용해 추론 성능을 개선했다. 이는 SFT 를 제외한, 순수 RL 과정을 통해 <strong>자기 진화(self-evolution)</strong>하며 추론 능력을 발전시킬 수 있는 잠재력을 탐구한 시도로 생각할 수 있겠다.</p> <p>수천번의 RL 단계를 거치면서 흥미로운 추론 행동이 자연스럽게 발현된 DeepSeek-Zero 는 초창기 o1-0912 급 모델의 추론 수준을 보였으나, 가독성과 언어가 혼합되어 나오는 문제를 발견했다.</p> <p>이런 문제를 해결하기 위해 수천개의 cold-start 데이터만을 모아 v3-base 모델을 먼저 파인튜닝하고, 이후 zero 와 비슷한 RL 단계를 거치면서 훈련을 진행했다. RL 과정에서 수렴 단계로 도달한 경우, rejection sampling 을 통해 새로운 sft 데이터를 생성하고, 기존 DeepSeek-V3 의 지도 학습 데이터와 합쳐 v3-base 모델을 다시 학습했다. 이런 과정을 반복해서 만들어진 최종 deepseek-r1 checkpoint 는 o1-1217 와 동등한 수준의 추론 능력을 달성했다.</p> <h2 id="주요-모델">주요 모델</h2> <p>DeepSeek-R1-Zero, DeepSeek-R1, Qwen/Llama 기반의 1.5B~70B 수준 지식 증류 모델 공개.</p> <h3 id="1-deepseek-r1-zero">(1) DeepSeek-R1-Zero</h3> <p>SFT 없이 RL로 훈련된 초기 모델이다. 추론 능력은 뛰어나지만 가독성(poor readability), language mixing 이슈가 존재한다고 한다.</p> <p>RL 학습을 위해서 GRPO (Group Relative Policy Optimization) 방식을 적용했다. 일반적으로 강화 학습 과정에서는 policy 와 critic 모델을 따로 가져가는데, critic 모델 대신 어떤 하나의 그룹의 점수들로 critic 의 평가를 대체하는 전략을 활용한다.</p> <p>GRPO 학습 과정에서는 어떤 질문 $q$ 가 주어졌을 때, 이전 정책 $\pi_{\theta_{\text{old}}}$로부터 여러 개의 출력 ${o_1, o_2, \cdots, o_G}$을 생성하고, 보상(점수) ${r_1, r_2, \dots, r_G}$을 구한뒤, 이를 바탕으로 정책 모델 $\pi_{\theta}$를 최적화한다. 이때, 점수들은 그룹 내부의 평균 및 표준편차로 정규화하여 “어느 후보가 상대적으로 좋았는지의 개념인 advantage $A_i$ 를 계산한다. 즉, 우도(advantage)를 바탕으로 새 정책 $\pi_{\theta}$를 업데이트한다.</p> <p>그럼 보상(점수)는 어떻게 계산헀을까? Rule based 기반의 채점 방식을 사용했는데, 수학이나 코딩 문제처럼 정답이 명확한 경우에만 점수를 주도록 설계했다고 한다. 다른 신경망 모델을 이용해서 점수를 채점하는 방식은 사용하지 않았다고 하는데, 그 이유는 reward hacking 문제나 reward model 자체를 재학습 하는 과정이 복잡해지고 난이도가 상승하기 때문이라고 한다.</p> <p>응답을 생성할때는 특정 content 에 대한 bias 를 제한하기 위해 아래처럼 고정된 프롬프트를 사용했다고 한다.</p> <p><img src="https://i.imgur.com/aHE9I7H.png" alt="20250130130652" width="90%"/></p> <p>이렇게 학습을 할수록 모델의 응답 길이가 늘어나면서 동시에 aha moment 가 발생한다고 한다. aha moment 는 모델이 문제 해결 과정에서 초기 접근법을 다시 평가하며 사고 시간을 더 많이 할당하는 법이 자연스럽게 발현되는 현상을 의미한다. 즉, 문제를 해결하는 구체적 방법을 명시적으로 가르치지 않고도, 그저 올바른 보상 체계를 부여하기만 하면 모델이 스스로 고도의 문제 해결 전략을 창출해낸다는 점이라고 한다.</p> <p><img src="https://i.imgur.com/scvFkSj.png" alt="20250130131632" width="90%"/></p> <p>위 내용처럼 <code class="language-plaintext highlighter-rouge">Wait, wait. Wait.</code> 과 같은 문장과 함께 모델이 문제의 접근 방식을 다시한번 스스로 생각하는 모습을 볼 수 있다.</p> <p>이렇게 학습된 Zero 모델은 위에서 언급했듯이 추론 능력을 크게 끌어올릴 수 있었지만, 추론 과정에서 가독성이 떨어지고 여러 언어가 섞여서 응답하는 문제가 있다고 한다.</p> <h3 id="2-deepseek-r1">(2) DeepSeek-R1</h3> <p>DeepSeek-R1 모델은 초기 모델인 DeepSeek-R1-Zero 에서 발견된 문제를 완화한 모델이다. RL 적용전에 cold-start 데이터와 multi-stage 훈련을 진행했고, 그 결과 OpenAI-o1-1217과 유사한 성능을 달성했다고 한다.</p> <h4 id="cold-start-data">Cold-start Data</h4> <p>Zero 에서는 초기 RL 학습시 불안정한 모습을 보이는 경우가 많았다고 한다. 이런 문제를 해결하기 위해 초기 모델에 cold-start 데이터를 이용해서 학습을 진행했는데, cold-start 데이터는 다양한 시도를 진행했다고 한다.</p> <ol> <li>긴 CoT를 사례로 삼아 few-shot 프롬프트를 적용</li> <li>모델에 직접 reflection과 verification 과정을 포함해 상세한 답변을 생성하도록 지시하는 방법</li> <li>DeepSeek-R1-Zero의 출력을 읽기 쉬운 형태로 정리한 뒤 Human annotators가 후처리를 거쳐 결과를 개선하는 방법</li> </ol> <p>주로 데이터 형태는 <code class="language-plaintext highlighter-rouge">|special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;</code> 로 표현되었는데, reasoning_process는 쿼리에 대한 CoT(Chain of Thought)를, summary는 추론 결과를 요약하는 용도로 사용했다.</p> <p>인간의 사전 지식(prior)을 고려해 콜드스타트 데이터를 신중히 설계하면, DeepSeek-R1-Zero와 비교했을 때 더 나은 성능을 발휘한다고 한다 (역시 정성적으로 살펴본 고퀄리티 데이터도 중요한거 같다).</p> <h4 id="language-consistency-reward">Language Consistency Reward</h4> <p>Cold-start 데이터로 초벌된 v3-base 모델은 RL 학습을 거치게 되는데, zero 에서 발견한 language-mix 이슈를 해결하기 위해 <strong>언어 일관성 보상(Language Consistency Reward)</strong>을 추가한다. 이는 목표로 하는 언어가 CoT 과정에서 얼마나 차지하는지의 비율로 계산된다. 비록 ablation study 과정에서 이런 정렬(언어 일관성)을 적용하면 모델 성능이 다소 떨어지는 것으로 나타났으나, 그럼에도 인간이 선호하는 형태에 가까워져 가독성이 높아지는 장점이 있다.</p> <p>즉, GRPO 에서 보상을 계산할때 정확도 뿐만 아니라 언어 일관성 보상을 합산해 최종 보상을 구성하고, 이 보상을 활용해 파인튜닝된 모델에 RL 훈련을 적용하며, 수렴(convergence)을 이룰 때까지 모델을 학습한다.</p> <h4 id="rejection-sampling-for-sft">Rejection Sampling for SFT</h4> <p>이후 수렴된 체크포인트를 활용하여 cold-start 데이터를 보강하기 위한 추가적인 SFT 데이터를 수집한다고 한다. 초기 콜드스타트 데이터가 주로 추론 위주였다면, 이번 단계에서는 쓰기(writing), 롤플레이(role-playing) 등 범용 과제 수행 능력을 높이기 위해 다른 도메인 데이터도 함께 포함한다.</p> <p>총 80 만 건을 수집하는데, 약 20만건은 추론, 약 60만건은 writing, QA 등의 비추론 데이터로 구성한다.</p> <p>추론 데이터의 경우 (Zero 모델 phase 에서의) rule based 방식 더불어 DeepSeek-V3 모델을 이용한 reward 를 계산하여 데이터의 퀄리티를 측정했다. 즉, 정답(ground-truth)과 모델 예측값을 DeepSeek-V3에 입력해 데이터 수준의 적절성을 판단하는 방식이다.</p> <p>60만개의 비추론 데이터의 경우, DeepSeek-V3 의 SFT 데이터를 재활용했다고 하며, 이렇게 모인 80만개의 데이터셋을 DeepSeek-V3-Base 에 2 epoch 동안 재학습 시켰다고 한다.</p> <h4 id="apply-additional-rl-improve-helpfulness--harmlessness">Apply additional RL: improve helpfulness &amp; harmlessness</h4> <p>모델의 <strong>도움(Helpfulness)</strong>과 <strong>무해성(Harmlessness)</strong>을 높이는 동시에 추론 능력까지 한층 더 다듬는 것을 목표로, 두 번째 강화 학습 단계를 진행했다고 한다.</p> <p>추론 데이터는 이전에 설명했던것과 동일하게 정확도를 기반으로 보상을 계산했고, 범용 데이터의 경우 reward model 기반의 preference pairs 를 구성했다고 한다. 이 외에도 Helpfulness 를 평가하기 위해 r1 모델이 응답한 summary 사용자에게 얼마나 유용하고 관련성 있는 응답을 제공하는지 초점을 두고 평가하도록 하거나, 전체 응답 과정에서 발생할 수 있는 유해한 컨텐츠를 찾아내서 harmlessness 를 개선했다고 한다.</p> <h3 id="3-distillation-작은-모델에-추론-능력-이식하기">(3) Distillation: 작은 모델에 추론 능력 이식하기</h3> <p>DeepSeek-R1과 같은 추론 능력을 더 효율적인 소형 모델에게도 이식하기 위해, 약 80만 건(800k)**으로 구성된 데이터셋을 활용해 Qwen, Llama 등 오픈소스 모델을 직접 파인튜닝했다고 한다.</p> <p>실험 결과, 이러한 SFT 기반의 직접적인 증류(distillation) 방식만으로도 소형 모델의 추론 능력이 현저히 향상된다는 사실을 확인했으며, 공개된 distillation 모델들은 RL을 추가로 도입하면 성능이 크게 높아질 수 있음에도 따로 진행하지는 않았다고 한다.</p> <h2 id="lesson-learned">Lesson Learned</h2> <p>지식 증류 vs. RL</p> <ol> <li>소형 모델은 RL만으로는 성능 한계 존재 → 지식 증류가 효율적.</li> <li>단, 지능 한계 돌파에는 대형 모델과 대규모 RL 필요.</li> </ol> <p>실패 사례:</p> <ul> <li>PRM(Process Reward Model): 보상 해킹 및 복잡성 문제.</li> <li>MCTS(Monte Carlo Tree Search): 토큰 생성 공간의 복잡성으로 확장 어려움.</li> </ul> <h2 id="limitations">Limitations</h2> <p>강력한 모델이지만 o1 과 비슷한 느낌의 단점들이 존재하는 것으로 보인다.</p> <p><strong>(1) General Capability</strong></p> <p>함수 호출, 멀티턴 대화, 복잡한 롤플레잉, JSON 출력과 같은 작업에서 DeepSeek-V3에 비해 부족한 면</p> <p><strong>(2) Language Mixing</strong></p> <p>DeepSeek-R1은 현재 중국어와 영어에 특화되어 있으므로, 다른 언어 쿼리를 처리할 때 언어 혼합 문제가 발생할 수 있다. 예컨대 영어 외 언어로 쿼리를 입력해도, 영어로 추론 과정을 작성하고 응답을 출력할 수 있다.</p> <p><strong>(3) Prompting Engineering</strong></p> <p>모델이 프롬프트(prompt)에 민감하게 반응한다.Few-shot 프롬프트 설정을 할 경우, 성능이 지속적으로 저하되는 경향을 보인다. 따라서 가장 효율적인 결과를 얻기 위해서는 제로샷(zero-shot) 방식으로 문제를 직접 설명하고 출력 형식을 지정하는 방법을 권장한다.</p> <p><strong>(4) Software Engineering</strong></p> <p>추론 시간이 길어지면 RL 프로세스의 효율에 영향을 미치기 때문에, 대규모 RL이 소프트웨어 엔지니어링 과제에 광범위하게 적용된 적은 아직 많지 않다. 이로 인해 DeepSeek-R1은 소프트웨어 엔지니어링 벤치마크에서 DeepSeek-V3 대비 현저한 성능 향상을 보이진 못했다고 한다.</p> <h2 id="느낀점">느낀점</h2> <p>SFT 전에 RL 만 적용해도 성능이 좋아진다고 하는데, 추론 능력의 경우 이런 접근이 유용한것 같다.</p> <p>하지만 정답이 명확하지 않은 일반 케이스의 경우, 역시 reward model 이나 사람이 평가한 피드백이 필수로 들어가야 하는것 같다.</p> <p>그리고 Zero 에 SFT 목적의 cold-start 데이터를 넣은것도, 완벽하게 SFT 에서 벗어나긴 힘들어 보이는 느낌. 하지만 기존에 대용량 SFT 데이터에 의존하는 것보다는 훨씬 발전된 느낌이 들었다.</p> <p>결국 더 많은 문제집 (?) 을 넣고 학습할수록 모델이 좋아진다.. 의 느낌으로 이해하면 될려나.</p> <p>여전히 추론 모델이 가지고 있는 단점인 함수 호출, 멀티턴 대화 등을 못하는 이슈는 해결해야될 과제로 보인다. 이래서 4o 같은 모델로 응답을 생성하고, 정답을 판단하는건 o1 같은 모델이 하는게 좀 더 자연스러워 보인다.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1 (github)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="DeepSeek"/><category term="LLM"/><category term="RL"/><summary type="html"><![CDATA[최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.]]></summary></entry><entry><title type="html">KV-Cache 에 대해 알아보자</title><link href="https://zzong2006.github.io/blog/2025/kv-cache/" rel="alternate" type="text/html" title="KV-Cache 에 대해 알아보자"/><published>2025-01-24T10:00:00+00:00</published><updated>2025-01-24T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/kv-cache</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/kv-cache/"><![CDATA[<p>The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).</p> <p>Therefore, when iteratively calling forward() instead of the generate() method, it’s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape (batch_size, past_kv_length + new_tokens_length). This is usually handled internally when you call generate() method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.</p> <h2 id="kv-cache-계산-공식">KV Cache 계산 공식</h2> <p>모델을 vllm 같은 프레임워크에서 배포하기 위해 메모리가 충분한지 계산을 해야한다.</p> <p>이때 아래와 같은 공식을 사용할 수 있다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KV 캐시 메모리 (Bytes) = \
  2 * n_layers * n_heads * d_head * context_length * bytes_per_param
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">2</code>: Key와 Value를 모두 저장하므로 2배.</li> <li><code class="language-plaintext highlighter-rouge">n_layers</code>: 트랜스포머 레이어 수</li> <li><code class="language-plaintext highlighter-rouge">n_heads</code>: 어텐션 헤드 수</li> <li><code class="language-plaintext highlighter-rouge">d_head</code>: 헤드 당 차원 (예: hidden_size=8192면 <code class="language-plaintext highlighter-rouge">d_head = 8192 / n_heads</code>).</li> <li><code class="language-plaintext highlighter-rouge">context_length</code>: 컨텍스트 길이 (예: 32k=32768).</li> <li><code class="language-plaintext highlighter-rouge">bytes_per_param</code>: 데이터 타입 (FP16=2 bytes, FP32=4 bytes).</li> </ul> <h3 id="예시">예시</h3> <p>LLaMA-65B 모델을 fp16 모드로 사용하는 경우 아래와 같이 계산할 수 있다.</p> <ul> <li><code class="language-plaintext highlighter-rouge">n_layers</code>: 80</li> <li><code class="language-plaintext highlighter-rouge">hidden_size</code>: 8192</li> <li><code class="language-plaintext highlighter-rouge">n_heads</code>: 64</li> <li><code class="language-plaintext highlighter-rouge">d_head</code>: 8192 / 64 = 128</li> <li><code class="language-plaintext highlighter-rouge">context_length</code>: 32768</li> <li><code class="language-plaintext highlighter-rouge">bytes_per_param</code>: 2 (FP16)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KV 캐시 메모리 (Bytes) = \
  2 * 80 * 64 * 128 * 32768 * 2 = \
  85,899,345,920 Bytes = 약 85.9 GB
</code></pre></div></div> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/docs/transformers/kv_cache">huggingface transformers - kv_cache</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Transformers"/><category term="LLM"/><category term="WIP"/><summary type="html"><![CDATA[The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).]]></summary></entry><entry><title type="html">LLM 이용해서 임베딩 모델의 품질 높이기</title><link href="https://zzong2006.github.io/blog/2025/improving-text-embeddings-with-llm/" rel="alternate" type="text/html" title="LLM 이용해서 임베딩 모델의 품질 높이기"/><published>2025-01-24T01:00:00+00:00</published><updated>2025-01-24T01:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/improving-text-embeddings-with-llm</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/improving-text-embeddings-with-llm/"><![CDATA[<p>Main Contribution</p> <ul> <li>leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages</li> <li>fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss.</li> </ul> <p>E5 and BGE employ a more complex multi-stage training paradigm that first pre-trains on billions of weakly-supervised text pairs, and then fine-tunes on several high-quality labeled datasets.</p> <p>Existing multi-stage approaches suffer from several drawbacks.</p> <ol> <li>They rely on manually collected datasets that are often constrained by the diversity of tasks and the coverage of languages.</li> </ol> <h2 id="proposed-method">Proposed Method</h2> <p>we use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool of candidate tasks, and then prompts the LLMs to generate data conditioned on a given task from the pool.</p> <p>To cover various application scenarios, we design <strong>multiple prompt templates</strong> for each task type and combine the generated data from different templates to boost diversity.</p> <p>Mistral-7B, when finetuned solely on synthetic data, attains competitive performance on the BEIR (Thakur et al., 2021) and MTEB(Muennighoff et al., 2023) benchmarks.</p> <p>Depending on the length of the query and document, we further divide asymmetric tasks into four subgroups: short-long match, long-short match, short-short match, and long-long match.</p> <p>For instance, short-long match tasks involve a short query and a long document, which is a typical scenario in commercial search engines.</p> <p>For each subgroup, we design a two-step prompt template that first prompts LLMs brainstorm a list of tasks, and then generates a concrete example conditioned on the task definition.</p> <h3 id="training">Training</h3> <p>쿼리와 문서의 끝에 <code class="language-plaintext highlighter-rouge">[EOS]</code> 토큰을 추가한 후, 이를 LLM에 입력하여 마지막 레이어의 <code class="language-plaintext highlighter-rouge">[EOS]</code> 벡터를 가져와 쿼리 및 문서 임베딩을 얻는다.</p> <p>Standard InfoNCE loss over the in-batch negatives and hard negatives</p> <p>(+) adopt the temperature-scaled cosine similarity function</p> <ul> <li>$τ$ is a temperature hyper-parameter, which is fixed to $0.02$ in our experiments.</li> </ul> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/pdf/2401.00368">improving-text-embeddings-with-large-language-models (arxiv)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="Retrieval"/><category term="Embedding"/><summary type="html"><![CDATA[Main Contribution]]></summary></entry><entry><title type="html">RAG 구축 레슨런</title><link href="https://zzong2006.github.io/blog/2025/enhancing-rag-a-study-of-best-practices/" rel="alternate" type="text/html" title="RAG 구축 레슨런"/><published>2025-01-21T01:00:00+00:00</published><updated>2025-01-21T01:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/enhancing-rag-a-study-of-best-practices</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/enhancing-rag-a-study-of-best-practices/"><![CDATA[<p>아래 9가지 RAG 성능을 향상시키기 위한 연구 주제에 대해 실험 기반의 empirical 조사 결과를 정리</p> <ol> <li>RAG 시스템에서 LLM의 크기가 응답 품질에 어떤 영향을 미치는가?</li> <li>미세한 프롬프트 차이가 검색과 생성의 수준에 크게 영향을 미칠 수 있는가?</li> <li>retrieve 할 문서의 청크 사이즈가 전체 성능에 어떤 영향을 미치는가?</li> <li>지식 베이스의 크기가 전체 성능에 어떤 영향을 미치는가?</li> <li>문서 컨텍스트를 얼마나 자주 업데이트해야 정확도를 최적화할 수 있는가?</li> <li>쿼리를 확장하면 모델의 정밀도가 향상되는가?</li> <li>Contrastive In-Context Learning 예제를 포함하면 RAG 생성에 어떤 영향을 미치는가?</li> <li>다국어 문서를 포함하면 RAG 시스템의 응답에 어떤 영향을 미치는가?</li> <li>문서 대신 문장을 가져오는 reranking 방식이 성능에 어떤 영향을 미치는가?</li> </ol> <h2 id="key-findings">Key Findings</h2> <ul> <li>RAG 에서 Contrastive In-Context Learning 은 성능 향상에 유의미한 영향을 미침</li> <li>정확하면서도 간결한 문서를 이용하는것이 성능에 핵심 요소</li> <li>쿼리 확장, 문서 크기, retrieval stride 같은 요소들은 의미 있는 개선을 가져오지 않았음</li> <li>RAG 지식 베이스의 크기도 반드시 중요한것은 아니며, 오히려 <strong>문서의 품질과 관련성</strong>이 중요</li> </ul> <h2 id="experiments">Experiments</h2> <h3 id="1-llm-크기에-따른-응답-품질-조사">(1) LLM 크기에 따른 응답 품질 조사</h3> <p><img src="https://i.imgur.com/O5llP9M.png" alt="Image" width="80%"/></p> <ul> <li>Mistral 기반의 7B 와 45B 모델을 baseline 으로 하여 성능 (TruthfulQA, MMLU) 비교</li> <li>모델 크기가 증가할수록 성능이 증가하는것은 맞지만, 특정 task 에 대해서는 성능 향상이 눈에 띌 정도는 아니다.</li> </ul> <h3 id="2-프롬프트에-따른-응답-품질-조사">(2) 프롬프트에 따른 응답 품질 조사</h3> <p><img src="https://i.imgur.com/fwfGJGi.png" alt="Image" width="80%"/></p> <ul> <li>사용자 물음에 진실하게 답변하라는 프롬프트 <code class="language-plaintext highlighter-rouge">Help</code> 와 창의적이고 강아지처럼 답변하라는 적대적인 프롬프트 <code class="language-plaintext highlighter-rouge">Advers</code> 프롬프트를 비교</li> <li><code class="language-plaintext highlighter-rouge">Advers</code> 쪽이 일관되게 낮은 성능을 보임.</li> <li>근데 실제 프롬프트를 보면 대놓고 <code class="language-plaintext highlighter-rouge">Advers</code> 쪽을 망쳐놔서 너무 당연한걸 실험한거 아닌지 모르겠음.</li> </ul> <h3 id="3-청크-사이즈에-따른-응답-품질-조사">(3) 청크 사이즈에 따른 응답 품질 조사</h3> <p><img src="https://i.imgur.com/2seUB76.png" alt="Image" width="80%"/></p> <ul> <li>청크 사이즈를 S, M, L, XL 로 나누어 성능 비교 (순서대로 48, 64, 128, 192 토큰 사이즈)</li> <li>top-2 개의 document 만 검색하여 LLM 에 입력후 성능을 측정함</li> <li>청크 사이즈가 클수록 좋은 성능을 확인했지만, 유의미할 정도는 아녔음</li> <li>청크 사이즈를 늘리는 것이 시스템 성능에 크게 영향을 미치지 않는다는 것을 시사</li> </ul> <h3 id="4-지식-베이스-크기에-따른-응답-품질-조사">(4) 지식 베이스 크기에 따른 응답 품질 조사</h3> <p>지식 베이스 크기를 증가시키면 더 많은 정보를 제공할 수 있지만, 관련성을 희석시키고 검색 속도를 늦출 수 있다. 반면, 더 작은 지식 베이스는 더 빠른 검색과 더 높은 관련성을 제공하지만 포괄적인 범위를 갖지 못한다. 일종의 trade-off.</p> <p><img src="https://i.imgur.com/UCqxmmM.png" alt="Image" width="80%"/></p> <ul> <li>문서 개수를 1k 또는 10k로 조절하고, top-2 또는 top-5 개의 문서를 검색하여 성능을 측정함</li> <li>실험 결과, 지식 베이스의 크기를 확장하는 것이 성능에 통계적으로 유의미한 영향을 미치지 않았으며, 성능 차이는 거의 없었음 –&gt; 이는 더 큰 지식 베이스나 추가 문서 검색이 RAG 시스템의 출력 품질을 반드시 개선하지는 않는다는 것을 보여줌.</li> <li>이유를 추정하자면, 추가 문서가 특정 쿼리에 대해 관련이 없거나 중복될 수 있기 때문일 수 있음</li> </ul> <h3 id="5-문서-업데이트-빈도-in-retrieval-stride">(5) 문서 업데이트 빈도 (in retrieval stride)</h3> <p>WIP</p> <h3 id="6-쿼리-확장">(6) 쿼리 확장</h3> <p>WIP</p> <h3 id="7-contrastive-in-context-learning-여부에-따른-응답-품질-조사">(7) Contrastive In-Context Learning 여부에 따른 응답 품질 조사</h3> <p>Contrastive In-Context Learning(ICL) 은 아래처럼 query 에 대한 올바른 정답과 잘못된 정답 (few-shot) 을 제공하는 것이다.</p> <p><strong>ICL 예시</strong></p> <blockquote> <p>You are a truthful expert question answering bot and should correctly and concisely answer the following question. Considering these examples: Question: $q$, Correct Answer: $\text{Answer}<em>{\text{correct}}$. Question: $q$, Incorrect Answer: $\text{Answer}</em>{\text{incorrect}}$. Question: $q$, Correct Answer:</p> </blockquote> <p><img src="https://i.imgur.com/sStpLZ6.png" alt="Image" width="80%"/></p> <ul> <li>실험 결과에서 보다시피 다른 방식들에 비해 가장 좋은 성능을 보였다.</li> <li>Doc 과 Doc+ 차이 여부는 negative example 의 추가 여부다. 추가하니 성능이 더 좋아졌음.</li> </ul> <h3 id="8-다국어">(8) 다국어</h3> <p>WIP</p> <h3 id="9-문서-대신-일부-문장에-집중할때의-응답-품질-조사">(9) 문서 대신 일부 문장에 집중할때의 응답 품질 조사</h3> <p>논문에서는 Focus mode 라고 불리는데, document 대신 n 개의 문장을 찾아서 LLM 에 입력하는 방식이다. 아래는 Focus mode 방식에 따른 실험 결과인데, 각 이름은 얼마나 많은 문장을 가져올지 결정하는 것이다. 예를 들어, 20Doc20S 라면, 20개의 document 에서 가장 중요한 문장 한개씩을 가져와 총 20개의 문장을 구성하는 것이다. 일종의 <strong>reranking 방식</strong>이라고 생각할 수 있겠다.</p> <p><img src="https://i.imgur.com/lC34C0z.png" alt="Image" width="80%"/></p> <ul> <li>더 많은 문서에서 문장을 가져올수록 성능이 향상되었지만 80개 이상으로 가져오니 큰 성능 향상이 없었다.</li> <li>120Doc 이 80Doc 보다 성능이 유의미하게 좋아지지 않는 이유는 (4) 의 지식 베이스 크기나 (3) 의 청크 사이즈가 커져도 큰 성능 향상이 없는것과 비슷하다고 생각할 수 있겠다.</li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2501.07391.pdf">Enhancing Retrieval-Augmented Generation: A Study of Best Practices (pdf)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="Retrieval"/><category term="WIP"/><summary type="html"><![CDATA[아래 9가지 RAG 성능을 향상시키기 위한 연구 주제에 대해 실험 기반의 empirical 조사 결과를 정리]]></summary></entry><entry><title type="html">ML Recap - Beta Distribution</title><link href="https://zzong2006.github.io/blog/2025/beta-distribution/" rel="alternate" type="text/html" title="ML Recap - Beta Distribution"/><published>2025-01-19T23:00:00+00:00</published><updated>2025-01-19T23:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/beta-distribution</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/beta-distribution/"><![CDATA[<p>확률 내용이긴 하지만, ML 에서도 자주 사용되는 beta distribution 에 대해서 정리해보자.</p> <hr/> <p>베타 분포는 확률의 분포를 나타내는 것으로 이해할 수 있다. 즉, 우리가 어떤 확률이 무엇인지 모를 때 그 확률의 모든 가능한 값을 나타낸다.</p> <blockquote> <p>The Beta distribution is best for representing <strong>a probabilistic distribution of probabilities</strong>: the case where we don’t know what a probability is in advance, but we have some reasonable guesses.</p> </blockquote> <p>예를 들어 어떤 야구 선수의 타율에 대해서 베타 분포를 사용할 수 있다. 평균 타율이 0.26 정도이고, 0.21 과 0.35 사이에 타율이 분포(prior)하고 있는 야구선수에 대해서 Beta 분포로 표현하자면 $\alpha = 81, \beta = 219$ 로 아래와 같이 모델링할 수 있다.</p> <p><img src="https://i.imgur.com/9V5Lg8o.png" alt="Image" width="50%"/></p> <p>베타 분포 밀도 그래프에서 x축은 선수의 타율을 나타낸다. 따라서 이 그래프에서 y축이 확률(또는 더 정확히는 probability density)일 뿐만 아니라 x축도 확률이다 (타율은 결국 안타의 확률이다).</p> <p>참고로 베타 분포의 평균은 $\alpha / (\alpha + \beta)$ 이다.</p> <p>베타 분포가 유용한 이유는 새로운 피드백에 대한 반영이 간단하다는 점이다. 예를 들어, 위의 선수가 300 번 나가서 100번 안타를 쳤다고 가정해보자. 이러한 정보를 반영하기 위해서 아래와 같은 수식을 활용할 수 있다.</p> \[\mbox{Beta}(\alpha_0+\mbox{hits}, \beta_0+\mbox{misses})\] <p>여기서 $\alpha_0$ 와 $\beta_0$ 는 초기 베타 분포의 파라미터이다. 위 안타 사례에 의하면 alpha 쪽은 100, beta 쪽은 200 (300 - 100) 을 증가시키면 된다.</p> <p><img src="https://i.imgur.com/DJdG0wJ.png" alt="Image" width="50%"/></p> <p>선수의 타율에 대해 더 잘 알게 되었으므로, 이제 곡선이 더 얇아지고 오른쪽으로 이동했다(더 높은 타율).</p> <h2 id="compared-to-the-binomial-distribution">compared to the binomial distribution</h2> <p>binomial distribution, with $s$ success and $f$ failures out of a total of $(s+f)$ trials.</p> \[P(s, f \vert \theta) = \binom{s + f}{s} \theta^s (1 - \theta)^f \tag{2}\] <p>Thomson Sampling models uncertainty by building a probability distribution from historical rewards and then samples from the distribution when choosing actions. In the simple case where rewards are binary, a Beta distribution is used. The Beta distribution takes two parameters, α and β, and the mean value of the distribution is α/α + β which can be thought of as successes / successes + failures. To select an action, we sample from each arm’s Beta distribution and choose the arm with the highest sampled values.</p> <h2 id="usage-of-the-beta-distribution">Usage of the Beta Distribution</h2> <p>An example of Thompson Sampling is Doordash’s bandits for cuisine recommendations. User preferences for a cuisine is modeled via Beta(α=number of orders of the cuisine, β=number of orders of other cuisines). When selecting a set of cuisine filters to show on the explore page, the value for each cuisine is sampled from the cuisine’s Beta distribution. These values are then sorted in descending order to select the top cuisines to display.</p> <p>Doordash shared how they warm-start their cuisine bandits via higher-level regional data. For each cuisine, they learn a bandit policy at multiple levels (i.e., regional, subregional, user). The top-level bandit is initialized at Beta(α=1, β=1). Then, for each lower-level bandit, they update α by adding the average number of orders for the cuisine (at that level) and update β by adding the average number of orders for other cuisines (at that level). Finally, for the user-level bandit, α and β are updated with the user’s order data. As a result, a new user’s cuisine bandit is warm-started with higher-level marketplace data before each new order updates the bandit with their personal preferences.</p> <p>Reference: <a href="https://arxiv.org/abs/2009.06546">Carousel Personalization in Music Streaming Apps with Contextual Bandits</a></p> <hr/> <p>when feedback is delayed, Thompson Sampling outperforms UCB. Delayed feedback, where user-item interactions are not processed immediately, is common for most real-world systems due to resource and run-time constraints. In this situation, because UCB selects arms deterministically, it chooses the same action until new feedback is incorporated. In contrast, because Thompson Sampling chooses actions stochastically by sampling from the posterior distribution, it randomizes over actions even without updated rewards. Yahoo’s evaluation of Thompson Sampling and Deezer’s music bandit observed that this led to wider exploration and thus better outcomes.</p> <p>The initialization strategy is important.</p> <ul> <li>Naive initialization: the prior was Beta(1, 1).</li> <li>Pessimistic initialization: the prior was Beta(1, 99)</li> </ul> <p>That pessimistic initialization performed better due to the lower prior probabilities which were more reflective of real-world reward.</p> <h2 id="references">References</h2> <ul> <li><a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution (wikipedia)</a></li> <li><a href="https://applyingml.com/resources/bandits/">Bandits</a></li> <li><a href="https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution/47782#47782">What is the intuition behind beta distribution?</a></li> </ul>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="machine-learning"/><category term="probability"/><category term="WIP"/><summary type="html"><![CDATA[확률 내용이긴 하지만, ML 에서도 자주 사용되는 beta distribution 에 대해서 정리해보자.]]></summary></entry><entry><title type="html">BGE 임베딩 학습 방법 탐방해보기</title><link href="https://zzong2006.github.io/blog/2025/bge-embed-train/" rel="alternate" type="text/html" title="BGE 임베딩 학습 방법 탐방해보기"/><published>2025-01-15T10:35:00+00:00</published><updated>2025-01-15T10:35:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/bge-embed-train</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/bge-embed-train/"><![CDATA[<p>성능 좋은 한국어 임베딩 중에 <a href="https://huggingface.co/BAAI/bge-multilingual-gemma2">BAAI/bge-multilingual-gemma2</a> 가 있다.</p> <p>이 임베딩 모델을 커스텀 데이터셋으로 튜닝하고 싶은데 어떤식으로 진행하면 좋을지 확인해보자.</p> <p><a href="https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder#2-data-format">FlagEmbedding</a> 에서 파인튜닝을 위한 간단한 문서를 찾아볼 수 있다.</p> <h2 id="dataset">Dataset</h2> <p>학습 데이터셋은 <a href="https://huggingface.co/datasets/hanhainebula/bge-multilingual-gemma2-data/viewer/multilingual_miracl/ko_train">hanhainebula/bge-multilingual-gemma2-data</a> 에서 확인할 수 있다. 구분하기 쉽게 한국어 데이터셋 위주로 구성했다.</p> <p>query, pos, neg, pos_scores, neg_scores, prompt 로 구성되어 있다. pos 와 neg 는 각각 쿼리와 관련된 문서들과 관련이 없는 문서들을 의미한다. pos 와 neg 모두 여러개의 sentences 로 구성되어 있다.</p> <p>pos_scores 와 neg_scores 는 pos, neg 의 각 문서에 대한 점수를 의미하고, knowledge distillation 과정에서 사용되는 점수인것으로 보인다. 구체적인 방법은 찾아봐야 할것 같음 (TODO).</p> <p>prompt 는 retrieval 과정에서 query 와 함께 사용할 문장인것으로 보인다.</p> <h3 id="예시-샘플">예시 샘플</h3> <ul> <li>query : e스포츠란?</li> <li>pos: E스포츠\n일렉트로닉 스포츠(), 또는 간단히 줄여서 e스포츠()는 컴퓨터 통신이나 인터넷 따위를 통해서 온라인상으로 이루어지는 게임을 통틀어 이르는 말이다. (.. 생략 ..)</li> <li>neg: 국제 e스포츠 연맹\n국제e스포츠연맹은 2014년 7월부터 종목에 따라 남성부와 여성부로 분리하던 정책을 개정하여 여성도 남성부에 참가할 수 있도록 하였다.</li> <li>pos_scores: 90.75</li> <li>neg_scores: 89.81</li> <li>prompt: Given a question, retrieve Wikipedia passages that answer the question.</li> </ul> <h2 id="finetuning-process">Finetuning process</h2> <p>Negative mining -&gt; Teacher Scores (optional) -&gt; Data split -&gt; Finetuning 순으로 진행된다.</p> <h3 id="negative-mining">Negative Mining</h3> <p>임베딩 모델을 학습할 때 negative examples 은 매우 중요하다. 만약 특정 query 에 대해 negative text 가 없다면, 전체 corpus 에서 랜덤으로 샘플링해서 negative 로 사용할 수 있다.</p> <p>FlagEmbedding 에서 지원하는 negative examples 샘플링은 상당히 직관적인데 (<a href="https://github.com/FlagOpen/FlagEmbedding/blob/master/scripts/hn_mine.py">github code</a>), 다음과 같은 스텝을 거쳐서 뽑는다.</p> <ol> <li>query 와 corpus 를 준비한다.</li> <li>faiss 를 이용해서 index 를 구성하고, 각 query 마다 top-k 문서를 corpus 에서 뽑는다 (일반적으로 2 ~ 200).</li> <li>뽑은 문서들 중에서 쿼리와 positive 관계가 있는 문서들은 제외하고 나머지를 뽑아서 negative 로 사용한다. <ol> <li>negative 수준의 난이도를 낮추고 싶다면, top-k 를 더 낮추면 된다 (60 ~ 300).</li> </ol> </li> <li>만약 뽑은 negative 가 충분하지 않다면, 전체 corpus 에서 랜덤하게 뽑아서 채워준다.</li> </ol> <h3 id="teacher-scores">Teacher Scores</h3> <p>reranker 모델(e.g. <a href="https://huggingface.co/BAAI/bge-reranker-v2-m3">BAAI/bge-reranker-v2-m3</a>)을 이용해서 query 와 각 pos, neg 문서들의 점수를 계산한다.</p> <p>계산된 점수는 실제 학습에서 아래와 같이 loss 계산에 사용된다 (<a href="https://github.com/FlagOpen/FlagEmbedding/blob/b7efd286adbecff049949f3717b21a7b21c9d5ed/FlagEmbedding/abc/finetune/embedder/AbsModeling.py#L280-L318">코드 참고</a>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">distill_loss</span><span class="p">(</span><span class="n">kd_loss_type</span><span class="p">,</span> <span class="n">teacher_targets</span><span class="p">,</span> <span class="n">student_scores</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">
  teacher_targets: (batch_size, group_size) / (world_size * batch_size, group_size)
  student_scores: (batch_size, group_size) / (world_size * batch_size, group_size)
  </span><span class="sh">"""</span>

  <span class="k">return</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span>
      <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">student_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">teacher_targets</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="p">)</span>
</code></pre></div></div> <p>여기서 group_size 는 <code class="language-plaintext highlighter-rouge">group_size = p_reps.size(0) // q_reps.size(0)</code> 과 같이 한 query 가 처리할 수 있는 pessage 수를 의미하는데, 각 그룹당 local 의 개념을 가지고 있는것 같다.</p> <h3 id="data-split-by-length">Data split (by length)</h3> <p>학습할 데이터를 일정 길이 구간에 따라 나눈다: [0, 500], [500, 1000], [1000, 1500] …</p> <p>여기서 데이터 길이란 각 샘플 (query, pos, neg) 중 가장 긴 문장에 대한 길이를 의미한다.</p>]]></content><author><name></name></author><category term="code-review"/><category term="embedding"/><category term="RAG"/><category term="LLM"/><category term="finetuning"/><category term="WIP"/><summary type="html"><![CDATA[성능 좋은 한국어 임베딩 중에 BAAI/bge-multilingual-gemma2 가 있다.]]></summary></entry><entry><title type="html">ANN 방법론중 하나인 HNSW 알고리즘 정리</title><link href="https://zzong2006.github.io/blog/2025/hnsw/" rel="alternate" type="text/html" title="ANN 방법론중 하나인 HNSW 알고리즘 정리"/><published>2025-01-15T00:35:00+00:00</published><updated>2025-01-15T00:35:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/hnsw</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/hnsw/"><![CDATA[<p>HNSW(Hierarchical Navigable Small World Graphs) 는 이름 그대로 계층적이지만 서로 이동 가능한, 여러 Small World를 만들어 그 안에서 근접 이웃을 탐색하는 방법이다.</p> <p>빠른 검색 결과 및 높은 정확도라는 장점을 제공하지만 메모리 사용량이 높은 단점이 있다.</p> <p>HNSW는 NSW에 skip list의 개념을 적용하여, NSW 그래프를 계층화한다. 가장 아래의 레이어는 모든 노드를 포함하며, 위로 갈수록 더 적은 노드로 구성된 형태의 그래프로 구성된다.</p> <h2 id="nsw-navigable-small-world-graphs">NSW (Navigable Small World Graphs)</h2> <p>우선 계층(Hierarchical)을 고려하기 전에 NSW(Navigable Small World Graphs) 를 먼저 생각해보자.</p> <p>NSW 에서는 검색 과정에서 임의로 선택된(또는 미리 정의된) 진입 노드(entry point)에서 시작하여 query 노드와 가까운 노드를 찾을때까지 진행하는데, 계속 찾는것은 아니고 일정 수준까지만 탐색을 수행한다.</p> <p><img src="https://i.imgur.com/6OcawHl.png" alt="Image" width="70%"/></p> <p>Small world 이론을 바탕으로, 적은 단계의 탐색만 거쳐도 충분히 쿼리와 근사한 노드를 반환할 수 있다는 가정이 깔려있다.</p> <p>이렇게 탐색의 정도를 제한하기 때문에, 데이터의 양이 많아져도 빠른 인덱싱 및 검색이 가능하다.</p> <h2 id="skip-list">Skip List</h2> <p>Skip list는 linked list 와 binary tree 자료 구조에 영감을 얻어서 만들어진 확률 기반 계층적(hierarchical) 자료 구조다.</p> <p>Skip list의 가장 하위 레벨의 층은 모든 데이터 노드를 포함하며, 상위 레벨의 층으로 갈수록 더 적은 수의 노드를 가진 linked list로 구성된다. 검색에 사용되는 링크는 다음 노드로 이어지는 기본 링크와 다른 레벨의 노드로 이어지는 링크로 구성된다.</p> <p>이러한 링크 구조는 전체 구조를 가로지르며 데이터를 빠르게 탐색할 수 있는 경로를 형성하는데, 검색, 삭제, 삽입 연산은 log 복잡도를 가진다.</p> <p><img src="https://i.imgur.com/hXuBvsR.gif" alt="Image" width="100%"/></p> <p>왜 확률적인 자료 구조로 불리는가? 그 이유는 새로운 노드를 삽입할때의 level 을 랜덤으로 정하기 때문이다.</p> <h2 id="hnsw">HNSW</h2> <p>HNSW는 skip list 구조를 채용하여 NSW 에 계층구조 형식을 추가한 것이다.</p> <p><img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe63ca5c638bc3cd61cc1cd2ab33b101d82170426-1920x1080.png&amp;w=3840&amp;q=75" alt="Image" width="80%"/></p> <h3 id="search">Search</h3> <p>검색 과정에서는 가장 위쪽 레이어에 있는 노드부터 시작하는데, 일반적으로 진입 노드들은 더 높은 차수의 노드(=여러 레이어에 걸쳐 있는 링크를 가진 노드)로 설정하는 편이다.</p> <p>각 레이어에서 탐욕적으로 가장 가까운 정점으로 이동하여 local minimum(query 와 가장 가까운 노드) 을 찾는다. 그리고 이 시점에서 하위 레이어로 이동하여 다시 검색을 시작한다. 이 과정을 반복하여 최하위 레이어(레이어 0)의 로컬 최소값을 찾을 때까지 진행한다.</p> <p>이렇게 찾은 노드들은 최종적으로 쿼리와 가장 가까운 노드가 된다.</p> <h3 id="construction">Construction</h3> <p>그래프 구성도 상위 레이어에서 시작된다. 어떤 레이어에 주어진 노드를 넣을지는 특정 확률 값으로 결정되는데, 일반적으로 level 이 낮을수록 높은 확률을 가진다.</p> <p>처음 해야할일은 query 와 가장 가까운 (local minimum) <code class="language-plaintext highlighter-rouge">ef</code>(<code class="language-plaintext highlighter-rouge">efConstruction</code>)개의 노드를 찾는것이다. 그리고 하위 레이어로 내려가서 똑같은 과정을 반복하고, 랜덤으로 선택된 레벨에 도달할때까지 반복한다.</p> <p>이후 각 레이어에서 찾은 <code class="language-plaintext highlighter-rouge">ef</code>개의 노드들은 신규 노드에 대한 이웃으로 추가되는데, 다 추가되는것은 아니고 <code class="language-plaintext highlighter-rouge">M</code>개만 추가된다. 단순한 기준은 노드들의 거리가 가까운 순서대로 추가하는것이다.</p> <p>이렇게 정해진 레이어로 내려가면서 노드들을 추가하는 과정을 반복하여 타겟 노드에 이웃을 추가해주면 그래프 구성은 완료된다.</p> <h2 id="parameters">Parameters</h2> <p>HNSW의 벡터 인덱싱 및 검색에는 몇 가지 파라미터를 설정해주어야 하는데, 이러한 파라미터와 그 의미는 다음과 같다.</p> <ul> <li>$M$: 각 노드가 가질 수 있는 최대 이웃의 수를 나타낸다. $M$ 값이 높아지면 검색 정확도가 향상될 수 있지만, 그만큼 메모리 사용량과 인덱스 생성 시간이 증가한다.</li> <li>$\text{efConstruction}$: 인덱스를 생성할 때 탐색 크기를 의미한다. 이 값이 높을수록 더 깊고 정확한 탐색이 가능해져 인덱스 품질이 향상되지만, 인덱스 생성 시간이 길어질 수 있다.</li> <li>$\text{efSearch}$: 검색 시 탐색 크기를 의미한다. 이 값이 높을수록 더 깊고 정확한 탐색이 가능해져 검색 정확도가 향상되지만, **검색 시간이 늘어날 수 있다.</li> <li>$L_{max}$: 노드가 가질 수 있는 최대 레벨을 정의한다. 일반적으로 자동으로 설정되지만, 필요에 따라 사용자가 직접 지정할 수도 있다. $L_{max}$는 그래프의 계층적 구조와 깊이에 영향을 미친다.</li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://github.com/nmslib/nmslib">Non-Metric Space Library (nmslib)</a></li> <li><a href="https://jerry-ai.com/30">검색증강생성(RAG) - 그래프 기반 벡터 인덱스 HNSW(Hierarchical Navigable Small World)</a></li> <li><a href="https://ketansingh.me/posts/lets-talk-skiplist/">Let’s Talk SkipList</a></li> <li><a href="https://www.pinecone.io/learn/series/faiss/hnsw/">Pinecone: hNSW</a></li> </ul>]]></content><author><name></name></author><category term="search"/><category term="ANN"/><summary type="html"><![CDATA[HNSW(Hierarchical Navigable Small World Graphs) 는 이름 그대로 계층적이지만 서로 이동 가능한, 여러 Small World를 만들어 그 안에서 근접 이웃을 탐색하는 방법이다.]]></summary></entry><entry><title type="html">임베딩도 더 좋은 데이터가 필요하다, KaLM-Embedding</title><link href="https://zzong2006.github.io/blog/2025/kalm-embedding/" rel="alternate" type="text/html" title="임베딩도 더 좋은 데이터가 필요하다, KaLM-Embedding"/><published>2025-01-14T23:00:00+00:00</published><updated>2025-01-14T23:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/kalm-embedding</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/kalm-embedding/"><![CDATA[<p>KaLM-Embedding이라는 multi-lingual 임베딩 모델을 소개한다. Qwen2-0.5B 기반 임베딩 모델에 좋은 데이터를 부어서 임베딩 성능을 높인 전략이다.</p> <h2 id="massive-dataset">Massive Dataset</h2> <p>기존의 유명한 임베딩 모델들과 비교했을때 fine-tuning 데이터 양의 비율을 크게 높인걸 확인할 수 있다.</p> <p><img src="https://i.imgur.com/DXEs65z.png" alt="Image" width="100%"/></p> <p>약 70개 이상의 학습 dataset 을 사용했으며, 대부분 중국어나 영어로 구성된 데이터로 구성되었다. 하지만 다국어 케이스에서도 성능이 좋았다고 함.</p> <h2 id="proposed-methods">Proposed Methods</h2> <p>더 많으면서 깨끗하고, 다양하고, 도메인 특화된 학습 데이터를 구하는 세가지 방법</p> <ol> <li>Diversity: LLM에서 추출한 다양한 예제를 생성하는 페르소나 기반 합성 데이터</li> <li>Quality: 덜 유익한 샘플을 제거하는 ranking consistency filtering</li> <li>Efficiency: 학습 효율성을 높이는 semi-homogeneous task batch</li> </ol> <h3 id="1-persona-based-synthetic-data">(1) Persona-based Synthetic Data</h3> <ul> <li>Qwen2-72B-Instruct를 사용하여 55만 개의 합성 데이터를 생성: 6 types of tasks with 40k unique instructions</li> <li>Persona Hub 에서 시스템 프롬프트를 랜덤으로 추출하여 domain diversity 를 높임</li> </ul> <h3 id="2-ranking-consistency-filtering">(2) Ranking Consistency Filtering</h3> <p><img src="https://i.imgur.com/EHjO00K.png" alt="Image" width="100%"/></p> <p>어떤 쿼리는 너무 광범위해서 모든 문서와 연관도가 높아서, 하나의 쿼리가 여러개의 문서에 positive case 로 고려될 수 있다. 이런 경우 hard negative mining 과정에서 noisy 한 데이터로 처리될 수 있다.</p> <p>이러한 문제를 해결하기 위해, query 와 corpus 의 유사도를 계산시, positive document 가 top-k 안에 들지 못한 경우 해당 데이터를 제거하는 방식을 취했다.</p> <h3 id="3-semi-homogeneous-task-batch">(3) Semi-homogeneous Task Batch</h3> <p><strong>최종 모델에 적용된 방법은 아니지만</strong>, 제안 느낌으로 소개되었다.</p> <p><img src="https://i.imgur.com/8daD7KG.png" alt="Image" width="70%"/></p> <p>동일한(homogeneous) task 내에서 negative sample 을 뽑으면 in-batch negative sample 에서 hardness 를 높이므로 성능에 좋은 영향을 줄 수 있지만, 동시에 대용량 데이터를 다룬다고 생각해보면 false negative 의 위험도 감수해야 한다.</p> <p>이러한 문제를 해결하기 위해, 각 task 에서 일정 비율로 샘플링을 진행하고 다른 배치로 할당하는 방식을 시도했다고 한다.</p> <p>아래는 semi-homogeneous ratio (다른 taks 끼리 얼마나 데이터를 섞을지 비율) 에 따른 MTEB 점수 결과이다.</p> <p><img src="https://i.imgur.com/SktSktk.png" alt="Image" width="50%"/></p> <p>보다시피 semi-homogeneous ratio 가 높으면 성능이 감소해서 최종 모델에는 채택이 되지 않았다 (근데 왜 논문에는 소개했을까..).</p> <h3 id="4-matryoshka-representation-learning-mrl">(4) Matryoshka Representation Learning (MRL)</h3> <p>이 외에도 Matryoshka Representation Learning 방식을 취해서 896, 512, 256, 128, and 64 차원에 대해서 loss weight 를 1.0, 0.3, 0.2, 0.1, and 0.1 로 설정하고 학습을 진행했다고 한다.</p> <p>아래 실험 결과에서도 차원수가 작으면 MRL 방식이 효과가 좋은것으로 보인다.</p> <p><img src="https://i.imgur.com/fkz5LZg.png" alt="Image" width="50%"/></p> <h2 id="ablation-study">Ablation Study</h2> <p><img src="https://i.imgur.com/5rYgKvt.png" alt="Image" width="70%"/></p> <p>가장 효과가 좋았던건 task instruction … 인데, 이건 임베딩 모델을 파인튜닝 할때 task 마다 instruction 을 다르게 주는 방식이다.</p> <p>아래는 그 예시 (역시 diversity 를 높이는 방식이 효과가 좋다).</p> <p><img src="https://i.imgur.com/7yQXsZj.png" alt="Image" width="100%"/></p> <p>그나저나 페르소나 데이터는 왜 ablation 결과에 없을까?</p> <h2 id="느낀점">느낀점</h2> <p>많은 시도를 해본것 같은데, 결국 다양한 데이터가 중요하다는걸 다시 느낀 report paper 였다.</p> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2501.01028">KaLM-Embeddings: Superior Training Data Brings A Stronger Embedding Model</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="embedding"/><category term="RAG"/><category term="LLM"/><summary type="html"><![CDATA[KaLM-Embedding이라는 multi-lingual 임베딩 모델을 소개한다. Qwen2-0.5B 기반 임베딩 모델에 좋은 데이터를 부어서 임베딩 성능을 높인 전략이다.]]></summary></entry><entry><title type="html">ML Recap - Basic Feature Engineering</title><link href="https://zzong2006.github.io/blog/2025/basic-feature-engineering/" rel="alternate" type="text/html" title="ML Recap - Basic Feature Engineering"/><published>2025-01-12T23:00:00+00:00</published><updated>2025-01-12T23:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/basic-feature-engineering</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/basic-feature-engineering/"><![CDATA[<p>numerical &amp; categorical 데이터에 대해서 간단한 전처리 기술을 복습해보자.</p> <h2 id="normalization">Normalization</h2> <p>…</p> <h2 id="one-hot-encoding">One-hot encoding</h2> <p>Categorical 데이터를 Numerical 데이터로 변환하는 방법이다. 즉, class 를 표현하기 위한 sparse binary vector 를 생성하는 것이다.</p> <h3 id="pros-and-cons">Pros and Cons</h3> <ul> <li>Pros: 단순하고 직관적이다.</li> <li>Cons: class 수가 많을경우 차원이 커져서 overfitting 이 발생할 수 있다. 그렇다고 너무 class 수를 제한하면 이 역시 underfitting 이 발생할 수 있다.</li> </ul> <h3 id="multicollinearity-방지-기법-dropping-first-class">multicollinearity 방지 기법: dropping first class</h3> <p>만약 선형 회귀 모델에 one-hot encoding 을 적용했을때, 다중공선성 이슈를 완화하기 위해 첫번째 카테고리를 제거하는 방법이 있다.</p> <p>예시를 통해 직관적으로 생각해보자. A, B, C 라는 세 개의 카테고리가 있을때 이를 one-hot encoding 하면 다음과 같은 행렬이 생성된다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A, B, C
[0, 1, 0]
[1, 0, 0]
[0, 0, 1]
</code></pre></div></div> <p>여기서 A와 B 가 0일때 C 는 확정적으로 1이다. 즉, A 와 B 의 값에 따라 C 의 값이 정해지는 것이므로 상관관계가 있다고 볼 수 있다.</p> <p>이때, 첫번째 카테고리를 제거하면 다음과 같은 행렬이 생성된다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B, C
[1, 0]
[0, 1]
[0, 0]
</code></pre></div></div> <p>여기서 [0, 0] 은 당연히 A에 해당된다고 유추가 가능하다.</p> <p>Correlation 이슈를 해결하기 위한 또 다른 방법은 PCA (Principal Component Analysis) 를 통해서 차원을 축소하는 방법이 있다.</p> <h2 id="other-encoding-methods">Other Encoding methods</h2> <h3 id="1-ordinal-encoding">(1) Ordinal encoding</h3> <p>범주형 데이터를 순서를 고려한 숫자로 변환하는 방법이다.</p> <p>예를 들어, 색상을 빨강, 노랑, 파랑으로 표현하면 1, 2, 3 으로 변환하는 것이다. 또는 어떤 구간을 나눠서 각 구간에 해당되는 클래스를 숫자로 변환하는 것도 가능하다.</p> <p>직관적인 방법이지만, 서로 관련없는 클래스도 하나의 숫자로 변환하는 것이 문제가 될 수 있어서 underfitting 이 발생할 수 있다.</p> <h3 id="2-target-encoding">(2) Target encoding</h3> <p>범주형 데이터를 대상 변수의 평균값으로 변환하는 방법이다. 범주형 데이터의 cardinality 가 높아서 one-hot encoding 을 적용하기 어려울 때 이 방법이 효과적이다. 주로 zip code 나 region 과 같은 feature 에 적용된다.</p> <p>예를 들어, 온라인 쇼핑몰에서 고객의 거주 지역에 따른 구매 금액을 예측하려고 한다고 가정해보자.</p> <table> <thead> <tr> <th>거주 지역</th> <th>구매 금액(타겟)</th> </tr> </thead> <tbody> <tr> <td>A</td> <td>100</td> </tr> <tr> <td>B</td> <td>150</td> </tr> <tr> <td>A</td> <td>200</td> </tr> <tr> <td>C</td> <td>300</td> </tr> <tr> <td>B</td> <td>100</td> </tr> <tr> <td>C</td> <td>400</td> </tr> </tbody> </table> <p>각 거주 지역에 대한 구매 금액의 평균을 계산하여 target encoding 을 수행할 수 있다.</p> <table> <thead> <tr> <th>거주 지역</th> <th>구매 금액 평균(인코딩 값)</th> </tr> </thead> <tbody> <tr> <td>A</td> <td>(100 + 200) / 2 = 150</td> </tr> <tr> <td>B</td> <td>(150 + 100) / 2 = 125</td> </tr> <tr> <td>C</td> <td>(300 + 400) / 2 = 350</td> </tr> </tbody> </table> <p>이런 방식은 orinal encoding 보다는 평균과 같은 통계적인 방식을 접목하므로, 좀 더 smooth 한 결과로 생각할 수 있다.</p> <h2 id="references">References</h2> <ul> <li><a href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features">categorical-features (sklearn)</a></li> </ul>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="machine-learning"/><category term="feature-engineering"/><category term="pre-processing"/><category term="WIP"/><summary type="html"><![CDATA[numerical &amp; categorical 데이터에 대해서 간단한 전처리 기술을 복습해보자.]]></summary></entry><entry><title type="html">ML Recap - Linear Regression</title><link href="https://zzong2006.github.io/blog/2025/linear-regression/" rel="alternate" type="text/html" title="ML Recap - Linear Regression"/><published>2025-01-12T21:00:00+00:00</published><updated>2025-01-12T21:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/linear-regression</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/linear-regression/"><![CDATA[<h2 id="lasso-regression">Lasso Regression</h2> <p>Lasso 는 L1 정규화(Regularization) 라고 불리는 sparse 한 선형 회귀 모델이다.</p> <p>이 모델의 특징으로는 variable selection 과 regularization 을 통해 모델의 예측 정확도와 interpretability 을 향상시키는 것이다.</p> <p>아래는 regression 의 cost function $J(\mathbf{w})$ 이다. 이 함수를 최소화 하는 계수 $\mathbf{w}$ 를 찾는 것이 목적이다.</p> \[J(\mathbf{w})=\frac{1}{N} \sum_{i=1}^N\left(y_i-\mathbf{w}^T \mathbf{x}_i\right)^2+\lambda \sum_{j=1}^m\left|w_j\right|\] <p>regression parameter $\lambda$ 는 성능에 큰 영향을 미친다. 아래 그림과 같이 $\lambda$ 가 커질수록 모델의 bias 가 커지며, 반대로 모델의 variance 는 작아진다.</p> <p><img src="https://i.imgur.com/wYVpKQH.png" alt="20250112223049" width="85%"/></p> <h3 id="limitation">Limitation</h3> <p>Lasso 는 독립 변수들 간에 강한 상관관계가 존재하는 현상인 다중공선성(multicollinearity) 문제를 해결하기엔 한계가 있다. 왜냐하면 lasso 는 다중공선성에 관련된 변수들 중 임의로 하나만 살려두기 때문에, 모델 해석에 어려움을 줄 수 있기 때문이다 (동시에 성능 하락은 덤).</p> <h2 id="ridge-regression">Ridge Regression</h2> <p>Ridge regression 은 과적합을 방지하기 위해 회귀 계수(coefficient)의 제곱의 합을 작게 만들어 준다. 하지만, 이 방식은 변수 선택을 하지 않기 때문에 모델을 해석하는데 어려움이 있다.</p> <p>반대로 Lasso 는 회귀 계수의 절대값의 합을 작게 만들어 준다. 이 방식은 특정 계수를 0으로 만들어서 예측에 영향을 가지 않도록 만들기 때문에 변수 선택 과정에서 이점이 있다.</p> <p>아래는 Lasso 와 Ridge 모델이 두개의 parameter $\beta_1$ 과 $\beta_2$ 로 표현될때, 각 방식 별 가능한한 parameter 의 조합을 나타낸 그래프이다.</p> <p><img src="https://i.imgur.com/vDcn664.png" alt="20250112221358" width="80%"/></p> <h3 id="as-classification">As classification</h3> <p>Ridge 방식은 regression 뿐만 아니라 분류 task 를 해결하는데도 자주 사용되는데, 종종 Logistic Regression 보다 선호되는 경우가 많다. 왜냐하면 계수를 찾기위해서는 아래와 같이 projection matrix 를 한번만 계산하면 되기 때문이다.</p> \[\hat{\beta}_{\text {ridge }}=\left(X^T X+\lambda I\right)^{-1} X^T y\] <p>그래서 계산 효율적인 특성이 있으며 대규모 데이터셋을 처리할때 자주 사용된다. 또한 accuracy 나 recall 에서도 SVM 이나 Logistic Regression 대비해서 유사한 결과를 얻는 사례가 많다.</p> <h2 id="elasticnet">ElasticNet</h2> <p>ElasticNet 은 선형 회귀 모델에서 L1 regularization (Lasso) 과 L2 regularization (Ridge) 를 결합한 정규화 기법이다. 이 모델은 계수의 절대값의 합과 제곱의 합을 작게 만들어 준다.</p> \[J(\mathbf{w})=\frac{1}{N} \sum_{i=1}^N\left(y_i-\mathbf{w}^T \mathbf{x}_i\right)^2+\alpha\left(r \sum_{i=1}^n\left|\mathbf{w}_i\right|+\left(1-r\right) \sum_{i=1}^n \mathbf{w}_i^2\right)\] <ul> <li>$\alpha$ 는 regularization 의 강도를 조절하는 매개변수이다.</li> <li>$r$ 은 L1 과 L2 의 비율을 조절하는 매개변수이다 (0 &lt; $r$ &lt; 1). $r$ 이 0 에 가까울수록 L2 정규화가 강해지고, 1 에 가까울수록 L1 정규화가 강해진다.</li> </ul> <p>ElasticNet 은 상관관계가 높은 변수들이 함께 선택되거나 제외되는 그룹화 효과(grouping effect)를 보인다. 그래서 ElasticNet은 여러 특성(feature)들이 서로 상관관계가 있을 때 유용하다. Lasso는 이러한 특성들 중 하나를 무작위로 선택하는 경향이 있는 반면, ElasticNet은 이들 모두를 선택하는 경향이 있다.</p> <h3 id="limitation-1">Limitation</h3> <p>L1 과 L2 비율을 조절하는 과정이 필요하므로, 기존의 lasso 나 ridige 보다 튜닝이 복잡해지거나 계산 비용이 증가할 수 있다. 또한 L1 방식과 달리 모델의 해석이 좀 더 복잡해지는 것도 단점으로 생각할 수 있다.</p> <h2 id="skicit-learn-을-통한-구현">skicit-learn 을 통한 구현</h2> <p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html">LassoCV</a> 는 cross-validation 을 통해 최적의 $\alpha$ 를 찾아서 회귀 모델을 튜닝해준다. 여기서는 regularization parameter $\alpha$ 로 표현한다.</p> <p>이 외에도 LogisticRegressionCV, ElasticNetCV 도 있다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="nc">LassoCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">reg</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">reg</span><span class="p">.</span><span class="n">alpha_</span>
</code></pre></div></div> <p>위 코드는 아래 그래프처럼 여러 fold 에 대한 MSE 를 계산하고, 가능한 alpha 값들 중 최적의 alpha 값을 찾아준다.</p> <p><img src="https://i.imgur.com/IPGrB1d.png" alt="20250112225530" width="60%"/></p> <h2 id="references">References</h2> <ul> <li><a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso Regression (wikipedia)</a></li> <li><a href="https://scikit-learn.org/stable/modules/linear_model.html#using-cross-validation">Using cross-validation (sklearn)</a></li> </ul>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="machine-learning"/><category term="linear-regression"/><summary type="html"><![CDATA[Lasso Regression]]></summary></entry></feed>