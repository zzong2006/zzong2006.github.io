<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-19T09:11:26+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">Believe I.Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ML Recap - BM25 &amp;amp; TF-IDF</title><link href="https://zzong2006.github.io/blog/2025/bm25/" rel="alternate" type="text/html" title="ML Recap - BM25 &amp;amp; TF-IDF"/><published>2025-02-03T21:00:00+00:00</published><updated>2025-02-03T21:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/bm25</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/bm25/"><![CDATA[<h3 id="bm25와-tf-idf의-관계-단계별-설명"><strong>BM25와 TF-IDF의 관계: 단계별 설명</strong></h3> <p>(1) TF (Term Frequency): 단어 빈도</p> <ul> <li>정의: 특정 문서 내에서 <strong>단어가 등장하는 빈도</strong>를 측정합니다.</li> <li>예: 문서 A에 “사과”가 5번 등장 → <code class="language-plaintext highlighter-rouge">TF("사과", A) = 5</code></li> </ul> <p>(2) TF-IDF: TF와 IDF의 결합</p> <p>단어의 <strong>문서 내 중요도(TF)</strong>와 <strong>전체 집합에서의 희귀도(IDF)</strong>를 곱한 값입니다.</p> \[TF\text{-}IDF(t, d) = TF(t, d) \times IDF(t)\] <p><strong>특정 문서에 집중적으로 등장하면서 다른 문서에는 드문 단어</strong>를 강조합니다.</p> <hr/> <h3 id="tf-idf의-한계">TF-IDF의 한계</h3> <ol> <li>문서 길이 정규화 부재: <ul> <li>긴 문서는 단어가 자연스럽게 더 자주 등장하여 TF가 높아지는 경향이 있지만, 실제 관련성은 낮을 수 있습니다.</li> <li>예: 1000단어 문서에서 “사과” 10회 → TF = 0.01</li> <li>100단어 문서에서 “사과” 5회 → TF = 0.05 (더 높음)</li> </ul> </li> <li><strong>단어 빈도 포화 문제</strong>: <ul> <li>단어가 특정 횟수 이상 등장해도 점수가 선형적으로 증가합니다.</li> <li>예: “사과”가 10번 등장한 문서와 20번 등장한 문서의 TF 차이가 큼.</li> <li>실제로는 10번 이후 추가 등장은 관련성을 크게 높이지 않을 수 있습니다.</li> </ul> </li> <li><strong>단순한 IDF 계산</strong>: <ul> <li>희귀도만 고려하고, 단어 분포의 복잡성을 반영하지 못합니다.</li> </ul> </li> </ol> <hr/> <h3 id="bm25-tf-idf의-발전형">BM25: TF-IDF의 발전형</h3> <ul> <li>정의: TF-IDF의 한계를 해결하기 위해 <strong>문서 길이 정규화</strong>와 <strong>단어 빈도 포화 제어</strong>를 도입한 랭킹 함수입니다.</li> </ul> \[\text{BM25}(t, d) = IDF(t) \times \frac{TF(t, d) \times (k_1 + 1)}{TF(t, d) + k_1 \times \left(1 - b + b \times \frac{\text{문서 길이}}{\text{평균 문서 길이}}\right)}\] <ul> <li>$k_1$: <strong>단어 빈도 포화 제어 파라미터</strong> (기본값 = 1.2)</li> <li>$b$: <strong>문서 길이 정규화 강도</strong> (기본값 = 0.75)</li> </ul> <hr/> <h3 id="bm25의-핵심-개선점">BM25의 핵심 개선점</h3> <p>(1) 단어 빈도 포화 제어</p> <ul> <li>문제: TF-IDF는 단어 빈도가 높을수록 점수가 무한정 증가합니다.</li> <li>해결: BM25는 $k_1$ 파라미터로 <strong>포화 지점</strong>을 조절합니다.</li> </ul> <p>예: $k_1 = 1.2$일 때, 단어 빈도가 5번 이상이면 점수 증가가 둔해집니다.</p> \[\frac{TF \times (k_1 + 1)}{TF + k_1} \quad \text{(점수 증가율 감소)}\] <p>(2) 문서 길이 정규화</p> <ul> <li>문제: 긴 문서는 단어가 우연히 더 많이 등장할 수 있습니다.</li> <li>해결: $b$ 파라미터로 <strong>문서 길이 편차를 보정</strong>합니다. <ul> <li>문서 길이가 평균보다 길면 TF 점수를 낮춥니다.</li> </ul> </li> </ul> \[1 - b + b \times \frac{\text{문서 길이}}{\text{평균 문서 길이}}\] <p>(3) IDF의 개선된 계산</p> \[IDF(t) = \log\left(\frac{N - n_t + 0.5}{n_t + 0.5}\right)\] <ul> <li>$N$: 전체 문서 수</li> <li>$n_t$: 단어 $t$가 등장한 문서 수</li> <li>$n_t$가 작을 때 안정적인 값을 제공합니다.</li> </ul> <hr/> <h3 id="요약">요약</h3> <table> <thead> <tr> <th><strong>기능</strong></th> <th><strong>TF-IDF</strong></th> <th><strong>BM25</strong></th> </tr> </thead> <tbody> <tr> <td><strong>단어 빈도</strong></td> <td>선형 증가</td> <td>포화 지점 이후 증가율 감소 ($k_1$)</td> </tr> <tr> <td><strong>문서 길이</strong></td> <td>고려하지 않음</td> <td>정규화 적용 ($b$)</td> </tr> <tr> <td><strong>IDF 계산</strong></td> <td>$\log(N/n_t)$</td> <td>보정된 $\log((N - n_t + 0.5)/(n_t + 0.5))$</td> </tr> <tr> <td><strong>적용 분야</strong></td> <td>기본 검색, 작은 데이터셋</td> <td>대규모 검색 엔진 (Elasticsearch 등)</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="machine-learning"/><category term="retrieval"/><summary type="html"><![CDATA[BM25와 TF-IDF의 관계: 단계별 설명]]></summary></entry><entry><title type="html">ML Recap - Confusion Matrix</title><link href="https://zzong2006.github.io/blog/2025/confusion-matrix/" rel="alternate" type="text/html" title="ML Recap - Confusion Matrix"/><published>2025-02-03T21:00:00+00:00</published><updated>2025-02-03T21:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/confusion-matrix</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/confusion-matrix/"><![CDATA[<p>Confusion Matrix 는 분류 모델의 예측 결과를 실제 정답과 비교하여, 모델이 어느 정도 잘 예측했는지를 한눈에 보여주는 표(매트릭스)이다.</p> <table> <thead> <tr> <th> </th> <th>예측: Positive</th> <th>예측: Negative</th> </tr> </thead> <tbody> <tr> <td>실제: Positive</td> <td>True Positive (TP)</td> <td>False Negative (FN)</td> </tr> <tr> <td>실제: Negative</td> <td>False Positive (FP)</td> <td>True Negative (TN)</td> </tr> </tbody> </table> <ul> <li>True Positive (TP): 실제로 Positive인 데이터를 모델이 Positive로 올바르게 예측한 경우</li> <li>False Negative (FN): 실제로 Positive인데 모델이 Negative로 잘못 예측한 경우</li> <li>False Positive (FP): 실제로 Negative인데 모델이 Positive로 잘못 예측한 경우</li> <li>True Negative (TN): 실제로 Negative인 데이터를 모델이 Negative로 올바르게 예측한 경우</li> </ul> <h3 id="metrics-using-confusion-matrix">Metrics using Confusion Matrix</h3> <p>Confusion Matrix 의 각 셀에 있는 값을 바탕으로 여러 성능 지표를 계산할 수 있다.</p> <p>(1) 정확도 (Accuracy): 전체 샘플 중 올바르게 예측한 비율</p> \[\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}\] <p>(2) 정밀도 (Precision): Positive로 예측한 것 중 실제로 Positive인 비율</p> \[\text{Precision} = \frac{TP}{TP + FP}\] <p>(3) 재현율 (Recall) 또는 민감도 (Sensitivity): 실제 Positive 샘플 중 Positive로 올바르게 예측한 비율</p> \[\text{Recall} = \frac{TP}{TP + FN}\] <p>(4) F1 Score: 정밀도와 재현율의 조화평균</p> \[\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\] <h2 id="application-ragas">Application: RAGAS</h2> <p>RAGAS 는 Retrieval-Augmented Generation 의 성능을 측정하는 방법이다.</p> <ul> <li>Context Precision (문맥 정밀도) <ul> <li>모델이 검색한(또는 제공한) 문맥 중 실제로 관련성이 높은 부분의 비율을 측정합니다.</li> <li>질문과 컨텍스트 간의 관련성을 LLM 또는 임베딩 유사도로 측정.</li> <li>높을수록 검색된 컨텍스트가 질문 해결에 적합함을 의미.</li> </ul> </li> <li>Context Recall (문맥 재현율) <ul> <li>실제로 필요한 문맥 중 모델이 검색해낸 비율을 나타냅니다.</li> <li>Ground Truth 정답과 검색된 컨텍스트의 내용 일치도 측정.</li> <li>높을수록 검색 시스템이 핵심 정보를 누락하지 않았음을 의미.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="machine-learning"/><category term="metrics"/><summary type="html"><![CDATA[Confusion Matrix 는 분류 모델의 예측 결과를 실제 정답과 비교하여, 모델이 어느 정도 잘 예측했는지를 한눈에 보여주는 표(매트릭스)이다.]]></summary></entry><entry><title type="html">AB Test 개념 정리</title><link href="https://zzong2006.github.io/blog/2025/ab-test/" rel="alternate" type="text/html" title="AB Test 개념 정리"/><published>2025-02-03T02:18:00+00:00</published><updated>2025-02-03T02:18:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/ab-test</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/ab-test/"><![CDATA[<p>CTR(Click-Through Rate) 예측 기반 A/B 테스트에서 A 로직이 B 로직보다 통계적으로 유의미하게 우수함을 증명하려면?</p> <p>A와 B의 CTR 차이가 통계적으로 유의미한지 확인 (예: 95% 신뢰수준).</p> <ul> <li>대상: 무작위로 분할된 사용자 그룹 (A/B 그룹).</li> </ul> <h3 id="notation">Notation</h3> <ul> <li>$CTR_A$ = A 그룹 클릭 수 / A 그룹 노출 수</li> <li>$CTR_B$ = B 그룹 클릭 수 / B 그룹 노출 수</li> </ul> <h3 id="통계적-검정">통계적 검정</h3> <ul> <li>귀무가설 (H₀): $CTR_A = CTR_B$ (A와 B의 성능 차이 없음).</li> <li>대립가설 (H₁): $CTR_A &gt; CTR_B$ (A가 B보다 우수함).</li> </ul> <h3 id="수식-z-검정">수식: Z-검정</h3> <p>Z-검정은 표본 분포가 정규분포를 따른다는 가정 아래 수행됩니다. CTR과 같은 비율 데이터는 본질적으로 이항분포(Binomial Distribution)를 따르지만, 중심극한정리(Central Limit Theorem) 덕분에 표본 크기가 충분히 클 경우 정규분포로 근사할 수 있습니다.</p> \[z=\frac{(\mathrm{A} \text { 의 CTR }-\mathrm{B} \text { 의 CTR) }}{\text { 표준 오차(Standard Error) }}=\frac{C T R_A-C T R_B}{\sqrt{p_{\text {pooled }} \cdot\left(1-p_{\text {pooled }}\right) \cdot\left(\frac{1}{n_A}+\frac{1}{n_B}\right)}}\] <ul> <li>$p_{\text {pooled }}=\frac{\text { 클릭 총합}}{\text { 노출 총합}}=\frac{C_A+C_B}{N_A+N_B}$ <ul> <li>$p_{\text {pooled }} \cdot\left(1-p_{\text {pooled }}\right)$ : CTR의 분산 (비율의 변동성).</li> </ul> </li> <li>$n_A, n_B: \mathrm{A} / \mathrm{B}$ 그룹 노출 수 <ul> <li>$\frac{1}{n_A}+\frac{1}{n_B}$ : 표본 크기가 클수록 분산이 작아짐.</li> </ul> </li> </ul> <h4 id="수식-해석">수식 해석</h4> <ul> <li>분자: A와 B의 CTR 차이 (실제 관측된 차이).</li> <li>분모: “우연히 발생할 수 있는 차이의 평균적인 크기” (표준 오차). → Z값이 크다 = 관측된 차이가 우연보다 의미 있다! <ul> <li>표준 오차는 “비율 차이가 우연히 변동할 수 있는 정도”를 나타냅니다.</li> <li>두 그룹의 데이터가 많을수록(표본 크기 ↑), 표준 오차는 작아집니다.</li> </ul> </li> <li>Z값은 표준 정규 분포 (평균 0, 표준편차 1)와 비교됩니다. <ul> <li>임계값: 95% 신뢰수준에서 1.645 (단측 검정 기준).</li> <li>즉, Z값이 1.645 보다 크면 귀무가설 기각 성공 (A와 B의 차이는 통계적으로 유의미하다).</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="Experiment"/><summary type="html"><![CDATA[CTR(Click-Through Rate) 예측 기반 A/B 테스트에서 A 로직이 B 로직보다 통계적으로 유의미하게 우수함을 증명하려면?]]></summary></entry><entry><title type="html">Airflow &amp;amp; PySpark 개념 정리</title><link href="https://zzong2006.github.io/blog/2025/airflow/" rel="alternate" type="text/html" title="Airflow &amp;amp; PySpark 개념 정리"/><published>2025-02-03T02:18:00+00:00</published><updated>2025-02-03T02:18:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/airflow</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/airflow/"><![CDATA[<h2 id="airflow">Airflow</h2> <p>Airflow에서 모든 워크플로우는 DAG로 표현됩니다. DAG는 순환이 없는 방향 그래프이며, 특정 Task가 다른 Task보다 먼저 수행되어야 한다면 Directed Edge로 순서를 정의합니다.</p> <p>주요 구성 요소는 다음과 같습니다.</p> <ol> <li> <p>DAG (Directed Acyclic Graph): 워크플로우 전체 구조를 정의하는 그래프로, 태스크 간의 실행 순서와 의존 관계를 나타냅니다. DAG는 순환이 없는 방향성 그래프로, 각 노드는 태스크를 나타내며, 엣지를 통해 실행 순서를 결정합니다.</p> </li> <li> <p>Task: DAG 내의 개별 실행 단위로, 실제 작업(예: 데이터 처리, 스크립트 실행 등)을 수행합니다.</p> </li> <li> <p>Operator: 특정 작업을 실행하기 위한 템플릿 클래스입니다. 예를 들어, BashOperator, PythonOperator, SparkSubmitOperator 등이 있으며, 이를 통해 표준화된 작업 실행 방식을 제공합니다.</p> </li> <li> <p>Sensor: 특정 조건(예: 파일 존재, 특정 이벤트 발생 등)이 충족될 때까지 대기하는 특수한 태스크입니다.</p> </li> <li> <p>Scheduler: DAG 파일을 주기적으로 스캔하여 태스크를 스케줄링하고 실행 순서를 결정하는 컴포넌트입니다.</p> </li> <li> <p>Executor: 태스크를 실제로 실행하는 백엔드로, LocalExecutor(단일 머신), CeleryExecutor(분산 환경), KubernetesExecutor(컨테이너 기반 실행) 등이 있습니다.</p> </li> </ol> <h2 id="pyspark">PySpark</h2> <p>Apache Spark의 파이썬 인터페이스로, 대규모 데이터 처리와 분산 컴퓨팅을 지원합니다.</p> <h3 id="architecture">Architecture</h3> <p>Spark 아키텍처는 다음과 같습니다.</p> <ul> <li>Driver: 애플리케이션의 메인 프로세스로, 전체 작업의 제어와 스케줄링을 담당합니다.</li> <li>Executors: 분산 환경에서 실제 작업을 실행하는 워커 프로세스입니다.</li> <li>Cluster Manager: 클러스터 내 자원(메모리, CPU 등)을 관리하며, Spark Standalone, YARN, Mesos, Kubernetes 등 다양한 옵션이 있습니다.</li> </ul> <h3 id="lazy-evaluation">lazy evaluation</h3> <p>Spark는 실제 연산을 바로 수행하지 않고, Transformation을 적용해 DAG(작업 그래프)를 구성한 후, Action이 호출될 때 전체 DAG를 최적화하여 실행합니다. 이로 인해 불필요한 계산을 줄이고 효율적인 실행 계획을 세울 수 있습니다.</p> <h3 id="transformation-vs-action">Transformation vs. Action</h3> <ul> <li>Transformation: 새로운 RDD나 DataFrame을 생성하는 연산으로, map, filter, join 등이 있으며, 실제 실행은 지연됩니다.</li> <li>Action: 실제 결과를 반환하거나 데이터를 저장하는 연산으로, count, collect, save 등이 있습니다. Action 호출 시 DAG가 실행됩니다.</li> </ul> <p>PySpark 작업 성능 최적화를 위한 전략(예: 파티션 수 조절, 셔플 최소화, 데이터 스키마 최적화 등)</p> <ul> <li>데이터를 여러 파티션으로 분할하여 작업을 병렬 처리합니다. 파티션 수를 적절히 조절하여 오버헤드를 줄이고, 병렬성을 극대화할 수 있습니다.</li> <li>셔플 최소화: 불필요한 데이터 이동(셔플)을 줄이기 위해 연산 순서를 최적화하고, <code class="language-plaintext highlighter-rouge">repartition()</code>, <code class="language-plaintext highlighter-rouge">coalesce()</code>를 사용합니다.</li> <li>추가 최적화: 데이터 스키마 최적화, predicate pushdown, 캐시 활용 등 다양한 기법을 통해 작업 성능을 향상시킬 수 있습니다.</li> </ul> <h3 id="파티셔닝">파티셔닝</h3> <ul> <li><code class="language-plaintext highlighter-rouge">repartition()</code>: 파티션 수를 증가/감소할 때 사용하며, 풀 셔플(Full Shuffle) 발생.</li> <li><code class="language-plaintext highlighter-rouge">coalesce()</code>: 파티션 수를 감소할 때 사용하며, 셔플 없이 인접 파티션을 병합 (성능 효율적).</li> <li>Too Few Partitions: 병렬성 감소 → 리소스 미활용.</li> <li>Too Many Partitions: 태스크 오버헤드 증가 → 스케줄링 지연.</li> </ul> <p>데이터 스큐(Data Skew) 문제</p> <ul> <li>Salting: 임의의 키를 추가해 데이터 분산</li> <li>파티션 수 조절: 파티션 수를 적절히 조절하여 데이터 분산을 최적화</li> </ul> <h3 id="메모리-관리-및-캐싱">메모리 관리 및 캐싱</h3> <ul> <li>메모리 관리: Spark는 데이터를 분산 메모리에 저장하여 빠른 연산을 가능하게 합니다.</li> <li>캐싱/Persistence: <code class="language-plaintext highlighter-rouge">cache()</code>, <code class="language-plaintext highlighter-rouge">persist()</code> 메서드를 사용하여 자주 재사용되는 데이터셋을 메모리에 저장함으로써, 반복 연산 시 재계산 비용을 절감할 수 있습니다. 이는 작업 성능을 크게 향상시키는 중요한 최적화 기법입니다.</li> </ul>]]></content><author><name></name></author><category term="mlops"/><category term="Airflow"/><summary type="html"><![CDATA[Airflow]]></summary></entry><entry><title type="html">네이버 컨퍼런스 DAN 24 정리</title><link href="https://zzong2006.github.io/blog/2025/naver-dan-2024/" rel="alternate" type="text/html" title="네이버 컨퍼런스 DAN 24 정리"/><published>2025-02-02T12:18:00+00:00</published><updated>2025-02-02T12:18:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/naver-dan-2024</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/naver-dan-2024/"><![CDATA[<p>DAN 발표를 보면서 네이버가 현재 어떤 방식으로 개발을 진행하고 있는지 정리해본다.</p> <h2 id="models">Models</h2> <p>추천/광고에 해당되는 파운데이션 모델을 따로 개발하고 있음</p> <ul> <li>언어 모델은 결국 next-token prediction 이므로, 입력으로 활용하는 pretext 를 얼마나 잘 구성하는지가 중요함</li> <li>파운데이션 모델을 활용한 downstream task 는 pretext task 로 표현할 수 있다</li> <li>pretrain loss 줄이는 가장 쉬운 방법은 scale-up 을 통한 computation 을 늘리는것</li> </ul> <h3 id="clip-for-e-commerce">CLIP For E-Commerce</h3> <ul> <li>쇼핑에서는 중복되는 상품이 많기 때문에, 이를 해결하지 않고서는 학습이 잘 안된다 <ul> <li>카탈로그 정보를 활용하여 동일한 카탈로그면 동일한 상품이라고 판단하여 학습하지 않도록 유도</li> </ul> </li> <li>기존의 constrative Learning 은 모든 negative-positive pair 를 동일한 선상으로 생각하지만, 사람은 이러한 차이를 상대적으로 생각한다 (Pairtial False Negative Problem).</li> <li>상품 텍스트만 가지고는 Masked Language Modeling 과 같이 빈칸 추론을 위한 충분한 맥락을 제공하고 있지 않다.</li> <li>eFoundation 이 제대로 학습되었다면 임베딩을 통한 상품의 이해를 표현할 수 있을 뿐만 아니라, 상품 관련 정보를 “생성” 할 수 있어야 한다.</li> <li>학습된 image encoder 가 제대로 학습되었는지 판단하기 위해, 기존의 LLM 모델이나 Masking 모델에 adapt layer 를 붙여서 성능이 잘 나오는지 확인한다.</li> </ul> <h2 id="metrics">Metrics</h2> <p>Search: nDCG, MRR, Recall, Precision</p> <h3 id="ndcg">nDCG</h3> \[\mathrm{DCG} @ k=\sum_i^k \frac{\mathrm{rel}_i}{\log _2(i+1)}\] <ul> <li>$\text{rel}_i$: ( i )번째 아이템의 관련성 점수 (예: 0=비관련, 1=관련, 2=매우 관련).</li> <li>( k ): 상위 ( k )개 아이템만 고려 (예: DCG@5는 상위 5개 평가).</li> </ul> \[\mathrm{nDCG} @ k=\frac{\mathrm{DCG} @ k}{\mathrm{DCG} @ k_{\text {ideal }}}\] <ul> <li>$k_{\text {ideal }}$: 이상적인 순서에서 상위 ( k )개 아이템의 관련성 점수.</li> </ul> <h2 id="embedding">Embedding</h2> <p>Language: ColBERT, ColBERTv2 Multi-modal: eCLIP (CLIP For E-Commerce), BLIP, LLaVA</p> <h2 id="ranking">Ranking</h2> <ul> <li>XTR Ranking</li> </ul> <h2 id="reference">Reference</h2> <p>[팀네이버 컨퍼런스 DAN 24]</p> <ul> <li><a href="https://tv.naver.com/v/67452264">벡터 검색의 정점에 오르다: 최적의 뉴럴 검색 엔진으로 업그레이드 하기</a></li> <li><a href="https://tv.naver.com/v/67444172">네이버 검색이 이렇게 좋아졌어? LLM의 Re-Ranking Ability 검색에 이식하기</a></li> <li><a href="https://tv.naver.com/v/67445059">LLM 기반 추천/광고 파운데이션 모델</a></li> <li><a href="https://tv.naver.com/v/67444878">eFoundation: 상품을 딱 잘 표현하는 임베딩을 만들었지 뭐야 ꒰⍢꒱ 완전 럭키비키잔앙 ☘︎</a></li> <li><a href="https://tv.naver.com/v/67453467">사람을 대신해야 진짜 AI지? : LLM 기반 임베딩부터 검색 품질 자동 평가 모델까지</a></li> <li><a href="https://tv.naver.com/v/67445325">사용자 경험을 극대화하는 AI 기반 장소 추천 시스템 : LLM과 유저 데이터의 융합</a></li> </ul>]]></content><author><name></name></author><category term="conference"/><category term="NAVER"/><category term="DAN"/><category term="2024"/><summary type="html"><![CDATA[DAN 발표를 보면서 네이버가 현재 어떤 방식으로 개발을 진행하고 있는지 정리해본다.]]></summary></entry><entry><title type="html">DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델</title><link href="https://zzong2006.github.io/blog/2025/deepseek-r1/" rel="alternate" type="text/html" title="DeepSeek-R1, o1 을 이기는 중국의 reasoning 모델"/><published>2025-01-30T12:18:00+00:00</published><updated>2025-01-30T12:18:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/deepseek-r1</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/deepseek-r1/"><![CDATA[<p>최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.</p> <p>도대체 어떤 모델을 만들었는지 한번 살펴보자.</p> <h2 id="introduction">Introduction</h2> <p>OpenAI o1 은 inference-time 이 확장될수록 수학, 코딩, 과학 추론 능력이 향상되는것을 보여준 모델이다. 이후 효율적인 test-time scaling 을 위해 프로세스 기반 보상 모델(PRM), 강화 학습(RL), 몬테카를로 트리 탐색(MCTS) 등의 기술을 시도해봤지만 o1 의 아성을 넘기에는 한계가 있었다.</p> <p>DeepSeek 은 DeepSeek-V3-Base를 기본 모델로 사용하고 GRPO(Shao et al., 2024)를 RL 프레임워크로 적용해 추론 성능을 개선했다. 이는 SFT 를 제외한, 순수 RL 과정을 통해 <strong>자기 진화(self-evolution)</strong>하며 추론 능력을 발전시킬 수 있는 잠재력을 탐구한 시도로 생각할 수 있겠다.</p> <p>수천번의 RL 단계를 거치면서 흥미로운 추론 행동이 자연스럽게 발현된 DeepSeek-Zero 는 초창기 o1-0912 급 모델의 추론 수준을 보였으나, 가독성과 언어가 혼합되어 나오는 문제를 발견했다.</p> <p>이런 문제를 해결하기 위해 수천개의 cold-start 데이터만을 모아 v3-base 모델을 먼저 파인튜닝하고, 이후 zero 와 비슷한 RL 단계를 거치면서 훈련을 진행했다. RL 과정에서 수렴 단계로 도달한 경우, rejection sampling 을 통해 새로운 sft 데이터를 생성하고, 기존 DeepSeek-V3 의 지도 학습 데이터와 합쳐 v3-base 모델을 다시 학습했다. 이런 과정을 반복해서 만들어진 최종 deepseek-r1 checkpoint 는 o1-1217 와 동등한 수준의 추론 능력을 달성했다.</p> <h2 id="주요-모델">주요 모델</h2> <p>DeepSeek-R1-Zero, DeepSeek-R1, Qwen/Llama 기반의 1.5B~70B 수준 지식 증류 모델 공개.</p> <h3 id="1-deepseek-r1-zero">(1) DeepSeek-R1-Zero</h3> <p>SFT 없이 RL로 훈련된 초기 모델이다. 추론 능력은 뛰어나지만 가독성(poor readability), language mixing 이슈가 존재한다고 한다.</p> <p>RL 학습을 위해서 GRPO (Group Relative Policy Optimization) 방식을 적용했다. 일반적으로 강화 학습 과정에서는 policy 와 critic 모델을 따로 가져가는데, critic 모델 대신 어떤 하나의 그룹의 점수들로 critic 의 평가를 대체하는 전략을 활용한다.</p> <p>GRPO 학습 과정에서는 어떤 질문 $q$ 가 주어졌을 때, 이전 정책 $\pi_{\theta_{\text{old}}}$로부터 여러 개의 출력 ${o_1, o_2, \cdots, o_G}$을 생성하고, 보상(점수) ${r_1, r_2, \dots, r_G}$을 구한뒤, 이를 바탕으로 정책 모델 $\pi_{\theta}$를 최적화한다. 이때, 점수들은 그룹 내부의 평균 및 표준편차로 정규화하여 “어느 후보가 상대적으로 좋았는지의 개념인 advantage $A_i$ 를 계산한다. 즉, 우도(advantage)를 바탕으로 새 정책 $\pi_{\theta}$를 업데이트한다.</p> <p>그럼 보상(점수)는 어떻게 계산헀을까? Rule based 기반의 채점 방식을 사용했는데, 수학이나 코딩 문제처럼 정답이 명확한 경우에만 점수를 주도록 설계했다고 한다. 다른 신경망 모델을 이용해서 점수를 채점하는 방식은 사용하지 않았다고 하는데, 그 이유는 reward hacking 문제나 reward model 자체를 재학습 하는 과정이 복잡해지고 난이도가 상승하기 때문이라고 한다.</p> <p>응답을 생성할때는 특정 content 에 대한 bias 를 제한하기 위해 아래처럼 고정된 프롬프트를 사용했다고 한다.</p> <p><img src="https://i.imgur.com/aHE9I7H.png" alt="20250130130652" width="90%"/></p> <p>이렇게 학습을 할수록 모델의 응답 길이가 늘어나면서 동시에 aha moment 가 발생한다고 한다. aha moment 는 모델이 문제 해결 과정에서 초기 접근법을 다시 평가하며 사고 시간을 더 많이 할당하는 법이 자연스럽게 발현되는 현상을 의미한다. 즉, 문제를 해결하는 구체적 방법을 명시적으로 가르치지 않고도, 그저 올바른 보상 체계를 부여하기만 하면 모델이 스스로 고도의 문제 해결 전략을 창출해낸다는 점이라고 한다.</p> <p><img src="https://i.imgur.com/scvFkSj.png" alt="20250130131632" width="90%"/></p> <p>위 내용처럼 <code class="language-plaintext highlighter-rouge">Wait, wait. Wait.</code> 과 같은 문장과 함께 모델이 문제의 접근 방식을 다시한번 스스로 생각하는 모습을 볼 수 있다.</p> <p>이렇게 학습된 Zero 모델은 위에서 언급했듯이 추론 능력을 크게 끌어올릴 수 있었지만, 추론 과정에서 가독성이 떨어지고 여러 언어가 섞여서 응답하는 문제가 있다고 한다.</p> <h3 id="2-deepseek-r1">(2) DeepSeek-R1</h3> <p>DeepSeek-R1 모델은 초기 모델인 DeepSeek-R1-Zero 에서 발견된 문제를 완화한 모델이다. RL 적용전에 cold-start 데이터와 multi-stage 훈련을 진행했고, 그 결과 OpenAI-o1-1217과 유사한 성능을 달성했다고 한다.</p> <h4 id="cold-start-data">Cold-start Data</h4> <p>Zero 에서는 초기 RL 학습시 불안정한 모습을 보이는 경우가 많았다고 한다. 이런 문제를 해결하기 위해 초기 모델에 cold-start 데이터를 이용해서 학습을 진행했는데, cold-start 데이터는 다양한 시도를 진행했다고 한다.</p> <ol> <li>긴 CoT를 사례로 삼아 few-shot 프롬프트를 적용</li> <li>모델에 직접 reflection과 verification 과정을 포함해 상세한 답변을 생성하도록 지시하는 방법</li> <li>DeepSeek-R1-Zero의 출력을 읽기 쉬운 형태로 정리한 뒤 Human annotators가 후처리를 거쳐 결과를 개선하는 방법</li> </ol> <p>주로 데이터 형태는 <code class="language-plaintext highlighter-rouge">|special_token|&lt;reasoning_process&gt;|special_token|&lt;summary&gt;</code> 로 표현되었는데, reasoning_process는 쿼리에 대한 CoT(Chain of Thought)를, summary는 추론 결과를 요약하는 용도로 사용했다.</p> <p>인간의 사전 지식(prior)을 고려해 콜드스타트 데이터를 신중히 설계하면, DeepSeek-R1-Zero와 비교했을 때 더 나은 성능을 발휘한다고 한다 (역시 정성적으로 살펴본 고퀄리티 데이터도 중요한거 같다).</p> <h4 id="language-consistency-reward">Language Consistency Reward</h4> <p>Cold-start 데이터로 초벌된 v3-base 모델은 RL 학습을 거치게 되는데, zero 에서 발견한 language-mix 이슈를 해결하기 위해 <strong>언어 일관성 보상(Language Consistency Reward)</strong>을 추가한다. 이는 목표로 하는 언어가 CoT 과정에서 얼마나 차지하는지의 비율로 계산된다. 비록 ablation study 과정에서 이런 정렬(언어 일관성)을 적용하면 모델 성능이 다소 떨어지는 것으로 나타났으나, 그럼에도 인간이 선호하는 형태에 가까워져 가독성이 높아지는 장점이 있다.</p> <p>즉, GRPO 에서 보상을 계산할때 정확도 뿐만 아니라 언어 일관성 보상을 합산해 최종 보상을 구성하고, 이 보상을 활용해 파인튜닝된 모델에 RL 훈련을 적용하며, 수렴(convergence)을 이룰 때까지 모델을 학습한다.</p> <h4 id="rejection-sampling-for-sft">Rejection Sampling for SFT</h4> <p>이후 수렴된 체크포인트를 활용하여 cold-start 데이터를 보강하기 위한 추가적인 SFT 데이터를 수집한다고 한다. 초기 콜드스타트 데이터가 주로 추론 위주였다면, 이번 단계에서는 쓰기(writing), 롤플레이(role-playing) 등 범용 과제 수행 능력을 높이기 위해 다른 도메인 데이터도 함께 포함한다.</p> <p>총 80 만 건을 수집하는데, 약 20만건은 추론, 약 60만건은 writing, QA 등의 비추론 데이터로 구성한다.</p> <p>추론 데이터의 경우 (Zero 모델 phase 에서의) rule based 방식 더불어 DeepSeek-V3 모델을 이용한 reward 를 계산하여 데이터의 퀄리티를 측정했다. 즉, 정답(ground-truth)과 모델 예측값을 DeepSeek-V3에 입력해 데이터 수준의 적절성을 판단하는 방식이다.</p> <p>60만개의 비추론 데이터의 경우, DeepSeek-V3 의 SFT 데이터를 재활용했다고 하며, 이렇게 모인 80만개의 데이터셋을 DeepSeek-V3-Base 에 2 epoch 동안 재학습 시켰다고 한다.</p> <h4 id="apply-additional-rl-improve-helpfulness--harmlessness">Apply additional RL: improve helpfulness &amp; harmlessness</h4> <p>모델의 <strong>도움(Helpfulness)</strong>과 <strong>무해성(Harmlessness)</strong>을 높이는 동시에 추론 능력까지 한층 더 다듬는 것을 목표로, 두 번째 강화 학습 단계를 진행했다고 한다.</p> <p>추론 데이터는 이전에 설명했던것과 동일하게 정확도를 기반으로 보상을 계산했고, 범용 데이터의 경우 reward model 기반의 preference pairs 를 구성했다고 한다. 이 외에도 Helpfulness 를 평가하기 위해 r1 모델이 응답한 summary 사용자에게 얼마나 유용하고 관련성 있는 응답을 제공하는지 초점을 두고 평가하도록 하거나, 전체 응답 과정에서 발생할 수 있는 유해한 컨텐츠를 찾아내서 harmlessness 를 개선했다고 한다.</p> <h3 id="3-distillation-작은-모델에-추론-능력-이식하기">(3) Distillation: 작은 모델에 추론 능력 이식하기</h3> <p>DeepSeek-R1과 같은 추론 능력을 더 효율적인 소형 모델에게도 이식하기 위해, 약 80만 건(800k)**으로 구성된 데이터셋을 활용해 Qwen, Llama 등 오픈소스 모델을 직접 파인튜닝했다고 한다.</p> <p>실험 결과, 이러한 SFT 기반의 직접적인 증류(distillation) 방식만으로도 소형 모델의 추론 능력이 현저히 향상된다는 사실을 확인했으며, 공개된 distillation 모델들은 RL을 추가로 도입하면 성능이 크게 높아질 수 있음에도 따로 진행하지는 않았다고 한다.</p> <h2 id="lesson-learned">Lesson Learned</h2> <p>지식 증류 vs. RL</p> <ol> <li>소형 모델은 RL만으로는 성능 한계 존재 → 지식 증류가 효율적.</li> <li>단, 지능 한계 돌파에는 대형 모델과 대규모 RL 필요.</li> </ol> <p>실패 사례:</p> <ul> <li>PRM(Process Reward Model): 보상 해킹 및 복잡성 문제.</li> <li>MCTS(Monte Carlo Tree Search): 토큰 생성 공간의 복잡성으로 확장 어려움.</li> </ul> <h2 id="limitations">Limitations</h2> <p>강력한 모델이지만 o1 과 비슷한 느낌의 단점들이 존재하는 것으로 보인다.</p> <p><strong>(1) General Capability</strong></p> <p>함수 호출, 멀티턴 대화, 복잡한 롤플레잉, JSON 출력과 같은 작업에서 DeepSeek-V3에 비해 부족한 면</p> <p><strong>(2) Language Mixing</strong></p> <p>DeepSeek-R1은 현재 중국어와 영어에 특화되어 있으므로, 다른 언어 쿼리를 처리할 때 언어 혼합 문제가 발생할 수 있다. 예컨대 영어 외 언어로 쿼리를 입력해도, 영어로 추론 과정을 작성하고 응답을 출력할 수 있다.</p> <p><strong>(3) Prompting Engineering</strong></p> <p>모델이 프롬프트(prompt)에 민감하게 반응한다.Few-shot 프롬프트 설정을 할 경우, 성능이 지속적으로 저하되는 경향을 보인다. 따라서 가장 효율적인 결과를 얻기 위해서는 제로샷(zero-shot) 방식으로 문제를 직접 설명하고 출력 형식을 지정하는 방법을 권장한다.</p> <p><strong>(4) Software Engineering</strong></p> <p>추론 시간이 길어지면 RL 프로세스의 효율에 영향을 미치기 때문에, 대규모 RL이 소프트웨어 엔지니어링 과제에 광범위하게 적용된 적은 아직 많지 않다. 이로 인해 DeepSeek-R1은 소프트웨어 엔지니어링 벤치마크에서 DeepSeek-V3 대비 현저한 성능 향상을 보이진 못했다고 한다.</p> <h2 id="느낀점">느낀점</h2> <p>SFT 전에 RL 만 적용해도 성능이 좋아진다고 하는데, 추론 능력의 경우 이런 접근이 유용한것 같다.</p> <p>하지만 정답이 명확하지 않은 일반 케이스의 경우, 역시 reward model 이나 사람이 평가한 피드백이 필수로 들어가야 하는것 같다.</p> <p>그리고 Zero 에 SFT 목적의 cold-start 데이터를 넣은것도, 완벽하게 SFT 에서 벗어나긴 힘들어 보이는 느낌. 하지만 기존에 대용량 SFT 데이터에 의존하는 것보다는 훨씬 발전된 느낌이 들었다.</p> <p>결국 더 많은 문제집 (?) 을 넣고 학습할수록 모델이 좋아진다.. 의 느낌으로 이해하면 될려나.</p> <p>여전히 추론 모델이 가지고 있는 단점인 함수 호출, 멀티턴 대화 등을 못하는 이슈는 해결해야될 과제로 보인다. 이래서 4o 같은 모델로 응답을 생성하고, 정답을 판단하는건 o1 같은 모델이 하는게 좀 더 자연스러워 보인다.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1 (github)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="DeepSeek"/><category term="LLM"/><category term="RL"/><summary type="html"><![CDATA[최근 DeepSeek 에서 추론 모델 R1 을 발표했는데, 그 성능이 대단하여 nvidia 주가에도 큰 영향을 미칠 정도로 주목을 받고있다.]]></summary></entry><entry><title type="html">KV-Cache 에 대해 알아보자</title><link href="https://zzong2006.github.io/blog/2025/kv-cache/" rel="alternate" type="text/html" title="KV-Cache 에 대해 알아보자"/><published>2025-01-24T10:00:00+00:00</published><updated>2025-01-24T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/kv-cache</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/kv-cache/"><![CDATA[<p>The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).</p> <p>Therefore, when iteratively calling forward() instead of the generate() method, it’s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape (batch_size, past_kv_length + new_tokens_length). This is usually handled internally when you call generate() method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.</p> <h2 id="kv-cache-계산-공식">KV Cache 계산 공식</h2> <p>모델을 vllm 같은 프레임워크에서 배포하기 위해 메모리가 충분한지 계산을 해야한다.</p> <p>이때 아래와 같은 공식을 사용할 수 있다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KV 캐시 메모리 (Bytes) = \
  2 * n_layers * n_heads * d_head * context_length * bytes_per_param
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">2</code>: Key와 Value를 모두 저장하므로 2배.</li> <li><code class="language-plaintext highlighter-rouge">n_layers</code>: 트랜스포머 레이어 수</li> <li><code class="language-plaintext highlighter-rouge">n_heads</code>: 어텐션 헤드 수</li> <li><code class="language-plaintext highlighter-rouge">d_head</code>: 헤드 당 차원 (예: hidden_size=8192면 <code class="language-plaintext highlighter-rouge">d_head = 8192 / n_heads</code>).</li> <li><code class="language-plaintext highlighter-rouge">context_length</code>: 컨텍스트 길이 (예: 32k=32768).</li> <li><code class="language-plaintext highlighter-rouge">bytes_per_param</code>: 데이터 타입 (FP16=2 bytes, FP32=4 bytes).</li> </ul> <h3 id="예시">예시</h3> <p>LLaMA-65B 모델을 fp16 모드로 사용하는 경우 아래와 같이 계산할 수 있다.</p> <ul> <li><code class="language-plaintext highlighter-rouge">n_layers</code>: 80</li> <li><code class="language-plaintext highlighter-rouge">hidden_size</code>: 8192</li> <li><code class="language-plaintext highlighter-rouge">n_heads</code>: 64</li> <li><code class="language-plaintext highlighter-rouge">d_head</code>: 8192 / 64 = 128</li> <li><code class="language-plaintext highlighter-rouge">context_length</code>: 32768</li> <li><code class="language-plaintext highlighter-rouge">bytes_per_param</code>: 2 (FP16)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KV 캐시 메모리 (Bytes) = \
  2 * 80 * 64 * 128 * 32768 * 2 = \
  85,899,345,920 Bytes = 약 85.9 GB
</code></pre></div></div> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/docs/transformers/kv_cache">huggingface transformers - kv_cache</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Transformers"/><category term="LLM"/><category term="WIP"/><summary type="html"><![CDATA[The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).]]></summary></entry><entry><title type="html">LLM 이용해서 임베딩 모델의 품질 높이기</title><link href="https://zzong2006.github.io/blog/2025/improving-text-embeddings-with-llm/" rel="alternate" type="text/html" title="LLM 이용해서 임베딩 모델의 품질 높이기"/><published>2025-01-24T01:00:00+00:00</published><updated>2025-01-24T01:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/improving-text-embeddings-with-llm</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/improving-text-embeddings-with-llm/"><![CDATA[<p>Main Contribution</p> <ul> <li>leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages</li> <li>fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss.</li> </ul> <p>E5 and BGE employ a more complex multi-stage training paradigm that first pre-trains on billions of weakly-supervised text pairs, and then fine-tunes on several high-quality labeled datasets.</p> <p>Existing multi-stage approaches suffer from several drawbacks.</p> <ol> <li>They rely on manually collected datasets that are often constrained by the diversity of tasks and the coverage of languages.</li> </ol> <h2 id="proposed-method">Proposed Method</h2> <p>we use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool of candidate tasks, and then prompts the LLMs to generate data conditioned on a given task from the pool.</p> <p>To cover various application scenarios, we design <strong>multiple prompt templates</strong> for each task type and combine the generated data from different templates to boost diversity.</p> <p>Mistral-7B, when finetuned solely on synthetic data, attains competitive performance on the BEIR (Thakur et al., 2021) and MTEB(Muennighoff et al., 2023) benchmarks.</p> <p>Depending on the length of the query and document, we further divide asymmetric tasks into four subgroups: short-long match, long-short match, short-short match, and long-long match.</p> <p>For instance, short-long match tasks involve a short query and a long document, which is a typical scenario in commercial search engines.</p> <p>For each subgroup, we design a two-step prompt template that first prompts LLMs brainstorm a list of tasks, and then generates a concrete example conditioned on the task definition.</p> <h3 id="training">Training</h3> <p>쿼리와 문서의 끝에 <code class="language-plaintext highlighter-rouge">[EOS]</code> 토큰을 추가한 후, 이를 LLM에 입력하여 마지막 레이어의 <code class="language-plaintext highlighter-rouge">[EOS]</code> 벡터를 가져와 쿼리 및 문서 임베딩을 얻는다.</p> <p>Standard InfoNCE loss over the in-batch negatives and hard negatives</p> <p>(+) adopt the temperature-scaled cosine similarity function</p> <ul> <li>$τ$ is a temperature hyper-parameter, which is fixed to $0.02$ in our experiments.</li> </ul> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/pdf/2401.00368">improving-text-embeddings-with-large-language-models (arxiv)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="Retrieval"/><category term="Embedding"/><summary type="html"><![CDATA[Main Contribution]]></summary></entry><entry><title type="html">RAG 구축 레슨런</title><link href="https://zzong2006.github.io/blog/2025/enhancing-rag-a-study-of-best-practices/" rel="alternate" type="text/html" title="RAG 구축 레슨런"/><published>2025-01-21T01:00:00+00:00</published><updated>2025-01-21T01:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/enhancing-rag-a-study-of-best-practices</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/enhancing-rag-a-study-of-best-practices/"><![CDATA[<p>아래 9가지 RAG 성능을 향상시키기 위한 연구 주제에 대해 실험 기반의 empirical 조사 결과를 정리</p> <ol> <li>RAG 시스템에서 LLM의 크기가 응답 품질에 어떤 영향을 미치는가?</li> <li>미세한 프롬프트 차이가 검색과 생성의 수준에 크게 영향을 미칠 수 있는가?</li> <li>retrieve 할 문서의 청크 사이즈가 전체 성능에 어떤 영향을 미치는가?</li> <li>지식 베이스의 크기가 전체 성능에 어떤 영향을 미치는가?</li> <li>문서 컨텍스트를 얼마나 자주 업데이트해야 정확도를 최적화할 수 있는가?</li> <li>쿼리를 확장하면 모델의 정밀도가 향상되는가?</li> <li>Contrastive In-Context Learning 예제를 포함하면 RAG 생성에 어떤 영향을 미치는가?</li> <li>다국어 문서를 포함하면 RAG 시스템의 응답에 어떤 영향을 미치는가?</li> <li>문서 대신 문장을 가져오는 reranking 방식이 성능에 어떤 영향을 미치는가?</li> </ol> <h2 id="key-findings">Key Findings</h2> <ul> <li>RAG 에서 Contrastive In-Context Learning 은 성능 향상에 유의미한 영향을 미침</li> <li>정확하면서도 간결한 문서를 이용하는것이 성능에 핵심 요소</li> <li>쿼리 확장, 문서 크기, retrieval stride 같은 요소들은 의미 있는 개선을 가져오지 않았음</li> <li>RAG 지식 베이스의 크기도 반드시 중요한것은 아니며, 오히려 <strong>문서의 품질과 관련성</strong>이 중요</li> </ul> <h2 id="experiments">Experiments</h2> <h3 id="1-llm-크기에-따른-응답-품질-조사">(1) LLM 크기에 따른 응답 품질 조사</h3> <p><img src="https://i.imgur.com/O5llP9M.png" alt="Image" width="80%"/></p> <ul> <li>Mistral 기반의 7B 와 45B 모델을 baseline 으로 하여 성능 (TruthfulQA, MMLU) 비교</li> <li>모델 크기가 증가할수록 성능이 증가하는것은 맞지만, 특정 task 에 대해서는 성능 향상이 눈에 띌 정도는 아니다.</li> </ul> <h3 id="2-프롬프트에-따른-응답-품질-조사">(2) 프롬프트에 따른 응답 품질 조사</h3> <p><img src="https://i.imgur.com/fwfGJGi.png" alt="Image" width="80%"/></p> <ul> <li>사용자 물음에 진실하게 답변하라는 프롬프트 <code class="language-plaintext highlighter-rouge">Help</code> 와 창의적이고 강아지처럼 답변하라는 적대적인 프롬프트 <code class="language-plaintext highlighter-rouge">Advers</code> 프롬프트를 비교</li> <li><code class="language-plaintext highlighter-rouge">Advers</code> 쪽이 일관되게 낮은 성능을 보임.</li> <li>근데 실제 프롬프트를 보면 대놓고 <code class="language-plaintext highlighter-rouge">Advers</code> 쪽을 망쳐놔서 너무 당연한걸 실험한거 아닌지 모르겠음.</li> </ul> <h3 id="3-청크-사이즈에-따른-응답-품질-조사">(3) 청크 사이즈에 따른 응답 품질 조사</h3> <p><img src="https://i.imgur.com/2seUB76.png" alt="Image" width="80%"/></p> <ul> <li>청크 사이즈를 S, M, L, XL 로 나누어 성능 비교 (순서대로 48, 64, 128, 192 토큰 사이즈)</li> <li>top-2 개의 document 만 검색하여 LLM 에 입력후 성능을 측정함</li> <li>청크 사이즈가 클수록 좋은 성능을 확인했지만, 유의미할 정도는 아녔음</li> <li>청크 사이즈를 늘리는 것이 시스템 성능에 크게 영향을 미치지 않는다는 것을 시사</li> </ul> <h3 id="4-지식-베이스-크기에-따른-응답-품질-조사">(4) 지식 베이스 크기에 따른 응답 품질 조사</h3> <p>지식 베이스 크기를 증가시키면 더 많은 정보를 제공할 수 있지만, 관련성을 희석시키고 검색 속도를 늦출 수 있다. 반면, 더 작은 지식 베이스는 더 빠른 검색과 더 높은 관련성을 제공하지만 포괄적인 범위를 갖지 못한다. 일종의 trade-off.</p> <p><img src="https://i.imgur.com/UCqxmmM.png" alt="Image" width="80%"/></p> <ul> <li>문서 개수를 1k 또는 10k로 조절하고, top-2 또는 top-5 개의 문서를 검색하여 성능을 측정함</li> <li>실험 결과, 지식 베이스의 크기를 확장하는 것이 성능에 통계적으로 유의미한 영향을 미치지 않았으며, 성능 차이는 거의 없었음 –&gt; 이는 더 큰 지식 베이스나 추가 문서 검색이 RAG 시스템의 출력 품질을 반드시 개선하지는 않는다는 것을 보여줌.</li> <li>이유를 추정하자면, 추가 문서가 특정 쿼리에 대해 관련이 없거나 중복될 수 있기 때문일 수 있음</li> </ul> <h3 id="5-문서-업데이트-빈도-in-retrieval-stride">(5) 문서 업데이트 빈도 (in retrieval stride)</h3> <p>WIP</p> <h3 id="6-쿼리-확장">(6) 쿼리 확장</h3> <p>WIP</p> <h3 id="7-contrastive-in-context-learning-여부에-따른-응답-품질-조사">(7) Contrastive In-Context Learning 여부에 따른 응답 품질 조사</h3> <p>Contrastive In-Context Learning(ICL) 은 아래처럼 query 에 대한 올바른 정답과 잘못된 정답 (few-shot) 을 제공하는 것이다.</p> <p><strong>ICL 예시</strong></p> <blockquote> <p>You are a truthful expert question answering bot and should correctly and concisely answer the following question. Considering these examples: Question: $q$, Correct Answer: $\text{Answer}<em>{\text{correct}}$. Question: $q$, Incorrect Answer: $\text{Answer}</em>{\text{incorrect}}$. Question: $q$, Correct Answer:</p> </blockquote> <p><img src="https://i.imgur.com/sStpLZ6.png" alt="Image" width="80%"/></p> <ul> <li>실험 결과에서 보다시피 다른 방식들에 비해 가장 좋은 성능을 보였다.</li> <li>Doc 과 Doc+ 차이 여부는 negative example 의 추가 여부다. 추가하니 성능이 더 좋아졌음.</li> </ul> <h3 id="8-다국어">(8) 다국어</h3> <p>WIP</p> <h3 id="9-문서-대신-일부-문장에-집중할때의-응답-품질-조사">(9) 문서 대신 일부 문장에 집중할때의 응답 품질 조사</h3> <p>논문에서는 Focus mode 라고 불리는데, document 대신 n 개의 문장을 찾아서 LLM 에 입력하는 방식이다. 아래는 Focus mode 방식에 따른 실험 결과인데, 각 이름은 얼마나 많은 문장을 가져올지 결정하는 것이다. 예를 들어, 20Doc20S 라면, 20개의 document 에서 가장 중요한 문장 한개씩을 가져와 총 20개의 문장을 구성하는 것이다. 일종의 <strong>reranking 방식</strong>이라고 생각할 수 있겠다.</p> <p><img src="https://i.imgur.com/lC34C0z.png" alt="Image" width="80%"/></p> <ul> <li>더 많은 문서에서 문장을 가져올수록 성능이 향상되었지만 80개 이상으로 가져오니 큰 성능 향상이 없었다.</li> <li>120Doc 이 80Doc 보다 성능이 유의미하게 좋아지지 않는 이유는 (4) 의 지식 베이스 크기나 (3) 의 청크 사이즈가 커져도 큰 성능 향상이 없는것과 비슷하다고 생각할 수 있겠다.</li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2501.07391.pdf">Enhancing Retrieval-Augmented Generation: A Study of Best Practices (pdf)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="Retrieval"/><category term="WIP"/><summary type="html"><![CDATA[아래 9가지 RAG 성능을 향상시키기 위한 연구 주제에 대해 실험 기반의 empirical 조사 결과를 정리]]></summary></entry><entry><title type="html">ML Recap - Beta Distribution</title><link href="https://zzong2006.github.io/blog/2025/beta-distribution/" rel="alternate" type="text/html" title="ML Recap - Beta Distribution"/><published>2025-01-19T23:00:00+00:00</published><updated>2025-01-19T23:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/beta-distribution</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/beta-distribution/"><![CDATA[<p>확률 내용이긴 하지만, ML 에서도 자주 사용되는 beta distribution 에 대해서 정리해보자.</p> <hr/> <p>베타 분포는 확률의 분포를 나타내는 것으로 이해할 수 있다. 즉, 우리가 어떤 확률이 무엇인지 모를 때 그 확률의 모든 가능한 값을 나타낸다.</p> <blockquote> <p>The Beta distribution is best for representing <strong>a probabilistic distribution of probabilities</strong>: the case where we don’t know what a probability is in advance, but we have some reasonable guesses.</p> </blockquote> <p>예를 들어 어떤 야구 선수의 타율에 대해서 베타 분포를 사용할 수 있다. 평균 타율이 0.26 정도이고, 0.21 과 0.35 사이에 타율이 분포(prior)하고 있는 야구선수에 대해서 Beta 분포로 표현하자면 $\alpha = 81, \beta = 219$ 로 아래와 같이 모델링할 수 있다.</p> <p><img src="https://i.imgur.com/9V5Lg8o.png" alt="Image" width="50%"/></p> <p>베타 분포 밀도 그래프에서 x축은 선수의 타율을 나타낸다. 따라서 이 그래프에서 y축이 확률(또는 더 정확히는 probability density)일 뿐만 아니라 x축도 확률이다 (타율은 결국 안타의 확률이다).</p> <p>참고로 베타 분포의 평균은 $\alpha / (\alpha + \beta)$ 이다.</p> <p>베타 분포가 유용한 이유는 새로운 피드백에 대한 반영이 간단하다는 점이다. 예를 들어, 위의 선수가 300 번 나가서 100번 안타를 쳤다고 가정해보자. 이러한 정보를 반영하기 위해서 아래와 같은 수식을 활용할 수 있다.</p> \[\text{Beta}(\alpha_0+\text{hits}, \beta_0+\text{misses})\] <p>여기서 $\alpha_0$ 와 $\beta_0$ 는 초기 베타 분포의 파라미터이다. 위 안타 사례에 의하면 alpha 쪽은 100, beta 쪽은 200 (300 - 100) 을 증가시키면 된다.</p> <p><img src="https://i.imgur.com/DJdG0wJ.png" alt="Image" width="50%"/></p> <p>선수의 타율에 대해 더 잘 알게 되었으므로, 이제 곡선이 더 얇아지고 오른쪽으로 이동했다(더 높은 타율).</p> <h2 id="compared-to-the-binomial-distribution">compared to the binomial distribution</h2> <p>binomial distribution, with $s$ success and $f$ failures out of a total of $(s+f)$ trials.</p> \[P(s, f \vert \theta) = \binom{s + f}{s} \theta^s (1 - \theta)^f \tag{2}\] <p>Thompson Sampling은 불확실성을 모델링하기 위해 과거 보상 데이터를 기반으로 확률 분포를 생성하고, 행동을 선택할 때 이 분포에서 샘플링하는 방법입니다. 보상이 이진(binary)인 간단한 경우에는 Beta 분포가 사용됩니다. Beta 분포는 두 개의 파라미터 α와 β를 가지며, 분포의 평균값은 α/α + β로 계산됩니다. 이는 성공 횟수 / (성공 횟수 + 실패 횟수)로 이해할 수 있습니다. 행동을 선택하기 위해 각 팔(arm)의 Beta 분포에서 샘플링을 수행하고, 가장 높은 샘플 값을 가진 팔을 선택합니다.</p> <h2 id="beta-분포의-활용">Beta 분포의 활용</h2> <p>Thompson Sampling의 예로 Doordash의 요리 추천을 위한 밴딧(bandit) 시스템을 들 수 있습니다. 사용자의 특정 요리에 대한 선호도는 Beta(α=해당 요리 주문 횟수, β=다른 요리 주문 횟수)로 모델링됩니다. 탐색 페이지에서 보여줄 요리 필터를 선택할 때, 각 요리의 값은 해당 요리의 Beta 분포에서 샘플링됩니다. 그런 다음 이 값들을 내림차순으로 정렬하여 상위 요리들을 선택해 표시합니다.</p> <p>Doordash는 상위 레벨의 지역 데이터를 활용하여 요리 밴딧을 초기화(warm-start)하는 방법을 공유했습니다. 각 요리에 대해 여러 레벨(예: 지역, 하위 지역, 사용자)에서 밴딧 정책을 학습합니다. 최상위 레벨 밴딧은 Beta(α=1, β=1)로 초기화됩니다. 그런 다음, 하위 레벨 밴딧마다 α는 해당 레벨에서 요리의 평균 주문 횟수를 더해 업데이트하고, β는 해당 레벨에서 다른 요리의 평균 주문 횟수를 더해 업데이트합니다. 마지막으로 사용자 레벨 밴딧에서는 α와 β가 사용자의 주문 데이터를 통해 업데이트됩니다. 결과적으로, 새로운 사용자의 요리 밴딧은 상위 레벨의 마켓플레이스 데이터를 기반으로 초기화되며, 이후 각 새로운 주문이 사용자의 개인 선호도를 반영하여 밴딧을 업데이트합니다.</p> <p>Reference: <a href="https://arxiv.org/abs/2009.06546">Carousel Personalization in Music Streaming Apps with Contextual Bandits</a></p> <hr/> <p>when feedback is delayed, Thompson Sampling outperforms UCB. Delayed feedback, where user-item interactions are not processed immediately, is common for most real-world systems due to resource and run-time constraints. In this situation, because UCB selects arms deterministically, it chooses the same action until new feedback is incorporated. In contrast, because Thompson Sampling chooses actions stochastically by sampling from the posterior distribution, it randomizes over actions even without updated rewards. Yahoo’s evaluation of Thompson Sampling and Deezer’s music bandit observed that this led to wider exploration and thus better outcomes.</p> <p>The initialization strategy is important.</p> <ul> <li>Naive initialization: the prior was Beta(1, 1).</li> <li>Pessimistic initialization: the prior was Beta(1, 99)</li> </ul> <p>That pessimistic initialization performed better due to the lower prior probabilities which were more reflective of real-world reward.</p> <h2 id="references">References</h2> <ul> <li><a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution (wikipedia)</a></li> <li><a href="https://applyingml.com/resources/bandits/">Bandits</a></li> <li><a href="https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution/47782#47782">What is the intuition behind beta distribution?</a></li> </ul>]]></content><author><name></name></author><category term="ml-fundamentals"/><category term="machine-learning"/><category term="probability"/><category term="WIP"/><summary type="html"><![CDATA[확률 내용이긴 하지만, ML 에서도 자주 사용되는 beta distribution 에 대해서 정리해보자.]]></summary></entry></feed>