<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-07T08:13:28+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">Believe I.Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">LLM 이 json 응답을 잘 하도록 하는법</title><link href="https://zzong2006.github.io/blog/2025/constrained-decoding/" rel="alternate" type="text/html" title="LLM 이 json 응답을 잘 하도록 하는법"/><published>2025-01-07T10:00:00+00:00</published><updated>2025-01-07T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/constrained-decoding</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/constrained-decoding/"><![CDATA[<h2 id="constrained-decoding">Constrained decoding</h2> <p><img src="https://blog.mlc.ai/img/xgrammar/constrained-decoding.png" alt="constrained decoding" width="80%"/></p> <p>Constrained decoding is a common technique to enforce the output format of an LLM. As shown in the figure above, an LLM engine maintains an internal state of the desired structure and the history of generated tokens. When generating a new token, the engine identifies tokens that may violate the required structure and masks them off in the logits. The masking causes the sampling process to avoid invalid tokens and only generate valid ones. In this example, only tokens <code class="language-plaintext highlighter-rouge">true</code> and <code class="language-plaintext highlighter-rouge">false</code> are allowed in the first decoding step, and only <code class="language-plaintext highlighter-rouge">,</code> and <code class="language-plaintext highlighter-rouge">,\n</code> are allowed in the second decoding step.</p> <h2 id="context-free-grammars-cfgs">Context-free grammars (CFGs)</h2> <p>Although JSON schema is a popular method for structure specification, it cannot define code syntax or recursive structures (such as nested brackets of any depth). Context-free grammars (CFGs) provide a more powerful and general representation that can describe many complex structures.</p> <h2 id="references">References</h2> <ul> <li><a href="https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar">Achieving Efficient, Flexible, and Portable Structured Generation with XGrammar</a></li> </ul>]]></content><author><name></name></author><category term="inference"/><category term="LLM"/><summary type="html"><![CDATA[Constrained decoding]]></summary></entry><entry><title type="html">Multi-Agent 관련 리소스 모음</title><link href="https://zzong2006.github.io/blog/2025/multi-agents/" rel="alternate" type="text/html" title="Multi-Agent 관련 리소스 모음"/><published>2025-01-07T10:00:00+00:00</published><updated>2025-01-07T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/multi-agents</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/multi-agents/"><![CDATA[<ul> <li><a href="https://github.com/huggingface/smolagents">huggingface/smolagents</a>: huggingface 에서 제공하는 agent 라이브러리</li> <li><a href="https://github.com/aymeric-roucher/GAIA">GAIA</a>: Multi-Agent 벤치마크 (2023, Meta)</li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar">Achieving Efficient, Flexible, and Portable Structured Generation with XGrammar</a></li> </ul>]]></content><author><name></name></author><category term="inference"/><category term="LLM"/><summary type="html"><![CDATA[huggingface/smolagents: huggingface 에서 제공하는 agent 라이브러리 GAIA: Multi-Agent 벤치마크 (2023, Meta)]]></summary></entry><entry><title type="html">Algorithm lesson learned - array</title><link href="https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array/" rel="alternate" type="text/html" title="Algorithm lesson learned - array"/><published>2025-01-06T10:00:00+00:00</published><updated>2025-01-06T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/algorithm-lesson-learned-array/"><![CDATA[<p>알고리즘 문제를 풀면서 인사이트를 얻은 내용들을 정리합니다.</p> <h2 id="구간-처리">구간 처리</h2> <p>어떤 구간을 처리하는 문제는 해당 구간을 모두 처리할려 하지말고, 구간의 앞과 끝 부분만 다룰 수 있는지 생각해보자.</p> <p>auxiliary array (difference array) 을 사용하여 범위 업데이트 작업을 효율적으로 적용할 수 있다. 범위 내의 모든 요소를 직접 업데이트하는 대신, 범위의 시작점과 끝점을 표시하고 나중에 prefix sum을 사용하여 업데이트를 적용한다.</p> <p>예를 들어, 주어진 배열에 대해 [0, 4] 구간에 +1을 적용하고, [2, 5] 구간에 -1을 적용한다고 가정해보자.</p> <ul> <li>auxiliary 배열의 [0] 인덱스에 +1을, [5] 인덱스에 -1을 적용하고, [2] 인덱스에 +1을, [6] 인덱스에 -1을 적용</li> <li>각 요소가 원래 배열에서 얼마나 증가하거나 감소했는지를 결정하기 위해 prefix sum을 사용하여 누적 합을 계산</li> </ul> <h3 id="관련-문제">관련 문제</h3> <ul> <li><a href="https://leetcode.com/problems/shifting-letters-ii/description/">leetcode: shifting-letters-ii</a></li> </ul>]]></content><author><name></name></author><category term="algorithm"/><category term="competitive-programming"/><category term="array"/><category term="string"/><category term="lesson-learned"/><summary type="html"><![CDATA[알고리즘 문제를 풀면서 인사이트를 얻은 내용들을 정리합니다.]]></summary></entry><entry><title type="html">llama 에 대해서 알아보자</title><link href="https://zzong2006.github.io/blog/2025/llama/" rel="alternate" type="text/html" title="llama 에 대해서 알아보자"/><published>2025-01-06T10:00:00+00:00</published><updated>2025-01-06T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/llama</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/llama/"><![CDATA[<p>LLaMA 1, 2 모델을 뜯어보면서 알게된 내용들을 정리합니다.</p> <blockquote> <p>We attribute their success, as all else, to divine benevolence.</p> </blockquote> <h2 id="rmsnorm-root-mean-square-layer-normalization">RMSNorm (Root Mean Square Layer Normalization)</h2> <p>Why need a layer normalization? Internal covariate shift</p> <ul> <li>각 레이어의 출력을 평균 0 (re-centering), 분산 1 (re-scaling)로 맞춘다.</li> <li>Layer Normalization은 행(row) 단위로 적용된다.</li> </ul> <p>RMS 는 LayerNorm 의 효과가 mean 보다는 variance 쪽에 기여치가 더 높을 것이라 가정한다. 그래서 RMSNorm 은 평균 계산을 포기하고 variance 만 계산하여 정규화해서 computation-efficiency 에 이득을 취한다.</p> <h2 id="rotary-positional-embedding">Rotary positional embedding</h2> <p>Rotary positional embedding 은 relative positional embedding 과 비슷하지만, distance 정보를 상수값으로 치환하여 embedding vector 에 적용하면서 계산 효율을 높인다.</p> <p>그 상수는 complex number 로, Euler’s formula 를 이용해서 attention 값을 계산한다.</p> <p>Other positional embedding methods</p> <ul> <li>Absolute: Vanilla transformers 에서 적용된 방법으로, Attention 계산 시 이미 고정된 constant position 정보가 적용되어 있는 각 embedding vector 를 계산에 활용한다.</li> <li>Relative: Attention 계산 시, 각 embedding vector pair 마다 상대적인 distance 정보를 변수로 활용하여 계산한다.</li> </ul> <p>Absolute, relative 와 다르게 rotary positional embedding 은 <strong>q, k weight 가 먼저 적용된 이후</strong> 에 적용된다는 점이다.</p> <h2 id="grouped-multi-query-attention">Grouped Multi-Query Attention</h2> <p><strong>Multi-Query Attention</strong></p> <p>일반적인 multi-head attention 은 각 head 마다 서로 다른 key, value 를 사용하는데, 이를 하나로 통일하여 모든 query 에 동일한 key, value 를 사용한다.</p> <p>Why?</p> <ul> <li>Problem: GPU 의 memory bandwidth 는 GPU 의 계산 속도 (FLOPS) 보다 훨씬 느리다.</li> <li>KV cache 를 사용하면서, 하나의 토큰 query 에 대해서만 계산하므로, i/o bottleneck 이 발생한다.</li> </ul> <p><strong>Grouped Multi-Query Attention</strong></p> <p>Grouped Multi-Query 는 일정 개수의 그룹마다 동일한 key, value 를 사용하는 방법이다. 즉, group 사이즈가 1 일 때는 일반적인 multi-head attention 과 동일하다.</p> <h2 id="swiglu-activation-function">SwiGLU activation function</h2> <p>SwiGLU는 Swish + GLU, 두개의 Activation Functions를 섞어 만든 함수</p> <p><strong>Swish Function</strong></p> <ul> <li>$\sigma(x) = x \cdot \sigma(x)$ 로 표현된다.</li> <li>Original Transformer 에서의 ReLU 와 비슷하지만, 음수쪽에서 0 에 가까워질때 기울기가 0 이 되는 문제(Dying ReLU)를 해결한다.</li> </ul> <p><strong>GLU (Gated Linear Unit)</strong></p> \[GLU(a, b) = a \otimes \sigma(b)\] <p>The GLU also has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.</p> <p><strong>SwiGLU</strong></p> <p>GLU 에서 sigmoid 대신 Swish Function 을 사용한다.</p> \[SwiGLU(a, b) = a \otimes \text{Swish}_\beta(b)\] <p>구체적으로는 총 3개의 weight matrix 를 사용하여 LLaMA 의 FFN 을 구성한다.</p> \[\text{FFN}_\text{SwiGLU}(x, W, V, W_2) = (\text{Swish}_1(xW) \otimes xV)W_2\] <p><strong>vs. ReLU</strong></p> <p>ReLU 보다는 SwiGLU 가 안정적으로 학습되는 느낌이지만, 그렇다고 엄청 좋은 성능을 보이는 것은 아니다. 실험 결과에서는 ReLU 는 83.80 점이고, SwiGLU 는 84.36 점 정도로, 거의 차이가 없는 느낌.</p> <p>하지만 전반적인 벤치마크 성능에서 SwiGLU 쪽이 우위인 상황.</p> <h2 id="references">References</h2> <ul> <li><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&amp;ab_channel=UmarJamil">LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</a></li> <li><a href="https://jihan819.tistory.com/entry/AI-SwiGLU%EB%8A%94-%EC%96%B4%EB%96%A4-%ED%95%A8%EC%88%98%EC%9D%BC%EA%B9%8C">SwiGLU는 어떤 함수일까?</a></li> </ul>]]></content><author><name></name></author><category term="algorithm"/><category term="LLaMA"/><category term="Meta"/><summary type="html"><![CDATA[LLaMA 1, 2 모델을 뜯어보면서 알게된 내용들을 정리합니다.]]></summary></entry><entry><title type="html">Semantic Retrieval at Walmart</title><link href="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/" rel="alternate" type="text/html" title="Semantic Retrieval at Walmart"/><published>2025-01-05T17:00:00+00:00</published><updated>2025-01-05T17:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/"><![CDATA[<p>Walmart 에서 적용한 Embedding-based neural retrieval (EBR) 에 대한 논문 리뷰.</p> <p>2022년, 2024년에 각각 하나씩 시리즈물 느낌으로 발표되었다.</p> <h2 id="1-summary-of-the-proposed-method">(1) Summary of the proposed method</h2> <ol> <li>A hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries.</li> <li>A novel method of selecting negative examples for training a large neural retrieval model and an approximate metric to evaluate the performance</li> </ol> <h2 id="2-related-works">(2) Related Works</h2> <p><strong>Production search vs. Web search</strong></p> <p>Production search is way more challenging than web search.</p> <ul> <li>Product titles (the main search-able text) are generally much shorter than web documents.</li> <li>While many web documents may contain the same information, a specific product from a seller rarely has a duplicate.</li> </ul> <p><strong>Traditional Solutions</strong></p> <ul> <li>knowledge graph: need a huge amount of domain expertise, and the cost of maintaining these components is high, since the catalog and product vocabulary frequently change in e-commerce</li> <li>BM25, an inverted index: suffers from vocabulary mismatch between the query and the product title</li> <li>neural systems: limited by the fact that the embedding size cannot be too large due to latency concerns</li> </ul> <h2 id="3-proposed-methods">(3) Proposed Methods</h2> <p><strong>Reducing the size of embedding</strong></p> <p>Reducing the size of embedding is beneficial as it allows the item embedding and the ANN index to be refreshed more frequently.</p> <p>Tried 2 approaches:</p> <ol> <li>Add a linear projection layer to reduce the embedding size to 368, 256, 128, and 64</li> <li>Use a transformer architecture that has a smaller embedding size: MiniLM (12 layers and an embedding size of 368), XtremeDistil (6 layers and an embedding size of 368)</li> </ol> <p>The <strong>linear projection</strong> is very effective in reducing the size of the embedding with very little performance cost.</p> <p><strong>A hybrid architecture</strong></p> <p>…</p> <p><strong>ANN</strong></p> <ul> <li>Normalized vectors of dimension 256, the ANN services can yield 99% for recall@20 evaluated against the full nearest neighborhood search, with an average latency around 13 ms;</li> <li>For normalized vectors of dimension 768, the services can achieve a similar recall@20 but with three times the storage space;</li> </ul> <h2 id="6-lesson-learned">(6) Lesson Learned</h2> <p><strong>Cosine similarity vs. Inner product</strong></p> <ul> <li>The inner product is more stable during training and does not require the temperature factor 𝜎.</li> <li>But, <strong>inner product was much harder to optimize</strong> when creating the ANN index, compared to cosine similarity.</li> </ul> <p><strong>Text fields</strong></p> <p>Many text fields are generally available for each product, and the quality of the text fields varies. But, we could not extract any boost in performance by using these text fields. This is probably because descriptions can contain a lot of irrelevant text that simply adds noise.</p> <p><strong>Model Complexity</strong></p> <p>A very deep model or very large embedding size is not necessary to achieve top performance. This is probably because queries and product titles are not very complex from a semantic perspective.</p> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2412.04637">Semantic Retrieval at Walmart</a></li> <li>[2] <a href="https://arxiv.org/abs/2408.04884">Enhancing Relevance of Embedding-based Retrieval at Walmart</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Walmart"/><category term="ANN"/><category term="RAG"/><category term="WIP"/><summary type="html"><![CDATA[Walmart 에서 적용한 Embedding-based neural retrieval (EBR) 에 대한 논문 리뷰.]]></summary></entry><entry><title type="html">KV-Cache 에 대해 알아보자</title><link href="https://zzong2006.github.io/blog/2025/kv-cache/" rel="alternate" type="text/html" title="KV-Cache 에 대해 알아보자"/><published>2025-01-05T10:00:00+00:00</published><updated>2025-01-05T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/kv-cache</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/kv-cache/"><![CDATA[<p>The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).</p> <p>Therefore, when iteratively calling forward() instead of the generate() method, it’s crucial to ensure that the attention mask shape matches the combined length of past and current key-values. The attention mask should have the shape (batch_size, past_kv_length + new_tokens_length). This is usually handled internally when you call generate() method. If you want to implement your own generation loop with Cache classes, take this into consideration and prepare the attention mask to hold values to current and past tokens.</p> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/docs/transformers/kv_cache">huggingface transformers - kv_cache</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Transformers"/><category term="LLM"/><category term="WIP"/><summary type="html"><![CDATA[The Attention module concatenates the current key-values with the past key-values stored in the cache. This results in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length). Essentially, the past and current key-values are combined to compute attention scores, ensuring that the model considers both previous context and new input. The concatenated key-values are used to compute the attention scores resulting in attention weights of shape (new_tokens_length, past_kv_length + new_tokens_length).]]></summary></entry><entry><title type="html">차원 축소 전략 중 하나인 Linear Discriminant Analysis</title><link href="https://zzong2006.github.io/blog/2025/lda/" rel="alternate" type="text/html" title="차원 축소 전략 중 하나인 Linear Discriminant Analysis"/><published>2025-01-05T10:00:00+00:00</published><updated>2025-01-05T10:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/lda</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/lda/"><![CDATA[<p>LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) are both dimensionality reduction techniques, but they serve different purposes.</p> <h2 id="lda-vs-pca">LDA vs PCA</h2> <h3 id="1-pca">(1) PCA</h3> <p><img src="https://i.imgur.com/PJgQ6w1.png" alt="PCA" style="width: 50%;"/></p> <ul> <li>PCA 는 데이터의 분산을 최대화하는 방향 (principal components) 을 찾는다. <ul> <li>쉽게 생각하면, 데이터들을 특정 축(axis, principal component)에 사영(projection)했을때, 가장 높은 분산을 가지는 축을 찾아 그곳으로 차원을 축소한다.</li> </ul> </li> <li>간단한 방식이지만, 데이터의 클래스 정보를 고려하지 않는다는 문제가 있다.</li> <li>주로 데이터 분석 및 노이즈 제거를 위해 사용된다.</li> <li>단점: 종종 feature 의 variance 가 낮으면서 중요한 경우가 있는데, PCA 는 이를 고려하는데 어려움을 겪는다.</li> </ul> <h3 id="2-lda">(2) LDA</h3> <ul> <li>LDA 는 데이터의 class label 을 고려하는 supervised 방식이다.</li> <li>데이터의 여러 클래스 간 분리 정도(separation)를 최대화하는 방향을 찾는다.</li> <li>LDA 는 주로 클래스 정보를 보존하기 위해서 분류 task 를 해결할 때 사용된다.</li> <li>단점: 클래스 분포가 불균형적인 경우, 특정 클래스에 편향(bias)된 결과를 내기 쉽다.</li> </ul> <h2 id="implementation">Implementation</h2> <p><code class="language-plaintext highlighter-rouge">sklearn</code> 에 이미 구현된 함수가 있다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">clf</span> <span class="o">=</span> <span class="nc">LinearDiscriminantAnalysis</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span> <span class="c1"># 1
</span></code></pre></div></div> <h2 id="references">References</h2> <ul> <li><a href="https://www.linkedin.com/advice/0/what-key-differences-between-pca-lda-dimensionality-7memc">LinkedIn - What are the key differences between PCA and LDA?</a></li> <li><a href="https://velog.io/@chiroya/23-PCA-LDA">velog - PCA vs LDA</a></li> </ul>]]></content><author><name></name></author><category term="dimension-reduction"/><category term="encoder"/><category term="WIP"/><category term="PCA"/><category term="LDA"/><category term="Preprocessing"/><summary type="html"><![CDATA[LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) are both dimensionality reduction techniques, but they serve different purposes.]]></summary></entry><entry><title type="html">The collection of RAG competitions</title><link href="https://zzong2006.github.io/blog/2025/rag-competition/" rel="alternate" type="text/html" title="The collection of RAG competitions"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/rag-competition</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/rag-competition/"><![CDATA[<p>RAG 관련 대회 목록을 정리해보려 한다.</p> <hr/> <h3 id="1-financerag-challenge-at-icaif-24-종료">1. FinanceRAG Challenge at ICAIF ‘24 (종료)</h3> <p>5th ACM International Conference on AI in Finance (ICAIF’24)에서 RAG 관련 경진대회를 개최했습니다.</p> <p>최근 금융권에서도 LLM 사용에 대해 매우 많은 관심들을 갖고 계시고, 보다 정교한 사용을 위해 RAG이 많은 주목을 받고 있습니다.</p> <p>이번 경진대회는 참가자들이 직접 RAG 시스템을 구현해볼 수 있도록 준비 하였습니다. Linq에서 직접 미국의 증권시장 공시, 리포트 등을 바탕으로 query-corpus dataset을 세심하게 준비해주셨습니다. 특히 금융분야에는 이러한 데이터셋이 없는데요, 참가자들이 매우 좋은 데이터셋을 바탕으로 RAG 시스템을 구현해보고 evaluation도 해볼 수 있는 매우 좋은 기회라고 생각합니다.</p> <p>https://www.kaggle.com/competitions/icaif-24-finance-rag-challenge/overview</p> <h3 id="2-dacon-재정정보-ai-검색-알고리즘-경진대회-종료">2. Dacon: <a href="https://dacon.io/competitions/official/236295/overview/description">재정정보 AI 검색 알고리즘 경진대회</a> (종료)</h3> <p>대회 기간: 2024년 7월 29일 ~ 2024년 8월 23일</p> <p>특이한점은 RAG 로 활용할 데이터를 PDF 형태로 제공한다는 점이다.</p> <p>물론 LLM 학습에 필요한 Question-Answer pair 역시 제공된다.</p> <p>대부분 e5 모델로 vector search 를 위한 임베딩을 진행하였다.</p> <p>한 우승팀에서는 YOLOv10 기반 Document Layout Analysis를 통해 PDF parser를 구현하였다고 한다.</p> <h3 id="3-aicrowd-crag-comprehensive-rag-benchmark-종료">3. AiCrowd: <a href="https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024">CRAG: Comprehensive RAG Benchmark</a> (종료)</h3> <p>대회 기간: 2024년 5월 20일 ~ 2025년 1월 25일</p>]]></content><author><name></name></author><category term="RAG"/><category term="competition"/><summary type="html"><![CDATA[RAG 관련 대회 목록을 정리해보려 한다.]]></summary></entry><entry><title type="html">Don’t do RAG</title><link href="https://zzong2006.github.io/blog/2025/no-rag/" rel="alternate" type="text/html" title="Don’t do RAG"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/no-rag</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/no-rag/"><![CDATA[<h2 id="quick-summary">Quick Summary</h2> <p>RAG는 검색 지연, 문서 선택 오류, 시스템 복잡성 증가 같은 문제를 일으킨다. 최근 LLM의 컨텍스트 길이가 크게 늘어나면서, 실시간 검색 전략인 캐시 증강 생성(CAG)이 제안된다.</p> <p>CAG는 모델의 컨텍스트 윈도우에 모든 관련 리소스를 미리 로드해서 KV-cache 형태로 적재하고, 인퍼런스 과정에서 모델이 이 캐시를 사용해서 추가적인 검색 과정 없이 쿼리에 답변할 수 있다.</p> <p>CAG는 검색 지연을 줄이고 검색 오류를 최소화하면서도 컨텍스트 관련성을 유지할 수 있다고 한다. 여러 벤치마크 데이터셋에서 CAG가 기존 RAG보다 더 나은 성능을 보인다고 주장한다.</p> <p>제한된 지식 기반을 가진 경우, CAG가 RAG보다 더 괜찮은 선택지로 고려해볼 수 있다고 한다. 왜냐하면, CAG 는 필요한 모든 정보를 모델의 context 안에 캐시 형태로 밀어 넣기 때문에 대용량 데이터를 다루는 상황이라면 적절하지 않기 때문이다. 또한 LLM 이 long context prompt 를 처리하는데 어려움을 겪는 경우가 많기 때문에, 이러한 단점을 고려해서 CAG 를 사용하는 것이 좋다.</p> <h2 id="implementation">Implementation</h2> <p>저자의 코드를 확인해보니 아래와 같이 kv cache 를 생성한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers.cache_utils</span> <span class="kn">import</span> <span class="n">DynamicCache</span>

<span class="c1"># Initialize dynamic cache
</span><span class="n">past_key_values</span> <span class="o">=</span> <span class="nc">DynamicCache</span><span class="p">()</span>

<span class="c1"># Generate KV cache with proper device placement
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

<span class="c1"># The model's device mapping will automatically place each layer's
# KV cache on the correct device
</span><span class="k">return</span> <span class="n">outputs</span><span class="p">.</span><span class="n">past_key_values</span>
</code></pre></div></div> <p>생성한 kv cache 를 사용하는 것은 간단한데, vllm 같은 framework 를 사용하지 않고 naive 한 iteration 을 통해서 진행한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Main generation loop
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="c1"># Forward pass with proper device placement
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">next_token</span><span class="p">,</span>  <span class="c1"># Only process last token
</span>        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>

    <span class="c1"># Get next token prediction (logits will be on the last device)
</span>    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token_logits</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="bp">...</span>

    <span class="c1"># Update KV cache
</span>    <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">past_key_values</span>

    <span class="c1"># Append prediction
</span>    <span class="n">output_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">output_ids</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="experiment">Experiment</h2> <h3 id="baselines">Baselines</h3> <ul> <li>Sparse Retrieval System (BM25): 일반적인 BM25. It ranks documents based on term frequency inverse document frequency (TF-IDF) and document length normalization.</li> <li>Dense Retrieval System (OpenAI Indexes): LlamaIndex framework 에 openai Embedding 모델을 사용하여 구축한 vector search system.</li> </ul> <h3 id="other-settings">Other Settings</h3> <ul> <li>Metrics: BERT-Score</li> <li>Datasets: SQuAD, HotpotQA</li> </ul> <h3 id="results">Results</h3> <ul> <li>성능: CAG 와 기존 Sparse, Dense RAG 비슷한 성능을 보이는데, CAG 가 아주 약소하게 좋음</li> <li>속도: CAG 가 압도적으로 빠른편이다. 데이터 사이즈를 small, medium, large 로 나눠서 모델 generation 속도를 비교하는데, 사이즈가 클수록 10배이상 빨라진다 (e.g. 2초 vs. 100초).</li> </ul> <h2 id="느낀점">느낀점</h2> <p>평가 진행에 있어서 좀 아쉬운 점들이 있다.</p> <ol> <li>BERT-Score 외에 Rouge, GPT-eval 같은 다양한 평가 지표로 진행하면 좋았을 것 같음.</li> <li>CAG 는 KV-cache 를 생성하는 속도까지 고려해야 더 공평한 결과일 것이다.</li> </ol> <p>이 접근에 착안해서 생각해봤는데 KV-cache 를 미리 생성하고, 해당 cache 를 검색하는 RAG 를 만드는것도 하나의 접근법일수도 있을것 같다.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/abs/2412.15605">paper (arxiv)</a></li> <li><a href="https://github.com/hhhuang/cag">code (github)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="LLM"/><summary type="html"><![CDATA[Quick Summary]]></summary></entry><entry><title type="html">행렬 미분 기초 (with Trace)</title><link href="https://zzong2006.github.io/blog/2025/matrix-derivative/" rel="alternate" type="text/html" title="행렬 미분 기초 (with Trace)"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/matrix-derivative</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/matrix-derivative/"><![CDATA[<p>행렬의 대각 성분의 합인 Trace ($ \text{Tr} $) 가 포함된 행렬 미분 규칙에 대해 정리해보려 한다.</p> <hr/> <h2 id="규칙-1--fracpartialpartial-w_d-texttra-w_d--atop-">규칙 1: $ \frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top $</h2> <p>여기서 $ A $는 $ m \times n $ 행렬이고 $ W_d $는 $ n \times m $ 행렬이라고 가정한다.</p> <p>행렬 미분의 결과는 미분한 행렬의 크기를 따라야 하므로, 이를 맞추기 위해서 transpose 를 취한다고 생각하면 편하다.</p> <p>(참고) Trace 는 정사각행렬에 대해서만 정의된다.</p> <h3 id="예시">예시</h3> <p>간단한 예시를 들어보자.</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix}, \quad W_d = \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix}\] <p>$ \text{Tr}(A W_d) $는 다음과 같이 계산된다.</p> \[\text{Tr}(A W_d) = \text{Tr}\left( \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix} \right) \\[10pt] = \text{Tr}\left( \begin{bmatrix} 1x + 2z &amp; 1y + 2w \\ 3x + 4z &amp; 3y + 4w \\ \end{bmatrix} \right) \\[10pt] = (1x + 2z) + (3y + 4w)\] <p>이제 $ W_d $ 의 각 원소에 대해 편미분을 계산해보자.</p> <ul> <li>$ \frac{\partial}{\partial x} \text{Tr}(A W_d) = 1 $</li> <li>$ \frac{\partial}{\partial y} \text{Tr}(A W_d) = 3 $</li> <li>$ \frac{\partial}{\partial z} \text{Tr}(A W_d) = 2 $</li> <li>$ \frac{\partial}{\partial w} \text{Tr}(A W_d) = 4 $</li> </ul> <p>Trace 연산은 스칼라 값으로 계산되지만, 이를 행렬 $ W_d $ 의 원소 각각에 대해 편미분하면, 원소별 변화율을 포함한 결과가 행렬 형태로 나타난다. 그 결과, 아래와 같이 transpose 된 A가 나온다.</p> \[\frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top = \begin{bmatrix} 1 &amp; 3 \\ 2 &amp; 4 \\ \end{bmatrix}\]]]></content><author><name></name></author><category term="math"/><category term="matrix"/><category term="calculus"/><summary type="html"><![CDATA[행렬의 대각 성분의 합인 Trace ($ \text{Tr} $) 가 포함된 행렬 미분 규칙에 대해 정리해보려 한다.]]></summary></entry></feed>