<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-05T08:36:44+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">Believe I.Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The collection of RAG competitions</title><link href="https://zzong2006.github.io/blog/2025/rag-competition/" rel="alternate" type="text/html" title="The collection of RAG competitions"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/rag-competition</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/rag-competition/"><![CDATA[<p>RAG 관련 대회 목록을 정리해보려 한다.</p> <hr/> <h3 id="1-financerag-challenge-at-icaif-24-종료">1. FinanceRAG Challenge at ICAIF ‘24 (종료)</h3> <p>5th ACM International Conference on AI in Finance (ICAIF’24)에서 RAG 관련 경진대회를 개최했습니다.</p> <p>최근 금융권에서도 LLM 사용에 대해 매우 많은 관심들을 갖고 계시고, 보다 정교한 사용을 위해 RAG이 많은 주목을 받고 있습니다.</p> <p>이번 경진대회는 참가자들이 직접 RAG 시스템을 구현해볼 수 있도록 준비 하였습니다. Linq에서 직접 미국의 증권시장 공시, 리포트 등을 바탕으로 query-corpus dataset을 세심하게 준비해주셨습니다. 특히 금융분야에는 이러한 데이터셋이 없는데요, 참가자들이 매우 좋은 데이터셋을 바탕으로 RAG 시스템을 구현해보고 evaluation도 해볼 수 있는 매우 좋은 기회라고 생각합니다.</p> <p>https://www.kaggle.com/competitions/icaif-24-finance-rag-challenge/overview</p> <h3 id="2-dacon-재정정보-ai-검색-알고리즘-경진대회-종료">2. Dacon: <a href="https://dacon.io/competitions/official/236295/overview/description">재정정보 AI 검색 알고리즘 경진대회</a> (종료)</h3> <p>대회 기간: 2024년 7월 29일 ~ 2024년 8월 23일</p> <p>특이한점은 RAG 로 활용할 데이터를 PDF 형태로 제공한다는 점이다.</p> <p>물론 LLM 학습에 필요한 Question-Answer pair 역시 제공된다.</p> <p>대부분 e5 모델로 vector search 를 위한 임베딩을 진행하였다.</p> <p>한 우승팀에서는 YOLOv10 기반 Document Layout Analysis를 통해 PDF parser를 구현하였다고 한다.</p> <h3 id="3-aicrowd-crag-comprehensive-rag-benchmark-종료">3. AiCrowd: <a href="https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024">CRAG: Comprehensive RAG Benchmark</a> (종료)</h3> <p>대회 기간: 2024년 5월 20일 ~ 2025년 1월 25일</p>]]></content><author><name></name></author><category term="RAG"/><category term="competition"/><summary type="html"><![CDATA[RAG 관련 대회 목록을 정리해보려 한다.]]></summary></entry><entry><title type="html">Don’t do RAG</title><link href="https://zzong2006.github.io/blog/2025/no-rag/" rel="alternate" type="text/html" title="Don’t do RAG"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/no-rag</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/no-rag/"><![CDATA[<h2 id="quick-summary">Quick Summary</h2> <p>RAG는 검색 지연, 문서 선택 오류, 시스템 복잡성 증가 같은 문제를 일으킨다. 최근 LLM의 컨텍스트 길이가 크게 늘어나면서, 실시간 검색 전략인 캐시 증강 생성(CAG)이 제안된다.</p> <p>CAG는 모델의 컨텍스트 윈도우에 모든 관련 리소스를 미리 로드해서 KV-cache 형태로 적재하고, 인퍼런스 과정에서 모델이 이 캐시를 사용해서 추가적인 검색 과정 없이 쿼리에 답변할 수 있다.</p> <p>CAG는 검색 지연을 줄이고 검색 오류를 최소화하면서도 컨텍스트 관련성을 유지할 수 있다고 한다. 여러 벤치마크 데이터셋에서 CAG가 기존 RAG보다 더 나은 성능을 보인다고 주장한다.</p> <p>제한된 지식 기반을 가진 경우, CAG가 RAG보다 더 괜찮은 선택지로 고려해볼 수 있다고 한다. 왜냐하면, CAG 는 필요한 모든 정보를 모델의 context 안에 캐시 형태로 밀어 넣기 때문에 대용량 데이터를 다루는 상황이라면 적절하지 않기 때문이다. 또한 LLM 이 long context prompt 를 처리하는데 어려움을 겪는 경우가 많기 때문에, 이러한 단점을 고려해서 CAG 를 사용하는 것이 좋다.</p> <h2 id="implementation">Implementation</h2> <p>저자의 코드를 확인해보니 아래와 같이 kv cache 를 생성한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers.cache_utils</span> <span class="kn">import</span> <span class="n">DynamicCache</span>

<span class="c1"># Initialize dynamic cache
</span><span class="n">past_key_values</span> <span class="o">=</span> <span class="nc">DynamicCache</span><span class="p">()</span>

<span class="c1"># Generate KV cache with proper device placement
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

<span class="c1"># The model's device mapping will automatically place each layer's
# KV cache on the correct device
</span><span class="k">return</span> <span class="n">outputs</span><span class="p">.</span><span class="n">past_key_values</span>
</code></pre></div></div> <p>생성한 kv cache 를 사용하는 것은 간단한데, vllm 같은 framework 를 사용하지 않고 naive 한 iteration 을 통해서 진행한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Main generation loop
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="c1"># Forward pass with proper device placement
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">next_token</span><span class="p">,</span>  <span class="c1"># Only process last token
</span>        <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>

    <span class="c1"># Get next token prediction (logits will be on the last device)
</span>    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token_logits</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="bp">...</span>

    <span class="c1"># Update KV cache
</span>    <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">past_key_values</span>

    <span class="c1"># Append prediction
</span>    <span class="n">output_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">output_ids</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h2 id="experiment">Experiment</h2> <h3 id="baselines">Baselines</h3> <ul> <li>Sparse Retrieval System (BM25): 일반적인 BM25. It ranks documents based on term frequency inverse document frequency (TF-IDF) and document length normalization.</li> <li>Dense Retrieval System (OpenAI Indexes): LlamaIndex framework 에 openai Embedding 모델을 사용하여 구축한 vector search system.</li> </ul> <h3 id="other-settings">Other Settings</h3> <ul> <li>Metrics: BERT-Score</li> <li>Datasets: SQuAD, HotpotQA</li> </ul> <h3 id="results">Results</h3> <ul> <li>성능: CAG 와 기존 Sparse, Dense RAG 비슷한 성능을 보이는데, CAG 가 아주 약소하게 좋음</li> <li>속도: CAG 가 압도적으로 빠른편이다. 데이터 사이즈를 small, medium, large 로 나눠서 모델 generation 속도를 비교하는데, 사이즈가 클수록 10배이상 빨라진다 (e.g. 2초 vs. 100초).</li> </ul> <h2 id="느낀점">느낀점</h2> <p>평가 진행에 있어서 좀 아쉬운 점들이 있다.</p> <ol> <li>BERT-Score 외에 Rouge, GPT-eval 같은 다양한 평가 지표로 진행하면 좋았을 것 같음.</li> <li>CAG 는 KV-cache 를 생성하는 속도까지 고려해야 더 공평한 결과일 것이다.</li> </ol> <p>이 접근에 착안해서 생각해봤는데 KV-cache 를 미리 생성하고, 해당 cache 를 검색하는 RAG 를 만드는것도 하나의 접근법일수도 있을것 같다.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/abs/2412.15605">paper (arxiv)</a></li> <li><a href="https://github.com/hhhuang/cag">code (github)</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="RAG"/><category term="LLM"/><summary type="html"><![CDATA[Quick Summary]]></summary></entry><entry><title type="html">행렬 미분 기초 (with Trace)</title><link href="https://zzong2006.github.io/blog/2025/matrix-derivative/" rel="alternate" type="text/html" title="행렬 미분 기초 (with Trace)"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/matrix-derivative</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/matrix-derivative/"><![CDATA[<p>행렬의 대각 성분의 합인 Trace ($ \text{Tr} $) 가 포함된 행렬 미분 규칙에 대해 정리해보려 한다.</p> <hr/> <h2 id="규칙-1--fracpartialpartial-w_d-texttra-w_d--atop-">규칙 1: $ \frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top $</h2> <p>여기서 $ A $는 $ m \times n $ 행렬이고 $ W_d $는 $ n \times m $ 행렬이라고 가정한다.</p> <p>행렬 미분의 결과는 미분한 행렬의 크기를 따라야 하므로, 이를 맞추기 위해서 transpose 를 취한다고 생각하면 편하다.</p> <p>(참고) Trace 는 정사각행렬에 대해서만 정의된다.</p> <h3 id="예시">예시</h3> <p>간단한 예시를 들어보자.</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix}, \quad W_d = \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix}\] <p>$ \text{Tr}(A W_d) $는 다음과 같이 계산된다.</p> \[\text{Tr}(A W_d) = \text{Tr}\left( \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix} \right) \\[10pt] = \text{Tr}\left( \begin{bmatrix} 1x + 2z &amp; 1y + 2w \\ 3x + 4z &amp; 3y + 4w \\ \end{bmatrix} \right) \\[10pt] = (1x + 2z) + (3y + 4w)\] <p>이제 $ W_d $ 의 각 원소에 대해 편미분을 계산해보자.</p> <ul> <li>$ \frac{\partial}{\partial x} \text{Tr}(A W_d) = 1 $</li> <li>$ \frac{\partial}{\partial y} \text{Tr}(A W_d) = 3 $</li> <li>$ \frac{\partial}{\partial z} \text{Tr}(A W_d) = 2 $</li> <li>$ \frac{\partial}{\partial w} \text{Tr}(A W_d) = 4 $</li> </ul> <p>Trace 연산은 스칼라 값으로 계산되지만, 이를 행렬 $ W_d $ 의 원소 각각에 대해 편미분하면, 원소별 변화율을 포함한 결과가 행렬 형태로 나타난다. 그 결과, 아래와 같이 transpose 된 A가 나온다.</p> \[\frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top = \begin{bmatrix} 1 &amp; 3 \\ 2 &amp; 4 \\ \end{bmatrix}\]]]></content><author><name></name></author><category term="math"/><category term="matrix"/><category term="calculus"/><summary type="html"><![CDATA[행렬의 대각 성분의 합인 Trace ($ \text{Tr} $) 가 포함된 행렬 미분 규칙에 대해 정리해보려 한다.]]></summary></entry><entry><title type="html">Text Embedding 모델: E5</title><link href="https://zzong2006.github.io/blog/2025/e5/" rel="alternate" type="text/html" title="Text Embedding 모델: E5"/><published>2025-01-02T00:00:00+00:00</published><updated>2025-01-02T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/e5</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/e5/"><![CDATA[<h1 id="original-e5">Original E5</h1> <h2 id="introduction">Introduction</h2> <p>기존의 BERT 와 GPT 같은 모델을 이용해 (추가 학습 없이) text representation 을 추출할 수 있으나, 이러한 모델들은 contrastive learning 으로 학습을 진행하지 않다보니 embedding 품질이 상대적으로 아쉬울 수 있다.</p> <p>a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings</p> <p>two prefix identifiers “query:” and “passage:” to $q$ and $d$</p> <h2 id="datasets-ccpairs">Datasets: CCPairs</h2> <p>1.3B unfiltered</p> <p>pair (query, passage) 예시</p> <ul> <li>reddit: (post, comment)</li> <li>stackoverflow: (question, upvoted answer)</li> <li>wikipedia: (title, abstract)</li> </ul> <h3 id="a-consistency-based-data-filtering-technique">A consistency-based data filtering technique</h3> <p>신경망 모델이 clean label 을 먼저 학습하고 그 이후에 noisy label 을 학습하는 경향이 있으므로, 이러한 경향을 활용하여 일관성 기반 데이터셋 필터링 기법을 적용</p> <p>아래의 방법을 통해 1.3B text pairs 를 270M 개로 줄임</p> <ol> <li>우선 1.3B 데이터셋을 이용해서 모델을 학습</li> <li>학습된 모델을 이용해 query 와 (정답이 포함된) 1M random passages 에 대한 연관성을 랭킹</li> <li>Top-k(=2) 개의 passage 안에 실제 정답이 포함되어 있는지 확인</li> <li>만약 정답이 포함되면 pair 를 keep, 그렇지 않다면 discard</li> </ol> <h2 id="training">Training</h2> <h3 id="contrastive-loss-for-pre-training">Contrastive loss (for pre-training)</h3> <p>아래와 같은 InfoNCE contrastive loss 를 사용하여 학습</p> \[\min L_{\mathrm{cont}}=-\frac{1}{n} \sum_i \log \frac{\mathrm{e}^{s_\theta\left(q_i, p_i\right)}}{\mathrm{e}^{s_\theta\left(q_i, p_i\right)}+\sum_j \mathrm{e}^{s_\theta\left(q_i, p_{i j}^{-}\right)}}\] <ul> <li>$s_\theta$: 모델 파라미터 $\theta$ 에 기반한 유사도 계산 함수</li> <li>$q_i$: query</li> <li>$p_i$: a positive passage</li> <li>$p_{i j}^{-}$: negative passages</li> </ul> <p>실제 구현은 sentence-transformers 라이브러리를 이용했는데, 찾아본 결과 <a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py#L13-L125">MultipleNegativesRankingLoss</a> 를 이용하여 학습한 것으로 보인다.</p> <h3 id="knowledge-distillation-for-finetuning">Knowledge distillation (for finetuning)</h3> <p>knowledge distillation from a cross-encoder (CE) teacher model</p> <p>KL divergence $D_{\mathrm{KL}}$ for distilling soft labels from the teacher model</p> \[\min D_{\mathrm{KL}}\left(p_{\mathrm{ce}}, p_{\mathrm{stu}}\right)+\alpha L_{\mathrm{cont}}\] <ul> <li>$p_{\mathrm{ce}}$, $p_{\mathrm{stu}}$: probability from the cross-encoder (CE) teacher and student model respectively</li> <li>$\alpha$: a hyperparameter to balance the two loss functions</li> <li>$L_{\mathrm{cont}}$: contrastive loss</li> </ul> <p>여기서는 SimLM 모델을 cross-encoder (teacher model)로 사용하였다고 한다.</p> <h1 id="multilingual-e5">Multilingual E5</h1> <p>e5-multilingual 모델은 다음과 같은 두 단계의 학습 전략을 통해 학습되었다.</p> <h2 id="training-two-stage-methodology">Training: Two-stage methodology</h2> <ol> <li>Weakly-supervised contrastive pre-training on billions of text pairs</li> <li>Supervised finetuning on small quantity of high-quality labeled data</li> </ol> <h3 id="1-weakly-supervised-contrastive-pre-training">(1) Weakly-supervised contrastive pre-training</h3> <p>약 1B 정도의 다국어 데이터셋(text pairs)을 이용하여 학습했다.</p> <p><img src="https://i.imgur.com/hNlpaUx.png" alt="Image" width="45%"/></p> <p>학습 loss 의 경우, in-batch negatives 가 적용된 InfoNCE contrastive loss 를 사용했다. 다른 하이퍼파라미터는 영어 전용 e5 모델을 학습할때와 동일하게 설정하였다고 한다.</p> <p>참고로, In-batch negative 는 딥러닝 모델 학습 시 배치(batch) 내의 다른 샘플들을 부정 샘플(negative sample)로 활용하는 기법이다. 해당 방식에 대해 몇가지 의견이 있다면…</p> <ul> <li>이는 별도의 negative sampling 작업없이 현재 배치에 포함된 데이터만으로 모델을 학습시킬 수 있어 효율적이다.</li> <li>또한, 배치 사이즈가 충분히 크다면 이런 단순한 전략이 다른 방식보다 훨씬 효율적이고 안정적인 학습을 가능하게 한다고 말한다.</li> <li>물론, batch size 를 줄이고 hard negatives 를 추가하는 것도 나쁘진 않지만, 100M 이상의 큰 데이터셋에서 hard negatives 를 찾아내는건 쉽지 않다 (non-trivial).</li> </ul> <h3 id="2-supervised-finetuning">(2) Supervised finetuning</h3> <p>이 단계에서는 in-batch negatives 전략 외에도, mined hard negatives 와 교차 인코더 모델(cross-encoder model)로부터의 지식 증류(knowledge distillation)를 전략을 추가로 활용하여 임베딩 품질을 더욱 향상시켰다.</p> <p><strong>Instruction model</strong> 의 경우, Reference [2] 에서 사용했던 GPT-3/4 기반 합성 데이터를 이용하여 embedding model 에 대해 instruction 튜닝을 진행하였다.</p> <h2 id="training-hyper-parameters">Training: hyper-parameters</h2> <p>mE5 (multilingual E5) 모델은 각각 다음과 같은 모델로부터 초기화되었다.</p> <ol> <li>mE5small: multilingual MiniLM</li> <li>mE5base: xlm-roberta-base</li> <li>mE5large: xlm-roberta-large</li> </ol> <p>Learning Rate 의 경우 모델 사이즈가 커질수록 더 낮게 설정했다.</p> <ul> <li>Pretraining: small, base, large 각각 {3, 2, 1}×10⁻⁴</li> <li>Finetuning: small, base, large 각각 {3, 2, 1}×10⁻⁵</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>MTEB benchmark</li> <li>MIRACL multilingual retrieval benchmark (MAP, nDCG)</li> <li>Bitext mining</li> </ul> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2402.05672">Multilingual E5 Text Embeddings: A Technical Report</a></li> <li>[2] <a href="https://arxiv.org/pdf/2402.05672">Improving Text Embeddings with Large Language Models</a></li> <li>[3] <a href="https://arxiv.org/abs/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></li> </ul> <p>Code</p> <ul> <li>e5-evaluation: https://github.com/microsoft/unilm/tree/master/e5</li> <li>sentence-transformers: https://github.com/UKPLab/sentence-transformers</li> </ul> <p>Models</p> <ul> <li>한국어 특화 임베딩 모델: https://github.com/nlpai-lab/KURE</li> </ul> <p>Blog</p> <ul> <li><a href="https://yjoonjang.medium.com/koe5-%EC%B5%9C%EC%B4%88%EC%9D%98-%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%AA%A8%EB%8D%B8-multilingual-e5-finetune-22fa7e56d220">KURE: 최초의 한국어 특화 임베딩 모델</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="Microsoft"/><category term="encoder"/><summary type="html"><![CDATA[Original E5]]></summary></entry><entry><title type="html">새로운 Bert 모델: ModernBERT</title><link href="https://zzong2006.github.io/blog/2025/modern-bert/" rel="alternate" type="text/html" title="새로운 Bert 모델: ModernBERT"/><published>2025-01-01T00:00:00+00:00</published><updated>2025-01-01T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/modern-bert</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/modern-bert/"><![CDATA[<p>기존 BERT family 들의 성능과 속도를 모두 이길 수 있는 ModernBERT 가 나왔다고 한다.</p> <p>여기서 성능이라하면 GLUE score 를 의미하고, 속도라 하면 초당 토큰 처리를 의미한다. 아래 그래프를 참고하자.</p> <p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/modernbert/modernbert_pareto_curve.png" style="width: 85%;"/></p> <h2 id="특징">특징</h2> <p>ModernBERT 는 기존의 BERT 대비 다음과 같은 장점을 가지고 있다.</p> <ol> <li>8k 토큰 사이즈 지원: 대부분의 BERT 시리즈는 최대 512 토큰 사이즈를 지원했다.</li> <li>메모리 효율적: Kaggle 에서 가장 많이 쓰이는 DeBERTaV3 모델보다 1/5 정도 메모리만 필요하다.</li> <li>추론 속도가 빠름: DeBERTa 모델보다 기본적으로 2배 빠르며, mixed length 인 케이스에서는 최대 4배 더 빠르다.</li> </ol> <p>그 외에도 학습 데이터셋 사이즈가 2T token 이라는 특징이 있다.</p> <p>아래의 세가지 방법론을 적용해서 ModernBERT 를 만들었다고 한다.</p> <ol> <li>현대화된 transformer 구조</li> <li>Particular attention to efficiency</li> <li>학습 데이터셋을 현대적으로 재구성</li> </ol> <h2 id="recipe-1-transformer-구조-업데이트">(Recipe 1) Transformer 구조 업데이트</h2> <p>Llama2 패밀리 모델에서 사용된 Transformer++ 구조에 영감을 얻어서 다음과 같은 업데이트를 적용했다고 한다.</p> <ul> <li>Old positional encoding 을 RoPE(rotary positional embeddings) 로 대체</li> <li>기존 BERT 의 GeLU activation function 이 포함된 MLP layers 를 GeGLU layers 로 교체</li> <li>불필요한 bias term 을 제거해서 parameter 수를 줄이는데 기여</li> <li>임베딩 레이어 이후에 normalization layer 를 추가</li> </ul> <h2 id="recipe-2-attention-방식-변경으로-효율성-높이기">(Recipe 2) Attention 방식 변경으로 효율성 높이기</h2> <p>Flash Attention 2 를 활용하여 아래와 같은 업데이트를 적용했다.</p> <ol> <li>Attention 변경: local and global</li> <li>Sequence Packing 적용</li> <li>하드웨어 밎춤형 모델 디자인</li> </ol> <h3 id="2-1-local-and-global-attention">2-1. Local and Global Attention</h3> <p>기존 BERT 에서는 모든 레이어에서 모든 input token 들에 대해 attention 로직을 적용했다. 이를 full attention 이라고 표현해보자.</p> <p>ModernBert 에서는 full attention 방식을 global 과 local attention 으로 나눴다.</p> <ul> <li>global attention: 매 3번째 layer 에서 모든 input token 들에 대해 attention 로직을 적용</li> <li>local attention: 나머지 layer 에서 sliding window 방식으로 <strong>일부 input token 들</strong>에 대해 attention 로직을 적용 (window size: 128)</li> </ul> <p>이는 마치 책을 읽을때와 비슷한데, 모든 문장에 대해서 집중하는 것(full attention) 과 주요 줄거리와 일부 문장에 대해서만 집중하는 것(global and local attention) 의 개념적 차이라고 볼 수 있다.</p> <p>Attention 의 계산 복잡도는 토큰 사이즈에 따라서 급증하므로, modernBert 는 긴 문장에 대해서 효율적으로 대처할 수 있다.</p> <h3 id="2-2-sequence-packing">2-2. Sequence Packing</h3> <p>일반적으로 서로 다른 sequence length 를 가진 데이터를 처리하기 위해서는 padding 을 적용해야 한다.</p> <p>하지만, 이 방식은 낭비되는 패딩 토큰에 의해서 성능 저하를 발생시키고, 패딩 토큰 역시 어떠한 의미론적 기여를 하지도 않는다.</p> <p>이런 문제를 해결하기 위해서, ModernBert 는 padding 을 붙이지 않는 Sequence packing 방식을 적용했다.</p> <p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/modernbert/modernbert_unpadding.png" style="width: 100%;"/></p> <p>Sequence packing 의 효과를 최대화하기 위해, 사전 학습 과정에서 model max-length 와 최대한 가까운 sequence 길이의 순서로 packing 을 적용하여 처리 속도를 높였다고 한다.</p> <h3 id="2-3-하드웨어-밎춤형-모델-디자인">2-3. 하드웨어 밎춤형 모델 디자인</h3> <p>연구에 따르면 deep &amp; narrow layer 조합이 wide &amp; shallow layer 조합보다 더 좋은 성능을 보이는 경우가 많다. 하지만 trade-off 가 발생하는데, 모델 layer 가 깊을수록 병렬화가 줄어들고 따라서 동일한 파라매터 개수라도 속도가 느려지게 된다.</p> <p>일반적인 GPU 사양이 RTX 3090/4090 정도 급이라는걸 가정하고, 제한된 grid search 를 통해서 최적의 모델 디자인을 찾았다고 한다.</p> <p>그 결과, base 는 150M 정도, large 는 400M 정도 크기의 모델이 되었으며, 임베딩 크기는 768 (base) 와 1024 (large) 로 확인할 수 있다.</p> <h2 id="recipe-3-학습-데이터셋-재구성">(Recipe 3) 학습 데이터셋 재구성</h2> <p>기존의 DeBERTaV3 같은 모델들은 Wikipedia and Wikibooks 와 같이 고품질 데이터만 학습셋으로 구성한 데이터로만 학습했다 (single text modality).</p> <p>하지만 modernBert 는 web documents, code, and scientific articles 같은 다양한 종류의 데이터도 학습 데이터셋을 활용한다.</p> <p>이런 학습 데이터셋의 변화는 ModernBERT 가 프로그래밍 관련 task 에서 더 좋은 성능을 보이는 것으로 증명할 수 있겠다.</p> <h2 id="학습-프로세스">학습 프로세스</h2> <p>기존의 BERT 학습 방식을 그대로 유지하되, 별 효과가 없는 next-sentence prediction 목적함수를 제거하고, masking rate 를 15% 에서 30% 로 증가시켰다.</p> <p>그리고 2T 데이터셋을 한번에 학습하지 않고, 총 3 단계의 학습을 거쳤다.</p> <ol> <li>1.7T 의 데이터는 1024 토큰 사이즈로 학습</li> <li>나머지 250B 토큰은 8192 토큰 사이즈로 학습</li> <li>마지막으로 나머지 50B 토큰은 길이를 랜덤으로 샘플링하여 학습</li> </ol> <p>이는 ProLong 이라는 것에서 영감을 받았다고 하는데, 구체적인 내용은 확인해보지 않아서 잘 모르겠다.</p> <p>이 외에도 아래 두가지 트릭으로 좀 더 효율적인 학습이 가능했다고 한다.</p> <ol> <li>Batch Size warming up: 배치 사이즈를 학습 초기에는 작게 가져갔다가 시간이 지날수록 조금씩 늘려가는 방법</li> <li>Tiling Model weight: Large 모델 weight 를 초기화할때 학습된 Base 모델을 활용하여 loss 수렴 속도를 높이는 방법</li> </ol> <h2 id="느낀점">느낀점</h2> <p>최근 8k 수준의 임베딩을 생성할 수 있는 인코더 전략들이 많이 나오는것 같다.</p> <p>FlashAttention 기반의 Sequence Packing 을 적용했다는 점, attention 방식을 Layer 마다 다르게 적용한 점도 흥미로웠다. 상당히 배워보고 싶은 기술들이다.</p> <p>최적의 레이어 디자인을 찾는것은 grid search 를 이용했다고 하니, 역시 이 부분은 노다가 아니면 안되는 일인듯 싶다.</p> <p>그리고 단순히 고품질 데이터가 문제가 아니라 더 다양한 데이터 역시 성능에 중요한 영향을 미친다는 것도 중요한 포인트인듯 싶다.</p> <h2 id="references">References</h2> <p>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</p> <ul> <li>blog: https://huggingface.co/blog/modernbert</li> <li>paper: https://huggingface.co/papers/2412.13663</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><category term="encoder"/><category term="BERT"/><category term="Flash_Attention"/><summary type="html"><![CDATA[기존 BERT family 들의 성능과 속도를 모두 이길 수 있는 ModernBERT 가 나왔다고 한다.]]></summary></entry><entry><title type="html">SFT 데이터셋의 노이즈를 줄여보자</title><link href="https://zzong2006.github.io/blog/2024/robust-ft/" rel="alternate" type="text/html" title="SFT 데이터셋의 노이즈를 줄여보자"/><published>2024-12-30T00:00:00+00:00</published><updated>2024-12-30T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2024/robust-ft</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/robust-ft/"><![CDATA[<p>Downstream task 를 위한 LLM 모델을 학습할 때, 파인튜닝(=SFT) 데이터에 노이즈가 있는 경우, 모델의 성능이 저하된다.</p> <p>이 논문에서는 RobustFT 라는 프레임워크를 제안하여, 데이터에 노이즈가 있는지 여부를 탐지하고, 노이즈가 있는 경우 재레이블링을 수행하여 모델의 성능을 향상시킨다.</p> <h2 id="노이즈-탐지-noise-identification">노이즈 탐지 (noise identification)</h2> <p>여기서 말하는 데이터에 포함된 “노이즈”란, 여러 LLM 의 응답이 일관성이 없는 경우를 말한다. 즉, prompt 와 response 가 주어졌을 때, 여러 LLM 의 응답이 일관성이 없는 경우 노이즈가 있다고 판단한다.</p> <h3 id="methods">Methods</h3> <p>여러 전문가 LLMs 과 Checker 메커니즘을 사용하여 데이터의 노이즈를 식별한다.</p> <ol> <li>어떤 base LLM 을 사용하여 정답 $y$ 가 있는 query $q_i$ 에 대해 응답 $\hat{y}_i$ 를 생성하는 것을 생각해보자.</li> <li>LLM 으로 (1) step-by-step reasoning (2) reflection 을 계속 반복하면서 응답 $\hat{y}^{reas}_i$ 를 생성하도록 한다.</li> <li>Checker 는 지금까지의 응답들 ($y,\hat{y}_i, \hat{y}^{reas}_i$) 을 이용해서 노이즈가 있는지 여부를 판단한다.</li> <li>Checker 는 응답의 일관성 정도를 0 또는 1 의 값으로 판단하는데, 0 이면 노이즈가 있는 것이고 1 이면 노이즈가 없는 것이다.</li> </ol> <p>여기서 Checker 메커니즘은 상당히 naive 한데, 데이터셋마다 메커니즘이 다르다. 예를 들어, MMLU 같은 경우는 응답들의 정답을 추출해서 서로 같은지 비교하는 것으로 노이즈 여부를 판단한다.</p> <h3 id="참고">참고</h3> <p>실험에서 LLM 으로 Gemma2-9B, Llama3.1-8B 를 사용하였다.</p> <h2 id="노이즈-제거-de-noising">노이즈 제거 (de-noising)</h2> <ol> <li>Review Agent: context-enhanced reasoning with clean samples to store label noisy instances</li> <li>a perplexity-based data selection mechanism to exclude samples with low confidence scores</li> </ol> <h2 id="느낀점">느낀점</h2> <p>정답(또는 golden reference) 이 없는 경우 Checker 가 잘 작동하지 않을것 같다.</p> <p>데이터 정제하는 건 좋은데, 다수의 LLM 이용해서 정제하는 비용은 또 다른 얘기다.</p> <h2 id="references">References</h2> <p>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</p> <ul> <li>paper: https://huggingface.co/papers/2412.14922</li> <li>code: https://github.com/luo-junyu/RobustFT</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="LLM"/><category term="SFT"/><category term="Preprocessing"/><summary type="html"><![CDATA[데이터 디노이징 프레임워크 RobustFT]]></summary></entry></feed>