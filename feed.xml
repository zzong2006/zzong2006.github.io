<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://zzong2006.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zzong2006.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-04T07:47:19+00:00</updated><id>https://zzong2006.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">RAG Competition</title><link href="https://zzong2006.github.io/blog/2025/rag-competition/" rel="alternate" type="text/html" title="RAG Competition"/><published>2025-01-04T00:00:00+00:00</published><updated>2025-01-04T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/rag-competition</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/rag-competition/"><![CDATA[<p>RAG 관련 대회 목록을 정리해보려 한다.</p> <hr/> <h3 id="1-financerag-challenge-at-icaif-24-종료">1. FinanceRAG Challenge at ICAIF ‘24 (종료)</h3> <p>5th ACM International Conference on AI in Finance (ICAIF’24)에서 RAG 관련 경진대회를 개최했습니다.</p> <p>최근 금융권에서도 LLM 사용에 대해 매우 많은 관심들을 갖고 계시고, 보다 정교한 사용을 위해 RAG이 많은 주목을 받고 있습니다.</p> <p>이번 경진대회는 참가자들이 직접 RAG 시스템을 구현해볼 수 있도록 준비 하였습니다. Linq에서 직접 미국의 증권시장 공시, 리포트 등을 바탕으로 query-corpus dataset을 세심하게 준비해주셨습니다. 특히 금융분야에는 이러한 데이터셋이 없는데요, 참가자들이 매우 좋은 데이터셋을 바탕으로 RAG 시스템을 구현해보고 evaluation도 해볼 수 있는 매우 좋은 기회라고 생각합니다.</p> <p>https://www.kaggle.com/competitions/icaif-24-finance-rag-challenge/overview</p> <h3 id="2-dacon-재정정보-ai-검색-알고리즘-경진대회-종료">2. Dacon: <a href="https://dacon.io/competitions/official/236295/overview/description">재정정보 AI 검색 알고리즘 경진대회</a> (종료)</h3> <p>대회 기간: 2024년 7월 29일 ~ 2024년 8월 23일</p> <p>특이한점은 RAG 로 활용할 데이터를 PDF 형태로 제공한다는 점이다.</p> <p>물론 LLM 학습에 필요한 Question-Answer pair 역시 제공된다.</p> <p>대부분 e5 모델로 vector search 를 위한 임베딩을 진행하였다.</p> <p>한 우승팀에서는 YOLOv10 기반 Document Layout Analysis를 통해 PDF parser를 구현하였다고 한다.</p> <h3 id="3-aicrowd-crag-comprehensive-rag-benchmark-종료">3. AiCrowd: <a href="https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024">CRAG: Comprehensive RAG Benchmark</a> (종료)</h3> <p>대회 기간: 2024년 5월 20일 ~ 2025년 1월 25일</p>]]></content><author><name></name></author><category term="RAG"/><category term="Competition"/><summary type="html"><![CDATA[RAG 관련 대회 목록을 정리해보려 한다.]]></summary></entry><entry><title type="html">행렬 미분 기초 (with Trace)</title><link href="https://zzong2006.github.io/blog/2025/matrix-derivative/" rel="alternate" type="text/html" title="행렬 미분 기초 (with Trace)"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/matrix-derivative</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/matrix-derivative/"><![CDATA[<p>행렬의 대각 성분의 합인 Trace ($ \text{Tr} $) 가 포함된 행렬 미분 규칙에 대해 정리해보려 한다.</p> <hr/> <h2 id="규칙-1--fracpartialpartial-w_d-texttra-w_d--atop-">규칙 1: $ \frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top $</h2> <p>여기서 $ A $는 $ m \times n $ 행렬이고 $ W_d $는 $ n \times m $ 행렬이라고 가정한다.</p> <p>행렬 미분의 결과는 미분한 행렬의 크기를 따라야 하므로, 이를 맞추기 위해서 transpose 를 취한다고 생각하면 편하다.</p> <p>(참고) Trace 는 정사각행렬에 대해서만 정의된다.</p> <h3 id="예시">예시</h3> <p>간단한 예시를 들어보자.</p> \[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix}, \quad W_d = \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix}\] <p>$ \text{Tr}(A W_d) $는 다음과 같이 계산된다.</p> \[\text{Tr}(A W_d) = \text{Tr}\left( \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} \begin{bmatrix} x &amp; y \\ z &amp; w \\ \end{bmatrix} \right) \\[10pt] = \text{Tr}\left( \begin{bmatrix} 1x + 2z &amp; 1y + 2w \\ 3x + 4z &amp; 3y + 4w \\ \end{bmatrix} \right) \\[10pt] = (1x + 2z) + (3y + 4w)\] <p>이제 $ W_d $ 의 각 원소에 대해 편미분을 계산해보자.</p> <ul> <li>$ \frac{\partial}{\partial x} \text{Tr}(A W_d) = 1 $</li> <li>$ \frac{\partial}{\partial y} \text{Tr}(A W_d) = 3 $</li> <li>$ \frac{\partial}{\partial z} \text{Tr}(A W_d) = 2 $</li> <li>$ \frac{\partial}{\partial w} \text{Tr}(A W_d) = 4 $</li> </ul> <p>Trace 연산은 스칼라 값으로 계산되지만, 이를 행렬 $ W_d $ 의 원소 각각에 대해 편미분하면, 원소별 변화율을 포함한 결과가 행렬 형태로 나타난다. 그 결과, 아래와 같이 transpose 된 A가 나온다.</p> \[\frac{\partial}{\partial W_d} \text{Tr}(A W_d) = A^\top = \begin{bmatrix} 1 &amp; 3 \\ 2 &amp; 4 \\ \end{bmatrix}\]]]></content><author><name></name></author><category term="matrix"/><category term="calculus"/><summary type="html"><![CDATA[행렬의 대각 성분의 합인 Trace ($ \text{Tr} $) 가 포함된 행렬 미분 규칙에 대해 정리해보려 한다.]]></summary></entry><entry><title type="html">Semantic Retrieval at Walmart</title><link href="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/" rel="alternate" type="text/html" title="Semantic Retrieval at Walmart"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/semantic-retrieval-at-walmart/"><![CDATA[<h2 id="proposed-method">Proposed Method</h2> <ol> <li>a hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries.</li> <li>a novel method of selecting negative examples for training a large neural retrieval model and an approximate metric to evaluate the performance</li> </ol> <h2 id="prudction-search-vs-web-search">Prudction search vs. Web search</h2> <p>Production search is way more challenging than web search.</p> <ul> <li>Product titles (the main search-able text) are generally much shorter than web documents.</li> <li>While many web documents may contain the same information, a specific product from a seller rarely has a duplicate.</li> </ul> <h2 id="traditional-solutions">Traditional Solutions</h2> <ul> <li>knowledge graph: need a huge amount of domain expertise, and the cost of maintaining these components is high, since the catalog and product vocabulary frequently change in e-commerce</li> <li>BM25, an inverted index: suffers from vocabulary mismatch between the query and the product title</li> <li>neural systems: limited by the fact that the embedding size cannot be too large due to latency concerns</li> </ul> <h2 id="a-hybrid-architecture">A hybrid architecture</h2> <p>…</p> <h2 id="lesson-learned">Lesson Learned</h2> <h3 id="cosine-similarity-vs-inner-product">Cosine similarity vs. Inner product</h3> <ul> <li>inner product is more stable during training and does not require the temperature factor 𝜎</li> <li>But, inner product was much harder to optimize when creating the ANN index, compared to cosine similarity.</li> </ul> <h3 id="text-fields">Text fields</h3> <p>Many text fields are generally available for each product, and the quality of the text fields varies. But, we could not extract any boost in performance by using these text fields. This is probably because descriptions can contain a lot of irrelevant text that simply adds noise.</p> <h3 id="model-complexity">Model Complexity</h3> <p>A very deep model or very large embedding size is not necessary to achieve top performance. This is probably because queries and product titles are not very complex from a semantic perspective.</p> <h2 id="abstract">Abstract</h2> <p>In product search, the retrieval of candidate products before re-ranking is more critical and challenging than other search like web search, especially for tail queries, which have a complex and specific search intent. In this paper, we present a hybrid system for e-commerce search deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to better answer user tail queries. Our system significantly improved the relevance of the search engine, measured by both offline and online evaluations. The improvements were achieved through a combination of different approaches. We present a new technique to train the neural model at scale. and describe how the system was deployed in production with little impact on response time. We highlight multiple learnings and practical tricks that were used in the deployment of this system.</p> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2412.04637">Semantic Retrieval at Walmart</a></li> </ul>]]></content><author><name></name></author><category term="Walmart"/><category term="ANN"/><category term="RAG"/><summary type="html"><![CDATA[Proposed Method]]></summary></entry><entry><title type="html">Text Embedding 모델: E5</title><link href="https://zzong2006.github.io/blog/2025/e5/" rel="alternate" type="text/html" title="Text Embedding 모델: E5"/><published>2025-01-02T00:00:00+00:00</published><updated>2025-01-02T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/e5</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/e5/"><![CDATA[<h1 id="original-e5">Original E5</h1> <h2 id="introduction">Introduction</h2> <p>기존의 BERT 와 GPT 같은 모델을 이용해 (추가 학습 없이) text representation 을 추출할 수 있으나, 이러한 모델들은 contrastive learning 으로 학습을 진행하지 않다보니 embedding 품질이 상대적으로 아쉬울 수 있다.</p> <p>a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings</p> <p>two prefix identifiers “query:” and “passage:” to $q$ and $d$</p> <h2 id="datasets-ccpairs">Datasets: CCPairs</h2> <p>1.3B unfiltered</p> <p>pair (query, passage) 예시</p> <ul> <li>reddit: (post, comment)</li> <li>stackoverflow: (question, upvoted answer)</li> <li>wikipedia: (title, abstract)</li> </ul> <h3 id="a-consistency-based-data-filtering-technique">A consistency-based data filtering technique</h3> <p>신경망 모델이 clean label 을 먼저 학습하고 그 이후에 noisy label 을 학습하는 경향이 있으므로, 이러한 경향을 활용하여 일관성 기반 데이터셋 필터링 기법을 적용</p> <p>아래의 방법을 통해 1.3B text pairs 를 270M 개로 줄임</p> <ol> <li>우선 1.3B 데이터셋을 이용해서 모델을 학습</li> <li>학습된 모델을 이용해 query 와 (정답이 포함된) 1M random passages 에 대한 연관성을 랭킹</li> <li>Top-k(=2) 개의 passage 안에 실제 정답이 포함되어 있는지 확인</li> <li>만약 정답이 포함되면 pair 를 keep, 그렇지 않다면 discard</li> </ol> <h2 id="training">Training</h2> <h3 id="contrastive-loss-for-pre-training">Contrastive loss (for pre-training)</h3> <p>아래와 같은 InfoNCE contrastive loss 를 사용하여 학습</p> \[\min L_{\mathrm{cont}}=-\frac{1}{n} \sum_i \log \frac{\mathrm{e}^{s_\theta\left(q_i, p_i\right)}}{\mathrm{e}^{s_\theta\left(q_i, p_i\right)}+\sum_j \mathrm{e}^{s_\theta\left(q_i, p_{i j}^{-}\right)}}\] <ul> <li>$s_\theta$: 모델 파라미터 $\theta$ 에 기반한 유사도 계산 함수</li> <li>$q_i$: query</li> <li>$p_i$: a positive passage</li> <li>$p_{i j}^{-}$: negative passages</li> </ul> <p>실제 구현은 sentence-transformers 라이브러리를 이용했는데, 찾아본 결과 <a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py#L13-L125">MultipleNegativesRankingLoss</a> 를 이용하여 학습한 것으로 보인다.</p> <h3 id="knowledge-distillation-for-finetuning">Knowledge distillation (for finetuning)</h3> <p>knowledge distillation from a cross-encoder (CE) teacher model</p> <p>KL divergence $D_{\mathrm{KL}}$ for distilling soft labels from the teacher model</p> \[\min D_{\mathrm{KL}}\left(p_{\mathrm{ce}}, p_{\mathrm{stu}}\right)+\alpha L_{\mathrm{cont}}\] <ul> <li>$p_{\mathrm{ce}}$, $p_{\mathrm{stu}}$: probability from the cross-encoder (CE) teacher and student model respectively</li> <li>$\alpha$: a hyperparameter to balance the two loss functions</li> <li>$L_{\mathrm{cont}}$: contrastive loss</li> </ul> <p>여기서는 SimLM 모델을 cross-encoder (teacher model)로 사용하였다고 한다.</p> <h1 id="multilingual-e5">Multilingual E5</h1> <p>e5-multilingual 모델은 다음과 같은 두 단계의 학습 전략을 통해 학습되었다.</p> <h2 id="training-two-stage-methodology">Training: Two-stage methodology</h2> <ol> <li>Weakly-supervised contrastive pre-training on billions of text pairs</li> <li>Supervised finetuning on small quantity of high-quality labeled data</li> </ol> <h3 id="1-weakly-supervised-contrastive-pre-training">(1) Weakly-supervised contrastive pre-training</h3> <p>약 1B 정도의 다국어 데이터셋(text pairs)을 이용하여 학습했다.</p> <p><img src="https://i.imgur.com/hNlpaUx.png" alt="Image" width="45%"/></p> <p>학습 loss 의 경우, in-batch negatives 가 적용된 InfoNCE contrastive loss 를 사용했다. 다른 하이퍼파라미터는 영어 전용 e5 모델을 학습할때와 동일하게 설정하였다고 한다.</p> <p>참고로, In-batch negative 는 딥러닝 모델 학습 시 배치(batch) 내의 다른 샘플들을 부정 샘플(negative sample)로 활용하는 기법이다. 해당 방식에 대해 몇가지 의견이 있다면…</p> <ul> <li>이는 별도의 negative sampling 작업없이 현재 배치에 포함된 데이터만으로 모델을 학습시킬 수 있어 효율적이다.</li> <li>또한, 배치 사이즈가 충분히 크다면 이런 단순한 전략이 다른 방식보다 훨씬 효율적이고 안정적인 학습을 가능하게 한다고 말한다.</li> <li>물론, batch size 를 줄이고 hard negatives 를 추가하는 것도 나쁘진 않지만, 100M 이상의 큰 데이터셋에서 hard negatives 를 찾아내는건 쉽지 않다 (non-trivial).</li> </ul> <h3 id="2-supervised-finetuning">(2) Supervised finetuning</h3> <p>이 단계에서는 in-batch negatives 전략 외에도, mined hard negatives 와 교차 인코더 모델(cross-encoder model)로부터의 지식 증류(knowledge distillation)를 전략을 추가로 활용하여 임베딩 품질을 더욱 향상시켰다.</p> <p><strong>Instruction model</strong> 의 경우, Reference [2] 에서 사용했던 GPT-3/4 기반 합성 데이터를 이용하여 embedding model 에 대해 instruction 튜닝을 진행하였다.</p> <h2 id="training-hyper-parameters">Training: hyper-parameters</h2> <p>mE5 (multilingual E5) 모델은 각각 다음과 같은 모델로부터 초기화되었다.</p> <ol> <li>mE5small: multilingual MiniLM</li> <li>mE5base: xlm-roberta-base</li> <li>mE5large: xlm-roberta-large</li> </ol> <p>Learning Rate 의 경우 모델 사이즈가 커질수록 더 낮게 설정했다.</p> <ul> <li>Pretraining: small, base, large 각각 {3, 2, 1}×10⁻⁴</li> <li>Finetuning: small, base, large 각각 {3, 2, 1}×10⁻⁵</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>MTEB benchmark</li> <li>MIRACL multilingual retrieval benchmark (MAP, nDCG)</li> <li>Bitext mining</li> </ul> <h2 id="references">References</h2> <p>Papers</p> <ul> <li>[1] <a href="https://arxiv.org/pdf/2402.05672">Multilingual E5 Text Embeddings: A Technical Report</a></li> <li>[2] <a href="https://arxiv.org/pdf/2402.05672">Improving Text Embeddings with Large Language Models</a></li> <li>[3] <a href="https://arxiv.org/abs/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></li> </ul> <p>Code</p> <ul> <li>e5-evaluation: https://github.com/microsoft/unilm/tree/master/e5</li> <li>sentence-transformers: https://github.com/UKPLab/sentence-transformers</li> </ul> <p>Models</p> <ul> <li>한국어 특화 임베딩 모델: https://github.com/nlpai-lab/KURE</li> </ul> <p>Blog</p> <ul> <li><a href="https://yjoonjang.medium.com/koe5-%EC%B5%9C%EC%B4%88%EC%9D%98-%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%AA%A8%EB%8D%B8-multilingual-e5-finetune-22fa7e56d220">KURE: 최초의 한국어 특화 임베딩 모델</a></li> </ul>]]></content><author><name></name></author><category term="microsoft"/><category term="encoder"/><summary type="html"><![CDATA[Original E5]]></summary></entry><entry><title type="html">새로운 Bert 모델: ModernBERT</title><link href="https://zzong2006.github.io/blog/2025/modern-bert/" rel="alternate" type="text/html" title="새로운 Bert 모델: ModernBERT"/><published>2025-01-01T00:00:00+00:00</published><updated>2025-01-01T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2025/modern-bert</id><content type="html" xml:base="https://zzong2006.github.io/blog/2025/modern-bert/"><![CDATA[<p>기존 BERT family 들의 성능과 속도를 모두 이길 수 있는 ModernBERT 가 나왔다고 한다.</p> <p>여기서 성능이라하면 GLUE score 를 의미하고, 속도라 하면 초당 토큰 처리를 의미한다. 아래 그래프를 참고하자.</p> <p><img src="https://cdn-lfs.hf.co/datasets/huggingface/documentation-images/32fba11bd9e3594f1f9b9eba90e4651cc565275afa85af5c68eae87aad19ccd0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27modernbert_pareto_curve.png%3B+filename%3D%22modernbert_pareto_curve.png%22%3B&amp;response-content-type=image%2Fpng&amp;Expires=1735985095&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNTk4NTA5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy8zMmZiYTExYmQ5ZTM1OTRmMWY5YjllYmE5MGU0NjUxY2M1NjUyNzVhZmE4NWFmNWM2OGVhZTg3YWFkMTljY2QwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&amp;Signature=TIQgVGH8PBIQYSnxWKvBRBEN7U4UGxL8HBZedeYaAhSg5I4OxXHyXgUYlZ5IWSWzzGof48OIDb8eiam-afiVsxNzT9PhmctUUYmYSZKRn2CuS6tOzkpBUjZ%7EbxridS6pXebaEr%7E-EcLpfNms20p9DTxbYtJm4qGuaNd7zlJabh7ihNbnagwPqFUNm9k7%7EtNSsuoJ2gsRLt4h-aONxmKP9dQZ0woIPzSf%7E3Ab3nmzeAYOWtECeXFa5Gm-AQ4RJBFd0F5v3GlQ0YbGNZ%7EDbx5SQkcS-GGRLz34GycQNwU83hBx77ylxuXSKXInHzIOSXT2I70X3rSGh17bXacspmidWw__&amp;Key-Pair-Id=K3RPWS32NSSJCE" style="width: 100%;"/></p> <h2 id="특징">특징</h2> <p>ModernBERT 는 기존의 BERT 대비 다음과 같은 장점을 가지고 있다.</p> <ol> <li>8k 토큰 사이즈 지원: 대부분의 BERT 시리즈는 최대 512 토큰 사이즈를 지원했다.</li> <li>메모리 효율적: Kaggle 에서 가장 많이 쓰이는 DeBERTaV3 모델보다 1/5 정도 메모리만 필요하다.</li> <li>추론 속도가 빠름: DeBERTa 모델보다 기본적으로 2배 빠르며, mixed length 인 케이스에서는 최대 4배 더 빠르다.</li> </ol> <p>그 외에도 학습 데이터셋 사이즈가 2T token 이라는 특징이 있다.</p> <p>아래의 세가지 방법론을 적용해서 ModernBERT 를 만들었다고 한다.</p> <ol> <li>현대화된 transformer 구조</li> <li>Particular attention to efficiency</li> <li>학습 데이터셋을 현대적으로 재구성</li> </ol> <h2 id="recipe-1-transformer-구조-업데이트">(Recipe 1) Transformer 구조 업데이트</h2> <p>Llama2 패밀리 모델에서 사용된 Transformer++ 구조에 영감을 얻어서 다음과 같은 업데이트를 적용했다고 한다.</p> <ul> <li>Old positional encoding 을 RoPE(rotary positional embeddings) 로 대체</li> <li>기존 BERT 의 GeLU activation function 이 포함된 MLP layers 를 GeGLU layers 로 교체</li> <li>불필요한 bias term 을 제거해서 parameter 수를 줄이는데 기여</li> <li>임베딩 레이어 이후에 normalization layer 를 추가</li> </ul> <h2 id="recipe-2-attention-방식-변경으로-효율성-높이기">(Recipe 2) Attention 방식 변경으로 효율성 높이기</h2> <p>Flash Attention 2 를 활용하여 아래와 같은 업데이트를 적용했다.</p> <ol> <li>Attention 변경: local and global</li> <li>Sequence Packing 적용</li> <li>하드웨어 밎춤형 모델 디자인</li> </ol> <h3 id="2-1-local-and-global-attention">2-1. Local and Global Attention</h3> <p>기존 BERT 에서는 모든 레이어에서 모든 input token 들에 대해 attention 로직을 적용했다. 이를 full attention 이라고 표현해보자.</p> <p>ModernBert 에서는 full attention 방식을 global 과 local attention 으로 나눴다.</p> <ul> <li>global attention: 매 3번째 layer 에서 모든 input token 들에 대해 attention 로직을 적용</li> <li>local attention: 나머지 layer 에서 sliding window 방식으로 <strong>일부 input token 들</strong>에 대해 attention 로직을 적용 (window size: 128)</li> </ul> <p>이는 마치 책을 읽을때와 비슷한데, 모든 문장에 대해서 집중하는 것(full attention) 과 주요 줄거리와 일부 문장에 대해서만 집중하는 것(global and local attention) 의 개념적 차이라고 볼 수 있다.</p> <p>Attention 의 계산 복잡도는 토큰 사이즈에 따라서 급증하므로, modernBert 는 긴 문장에 대해서 효율적으로 대처할 수 있다.</p> <h3 id="2-2-sequence-packing">2-2. Sequence Packing</h3> <p>일반적으로 서로 다른 sequence length 를 가진 데이터를 처리하기 위해서는 padding 을 적용해야 한다.</p> <p>하지만, 이 방식은 낭비되는 패딩 토큰에 의해서 성능 저하를 발생시키고, 패딩 토큰 역시 어떠한 의미론적 기여를 하지도 않는다.</p> <p>이런 문제를 해결하기 위해서, ModernBert 는 padding 을 붙이지 않는 Sequence packing 방식을 적용했다.</p> <p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/modernbert/modernbert_unpadding.png" style="width: 100%;"/></p> <p>Sequence packing 의 효과를 최대화하기 위해, 사전 학습 과정에서 model max-length 와 최대한 가까운 sequence 길이의 순서로 packing 을 적용하여 처리 속도를 높였다고 한다.</p> <h3 id="2-3-하드웨어-밎춤형-모델-디자인">2-3. 하드웨어 밎춤형 모델 디자인</h3> <p>연구에 따르면 deep &amp; narrow layer 조합이 wide &amp; shallow layer 조합보다 더 좋은 성능을 보이는 경우가 많다. 하지만 trade-off 가 발생하는데, 모델 layer 가 깊을수록 병렬화가 줄어들고 따라서 동일한 파라매터 개수라도 속도가 느려지게 된다.</p> <p>일반적인 GPU 사양이 RTX 3090/4090 정도 급이라는걸 가정하고, 제한된 grid search 를 통해서 최적의 모델 디자인을 찾았다고 한다.</p> <p>그 결과, base 는 150M 정도, large 는 400M 정도 크기의 모델이 되었으며, 임베딩 크기는 768 (base) 와 1024 (large) 로 확인할 수 있다.</p> <h2 id="recipe-3-학습-데이터셋-재구성">(Recipe 3) 학습 데이터셋 재구성</h2> <p>기존의 DeBERTaV3 같은 모델들은 Wikipedia and Wikibooks 와 같이 고품질 데이터만 학습셋으로 구성한 데이터로만 학습했다 (single text modality).</p> <p>하지만 modernBert 는 web documents, code, and scientific articles 같은 다양한 종류의 데이터도 학습 데이터셋을 활용한다.</p> <p>이런 학습 데이터셋의 변화는 ModernBERT 가 프로그래밍 관련 task 에서 더 좋은 성능을 보이는 것으로 증명할 수 있겠다.</p> <h2 id="학습-프로세스">학습 프로세스</h2> <p>기존의 BERT 학습 방식을 그대로 유지하되, 별 효과가 없는 next-sentence prediction 목적함수를 제거하고, masking rate 를 15% 에서 30% 로 증가시켰다.</p> <p>그리고 2T 데이터셋을 한번에 학습하지 않고, 총 3 단계의 학습을 거쳤다.</p> <ol> <li>1.7T 의 데이터는 1024 토큰 사이즈로 학습</li> <li>나머지 250B 토큰은 8192 토큰 사이즈로 학습</li> <li>마지막으로 나머지 50B 토큰은 길이를 랜덤으로 샘플링하여 학습</li> </ol> <p>이는 ProLong 이라는 것에서 영감을 받았다고 하는데, 구체적인 내용은 확인해보지 않아서 잘 모르겠다.</p> <p>이 외에도 아래 두가지 트릭으로 좀 더 효율적인 학습이 가능했다고 한다.</p> <ol> <li>Batch Size warming up: 배치 사이즈를 학습 초기에는 작게 가져갔다가 시간이 지날수록 조금씩 늘려가는 방법</li> <li>Tiling Model weight: Large 모델 weight 를 초기화할때 학습된 Base 모델을 활용하여 loss 수렴 속도를 높이는 방법</li> </ol> <h2 id="느낀점">느낀점</h2> <p>최근 8k 수준의 임베딩을 생성할 수 있는 인코더 전략들이 많이 나오는것 같다.</p> <p>FlashAttention 기반의 Sequence Packing 을 적용했다는 점, attention 방식을 Layer 마다 다르게 적용한 점도 흥미로웠다. 상당히 배워보고 싶은 기술들이다.</p> <p>최적의 레이어 디자인을 찾는것은 grid search 를 이용했다고 하니, 역시 이 부분은 노다가 아니면 안되는 일인듯 싶다.</p> <p>그리고 단순히 고품질 데이터가 문제가 아니라 더 다양한 데이터 역시 성능에 중요한 영향을 미친다는 것도 중요한 포인트인듯 싶다.</p> <h2 id="references">References</h2> <p>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</p> <ul> <li>blog: https://huggingface.co/blog/modernbert</li> <li>paper: https://huggingface.co/papers/2412.13663</li> </ul>]]></content><author><name></name></author><category term="llm"/><category term="encoder"/><category term="bert"/><category term="flash_attention"/><summary type="html"><![CDATA[기존 BERT family 들의 성능과 속도를 모두 이길 수 있는 ModernBERT 가 나왔다고 한다.]]></summary></entry><entry><title type="html">SFT 데이터셋의 노이즈를 줄여보자</title><link href="https://zzong2006.github.io/blog/2024/robust-ft/" rel="alternate" type="text/html" title="SFT 데이터셋의 노이즈를 줄여보자"/><published>2024-12-30T00:00:00+00:00</published><updated>2024-12-30T00:00:00+00:00</updated><id>https://zzong2006.github.io/blog/2024/robust-ft</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/robust-ft/"><![CDATA[<p>Downstream task 를 위한 LLM 모델을 학습할 때, 파인튜닝(=SFT) 데이터에 노이즈가 있는 경우, 모델의 성능이 저하된다.</p> <p>이 논문에서는 RobustFT 라는 프레임워크를 제안하여, 데이터에 노이즈가 있는지 여부를 탐지하고, 노이즈가 있는 경우 재레이블링을 수행하여 모델의 성능을 향상시킨다.</p> <h2 id="노이즈-탐지-noise-identification">노이즈 탐지 (noise identification)</h2> <p>여기서 말하는 데이터에 포함된 “노이즈”란, 여러 LLM 의 응답이 일관성이 없는 경우를 말한다. 즉, prompt 와 response 가 주어졌을 때, 여러 LLM 의 응답이 일관성이 없는 경우 노이즈가 있다고 판단한다.</p> <h3 id="methods">Methods</h3> <p>여러 전문가 LLMs 과 Checker 메커니즘을 사용하여 데이터의 노이즈를 식별한다.</p> <ol> <li>어떤 base LLM 을 사용하여 정답 $y$ 가 있는 query $q_i$ 에 대해 응답 $\hat{y}_i$ 를 생성하는 것을 생각해보자.</li> <li>LLM 으로 (1) step-by-step reasoning (2) reflection 을 계속 반복하면서 응답 $\hat{y}^{reas}_i$ 를 생성하도록 한다.</li> <li>Checker 는 지금까지의 응답들 ($y,\hat{y}_i, \hat{y}^{reas}_i$) 을 이용해서 노이즈가 있는지 여부를 판단한다.</li> <li>Checker 는 응답의 일관성 정도를 0 또는 1 의 값으로 판단하는데, 0 이면 노이즈가 있는 것이고 1 이면 노이즈가 없는 것이다.</li> </ol> <p>여기서 Checker 메커니즘은 상당히 naive 한데, 데이터셋마다 메커니즘이 다르다. 예를 들어, MMLU 같은 경우는 응답들의 정답을 추출해서 서로 같은지 비교하는 것으로 노이즈 여부를 판단한다.</p> <h3 id="참고">참고</h3> <p>실험에서 LLM 으로 Gemma2-9B, Llama3.1-8B 를 사용하였다.</p> <h2 id="노이즈-제거-de-noising">노이즈 제거 (de-noising)</h2> <ol> <li>Review Agent: context-enhanced reasoning with clean samples to store label noisy instances</li> <li>a perplexity-based data selection mechanism to exclude samples with low confidence scores</li> </ol> <h2 id="느낀점">느낀점</h2> <p>정답(또는 golden reference) 이 없는 경우 Checker 가 잘 작동하지 않을것 같다.</p> <p>데이터 정제하는 건 좋은데, 다수의 LLM 이용해서 정제하는 비용은 또 다른 얘기다.</p> <h2 id="references">References</h2> <p>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</p> <ul> <li>paper: https://huggingface.co/papers/2412.14922</li> <li>code: https://github.com/luo-junyu/RobustFT</li> </ul>]]></content><author><name></name></author><category term="llm"/><category term="sft"/><category term="data_preprocessing"/><summary type="html"><![CDATA[데이터 디노이징 프레임워크 RobustFT]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://zzong2006.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://zzong2006.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://zzong2006.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://zzong2006.github.io/blog/2024/typograms</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://zzong2006.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://zzong2006.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with echarts</title><link href="https://zzong2006.github.io/blog/2024/echarts/" rel="alternate" type="text/html" title="a post with echarts"/><published>2024-01-26T16:03:00+00:00</published><updated>2024-01-26T16:03:00+00:00</updated><id>https://zzong2006.github.io/blog/2024/echarts</id><content type="html" xml:base="https://zzong2006.github.io/blog/2024/echarts/"><![CDATA[<p>This is an example post with some <a href="https://echarts.apache.org/">echarts</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">echarts
</span><span class="sb">{
  "title": {
    "text": "ECharts Getting Started Example"
  },
  "responsive": true,
  "tooltip": {},
  "legend": {
    "top": "30px",
    "data": ["sales"]
  },
  "xAxis": {
    "data": ["Shirts", "Cardigans", "Chiffons", "Pants", "Heels", "Socks"]
  },
  "yAxis": {},
  "series": [
    {
      "name": "sales",
      "type": "bar",
      "data": [5, 20, 36, 10, 10, 20]
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-echarts">{
  "title": {
    "text": "ECharts Getting Started Example"
  },
  "responsive": true,
  "tooltip": {},
  "legend": {
    "top": "30px",
    "data": ["sales"]
  },
  "xAxis": {
    "data": ["Shirts", "Cardigans", "Chiffons", "Pants", "Heels", "Socks"]
  },
  "yAxis": {},
  "series": [
    {
      "name": "sales",
      "type": "bar",
      "data": [5, 20, 36, 10, 10, 20]
    }
  ]
}
</code></pre> <p>Note that this library offer support for both light and dark themes. You can switch between them using the theme switcher in the top right corner of the page.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included echarts code could look like]]></summary></entry></feed>